{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tianchi.aliyun.com/notebook-ai/detail?postId=103212"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:48:57.120744Z",
     "start_time": "2020-04-26T13:48:53.185854Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import jn\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', None)\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "from sklearn.decomposition import PCA, FastICA, FactorAnalysis, SparsePCA\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import scipy.signal as signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:48:57.139847Z",
     "start_time": "2020-04-26T13:48:57.120744Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" \n",
    "    iterate through all the columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:48:57.176824Z",
     "start_time": "2020-04-26T13:48:57.143845Z"
    }
   },
   "outputs": [],
   "source": [
    "# 处理异常值\n",
    "def smooth_cols(group,out_value,kind):\n",
    "    cols = ['power']\n",
    "    if kind == 'g':\n",
    "        for col in cols:\n",
    "            yes_no = (group[col]<out_value).astype('int')\n",
    "            new = yes_no * group[col]\n",
    "            group[col] = new.replace(0,group[col].quantile(q=0.995))\n",
    "        return group\n",
    "    if kind == 'l':\n",
    "        for col in cols:\n",
    "            yes_no = (group[col]>out_value).astype('int')\n",
    "            new = yes_no * group[col]\n",
    "            group[col] = new.replace(0,group[col].quantile(q=0.07))\n",
    "        return group        \n",
    "\n",
    "def date_proc(x):\n",
    "    # '20200426' '20200026'\n",
    "    m = int(x[4:6])\n",
    "    if m == 0:\n",
    "        m = 1\n",
    "    return x[:4] + '-' + str(m) + '-' + x[6:]\n",
    "\n",
    "# 定义日期提取函数\n",
    "def date_tran(df,fea_col):\n",
    "    for f in tqdm(fea_col):\n",
    "        df[f] = pd.to_datetime(df[f].astype('str').apply(date_proc))\n",
    "        df[f + '_year'] = df[f].dt.year # 年份\n",
    "        df[f + '_month'] = df[f].dt.month # 月份\n",
    "        df[f + '_day'] = df[f].dt.day # 多少号\n",
    "        df[f + '_dayofweek'] = df[f].dt.dayofweek # 周几\n",
    "    return df\n",
    "\n",
    "# 分桶操作\n",
    "def cut_group(df, cols, num_bins=50):\n",
    "    for col in cols:\n",
    "        all_range = int(df[col].max() - df[col].min())\n",
    "        bin = [i * all_range / num_bins for i in range(all_range)]\n",
    "        df[col + '_bin'] = pd.cut(df[col], bin, labels=False)\n",
    "    return df\n",
    "\n",
    "# count编码\n",
    "def count_coding(df, fea_col):\n",
    "    for f in fea_col:\n",
    "        df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "    return df\n",
    "\n",
    "# 定义交叉特征统计\n",
    "def cross_cat_num(df, num_col, cat_col):\n",
    "    for f1 in tqdm(cat_col):\n",
    "        g = df.groupby(f1, as_index=False)\n",
    "        for f2 in tqdm(num_col):\n",
    "            feat = g[f2].agg({\n",
    "                '{}_{}_max'.format(f1, f2): 'max', # 最大值\n",
    "                '{}_{}_min'.format(f1, f2): 'min', # 最小值\n",
    "                '{}_{}_median'.format(f1, f2): 'median', # 中位数\n",
    "            })\n",
    "            df = df.merge(feat, on=f1, how='left')\n",
    "    return df\n",
    "\n",
    "# 类别特征的二阶交叉\n",
    "from scipy.stats import entropy\n",
    "def cross_qua_cat_num(df):\n",
    "    for f_pair in tqdm([\n",
    "        ['model', 'brand'], ['model', 'regionCode'], ['brand', 'regionCode']\n",
    "    ]):\n",
    "        # 共现次数\n",
    "        df['_'.join(f_pair) + '_count'] = df.groupby(f_pair)['SaleID'].transform('count')\n",
    "        # n unique、熵\n",
    "        df = df.merge(df.groupby(f_pair[0], as_index=False)[f_pair[1]].agg({\n",
    "            '{}_{}_nunique'.format(f_pair[0], f_pair[1]): 'nunique',\n",
    "            '{}_{}_ent'.format(f_pair[0], f_pair[1]): lambda x: entropy(x.value_counts() / x.shape[0])\n",
    "        }), on=f_pair[0], how='left')\n",
    "        df = df.merge(df.groupby(f_pair[1], as_index=False)[f_pair[0]].agg({\n",
    "            '{}_{}_nunique'.format(f_pair[1], f_pair[0]): 'nunique',\n",
    "            '{}_{}_ent'.format(f_pair[1], f_pair[0]): lambda x: entropy(x.value_counts() / x.shape[0])\n",
    "        }), on=f_pair[1], how='left')\n",
    "        # 比例偏好\n",
    "        df['{}_in_{}_prop'.format(f_pair[0], f_pair[1])] = df['_'.join(f_pair) + '_count'] / df[f_pair[1] + '_count']\n",
    "        df['{}_in_{}_prop'.format(f_pair[1], f_pair[0])] = df['_'.join(f_pair) + '_count'] / df[f_pair[0] + '_count']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:48:59.071287Z",
     "start_time": "2020-04-26T13:48:57.179823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 37200080.00 MB\n",
      "Memory usage after optimization is: 10200184.00 MB\n",
      "Decreased by 72.6%\n",
      "Memory usage of dataframe is 12000080.00 MB\n",
      "Memory usage after optimization is: 3200184.00 MB\n",
      "Decreased by 73.3%\n",
      "Train data shape: (150000, 31)\n",
      "TestA_data shape: (50000, 30)\n"
     ]
    }
   ],
   "source": [
    "Train_data = reduce_mem_usage(pd.read_csv('input/used_car_train_20200313.csv',\n",
    "                                          sep=' '))\n",
    "TestA_data = reduce_mem_usage(pd.read_csv('input/used_car_testB_20200421.csv',\n",
    "                                          sep=' '))\n",
    "\n",
    "print('Train data shape: {}'.format(Train_data.shape))\n",
    "print('TestA_data shape: {}'.format(TestA_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:48:59.125257Z",
     "start_time": "2020-04-26T13:48:59.073284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SaleID</th>\n",
       "      <th>name</th>\n",
       "      <th>regDate</th>\n",
       "      <th>model</th>\n",
       "      <th>brand</th>\n",
       "      <th>bodyType</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>power</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>notRepairedDamage</th>\n",
       "      <th>regionCode</th>\n",
       "      <th>seller</th>\n",
       "      <th>offerType</th>\n",
       "      <th>creatDate</th>\n",
       "      <th>price</th>\n",
       "      <th>v_0</th>\n",
       "      <th>v_1</th>\n",
       "      <th>v_2</th>\n",
       "      <th>v_3</th>\n",
       "      <th>v_4</th>\n",
       "      <th>v_5</th>\n",
       "      <th>v_6</th>\n",
       "      <th>v_7</th>\n",
       "      <th>v_8</th>\n",
       "      <th>v_9</th>\n",
       "      <th>v_10</th>\n",
       "      <th>v_11</th>\n",
       "      <th>v_12</th>\n",
       "      <th>v_13</th>\n",
       "      <th>v_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>736</td>\n",
       "      <td>20040402</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1046</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160404</td>\n",
       "      <td>1850</td>\n",
       "      <td>43.34375</td>\n",
       "      <td>3.966797</td>\n",
       "      <td>0.050262</td>\n",
       "      <td>2.160156</td>\n",
       "      <td>1.143555</td>\n",
       "      <td>0.235718</td>\n",
       "      <td>0.101990</td>\n",
       "      <td>0.129517</td>\n",
       "      <td>0.022812</td>\n",
       "      <td>0.097473</td>\n",
       "      <td>-2.880859</td>\n",
       "      <td>2.804688</td>\n",
       "      <td>-2.419922</td>\n",
       "      <td>0.795410</td>\n",
       "      <td>0.914551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2262</td>\n",
       "      <td>20030301</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-</td>\n",
       "      <td>4366</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160309</td>\n",
       "      <td>3600</td>\n",
       "      <td>45.31250</td>\n",
       "      <td>5.234375</td>\n",
       "      <td>0.137939</td>\n",
       "      <td>1.380859</td>\n",
       "      <td>-1.421875</td>\n",
       "      <td>0.264893</td>\n",
       "      <td>0.121033</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0.026596</td>\n",
       "      <td>0.020584</td>\n",
       "      <td>-4.902344</td>\n",
       "      <td>2.095703</td>\n",
       "      <td>-1.030273</td>\n",
       "      <td>-1.722656</td>\n",
       "      <td>0.245483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>14874</td>\n",
       "      <td>20040403</td>\n",
       "      <td>115.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2806</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160402</td>\n",
       "      <td>6222</td>\n",
       "      <td>45.96875</td>\n",
       "      <td>4.824219</td>\n",
       "      <td>1.319336</td>\n",
       "      <td>-0.998535</td>\n",
       "      <td>-0.997070</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.114929</td>\n",
       "      <td>0.165161</td>\n",
       "      <td>0.062164</td>\n",
       "      <td>0.027069</td>\n",
       "      <td>-4.847656</td>\n",
       "      <td>1.803711</td>\n",
       "      <td>1.565430</td>\n",
       "      <td>-0.832520</td>\n",
       "      <td>-0.229980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>71865</td>\n",
       "      <td>19960908</td>\n",
       "      <td>109.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>193</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>434</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160312</td>\n",
       "      <td>2400</td>\n",
       "      <td>45.68750</td>\n",
       "      <td>4.492188</td>\n",
       "      <td>-0.050629</td>\n",
       "      <td>0.883789</td>\n",
       "      <td>-2.228516</td>\n",
       "      <td>0.274414</td>\n",
       "      <td>0.110291</td>\n",
       "      <td>0.121948</td>\n",
       "      <td>0.033386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.507812</td>\n",
       "      <td>1.286133</td>\n",
       "      <td>-0.501953</td>\n",
       "      <td>-2.437500</td>\n",
       "      <td>-0.478760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>111080</td>\n",
       "      <td>20120103</td>\n",
       "      <td>110.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6977</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160313</td>\n",
       "      <td>5200</td>\n",
       "      <td>44.37500</td>\n",
       "      <td>2.031250</td>\n",
       "      <td>0.572266</td>\n",
       "      <td>-1.571289</td>\n",
       "      <td>2.246094</td>\n",
       "      <td>0.228027</td>\n",
       "      <td>0.073181</td>\n",
       "      <td>0.091858</td>\n",
       "      <td>0.078796</td>\n",
       "      <td>0.121521</td>\n",
       "      <td>-1.896484</td>\n",
       "      <td>0.910645</td>\n",
       "      <td>0.931152</td>\n",
       "      <td>2.833984</td>\n",
       "      <td>1.923828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>137642</td>\n",
       "      <td>20090602</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3690</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160319</td>\n",
       "      <td>8000</td>\n",
       "      <td>46.31250</td>\n",
       "      <td>-3.228516</td>\n",
       "      <td>0.156616</td>\n",
       "      <td>-1.727539</td>\n",
       "      <td>-0.345703</td>\n",
       "      <td>0.260254</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.119812</td>\n",
       "      <td>0.090942</td>\n",
       "      <td>0.048767</td>\n",
       "      <td>1.885742</td>\n",
       "      <td>-2.722656</td>\n",
       "      <td>2.457031</td>\n",
       "      <td>-0.286865</td>\n",
       "      <td>0.206543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2402</td>\n",
       "      <td>19990411</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3073</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160317</td>\n",
       "      <td>3500</td>\n",
       "      <td>46.09375</td>\n",
       "      <td>4.925781</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>1.644531</td>\n",
       "      <td>-1.270508</td>\n",
       "      <td>0.268066</td>\n",
       "      <td>0.117676</td>\n",
       "      <td>0.142334</td>\n",
       "      <td>0.025452</td>\n",
       "      <td>0.028168</td>\n",
       "      <td>-4.902344</td>\n",
       "      <td>1.610352</td>\n",
       "      <td>-0.834473</td>\n",
       "      <td>-1.996094</td>\n",
       "      <td>-0.103210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>165346</td>\n",
       "      <td>19990706</td>\n",
       "      <td>26.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160326</td>\n",
       "      <td>1000</td>\n",
       "      <td>42.25000</td>\n",
       "      <td>-3.167969</td>\n",
       "      <td>-0.676758</td>\n",
       "      <td>1.942383</td>\n",
       "      <td>0.524414</td>\n",
       "      <td>0.239502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122925</td>\n",
       "      <td>0.039825</td>\n",
       "      <td>0.082397</td>\n",
       "      <td>3.693359</td>\n",
       "      <td>-0.244995</td>\n",
       "      <td>-2.193359</td>\n",
       "      <td>0.236694</td>\n",
       "      <td>0.195557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2974</td>\n",
       "      <td>20030205</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>179</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4679</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160326</td>\n",
       "      <td>2850</td>\n",
       "      <td>46.09375</td>\n",
       "      <td>4.894531</td>\n",
       "      <td>0.475342</td>\n",
       "      <td>0.556641</td>\n",
       "      <td>-1.262695</td>\n",
       "      <td>0.263916</td>\n",
       "      <td>0.116577</td>\n",
       "      <td>0.144287</td>\n",
       "      <td>0.039856</td>\n",
       "      <td>0.024384</td>\n",
       "      <td>-4.925781</td>\n",
       "      <td>1.587891</td>\n",
       "      <td>0.075317</td>\n",
       "      <td>-1.550781</td>\n",
       "      <td>0.069458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>82021</td>\n",
       "      <td>19980101</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160402</td>\n",
       "      <td>650</td>\n",
       "      <td>43.06250</td>\n",
       "      <td>1.666016</td>\n",
       "      <td>-2.201172</td>\n",
       "      <td>3.097656</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.262451</td>\n",
       "      <td>0.068237</td>\n",
       "      <td>0.012177</td>\n",
       "      <td>0.010292</td>\n",
       "      <td>0.098755</td>\n",
       "      <td>-1.089844</td>\n",
       "      <td>0.600586</td>\n",
       "      <td>-4.187500</td>\n",
       "      <td>0.198242</td>\n",
       "      <td>-1.025391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SaleID    name   regDate  model  brand  bodyType  fuelType  gearbox  power  \\\n",
       "0       0     736  20040402   30.0      6       1.0       0.0      0.0     60   \n",
       "1       1    2262  20030301   40.0      1       2.0       0.0      0.0      0   \n",
       "2       2   14874  20040403  115.0     15       1.0       0.0      0.0    163   \n",
       "3       3   71865  19960908  109.0     10       0.0       0.0      1.0    193   \n",
       "4       4  111080  20120103  110.0      5       1.0       0.0      0.0     68   \n",
       "5       5  137642  20090602   24.0     10       0.0       1.0      0.0    109   \n",
       "6       6    2402  19990411   13.0      4       0.0       0.0      1.0    150   \n",
       "7       7  165346  19990706   26.0     14       1.0       0.0      0.0    101   \n",
       "8       8    2974  20030205   19.0      1       2.0       1.0      1.0    179   \n",
       "9       9   82021  19980101    7.0      7       5.0       0.0      0.0     88   \n",
       "\n",
       "   kilometer notRepairedDamage  regionCode  seller  offerType  creatDate  \\\n",
       "0       12.5               0.0        1046       0          0   20160404   \n",
       "1       15.0                 -        4366       0          0   20160309   \n",
       "2       12.5               0.0        2806       0          0   20160402   \n",
       "3       15.0               0.0         434       0          0   20160312   \n",
       "4        5.0               0.0        6977       0          0   20160313   \n",
       "5       10.0               0.0        3690       0          0   20160319   \n",
       "6       15.0               0.0        3073       0          0   20160317   \n",
       "7       15.0               0.0        4000       0          0   20160326   \n",
       "8       15.0               0.0        4679       0          0   20160326   \n",
       "9       15.0               0.0         302       0          0   20160402   \n",
       "\n",
       "   price       v_0       v_1       v_2       v_3       v_4       v_5  \\\n",
       "0   1850  43.34375  3.966797  0.050262  2.160156  1.143555  0.235718   \n",
       "1   3600  45.31250  5.234375  0.137939  1.380859 -1.421875  0.264893   \n",
       "2   6222  45.96875  4.824219  1.319336 -0.998535 -0.997070  0.251465   \n",
       "3   2400  45.68750  4.492188 -0.050629  0.883789 -2.228516  0.274414   \n",
       "4   5200  44.37500  2.031250  0.572266 -1.571289  2.246094  0.228027   \n",
       "5   8000  46.31250 -3.228516  0.156616 -1.727539 -0.345703  0.260254   \n",
       "6   3500  46.09375  4.925781  0.113281  1.644531 -1.270508  0.268066   \n",
       "7   1000  42.25000 -3.167969 -0.676758  1.942383  0.524414  0.239502   \n",
       "8   2850  46.09375  4.894531  0.475342  0.556641 -1.262695  0.263916   \n",
       "9    650  43.06250  1.666016 -2.201172  3.097656  0.843750  0.262451   \n",
       "\n",
       "        v_6       v_7       v_8       v_9      v_10      v_11      v_12  \\\n",
       "0  0.101990  0.129517  0.022812  0.097473 -2.880859  2.804688 -2.419922   \n",
       "1  0.121033  0.135742  0.026596  0.020584 -4.902344  2.095703 -1.030273   \n",
       "2  0.114929  0.165161  0.062164  0.027069 -4.847656  1.803711  1.565430   \n",
       "3  0.110291  0.121948  0.033386  0.000000 -4.507812  1.286133 -0.501953   \n",
       "4  0.073181  0.091858  0.078796  0.121521 -1.896484  0.910645  0.931152   \n",
       "5  0.000518  0.119812  0.090942  0.048767  1.885742 -2.722656  2.457031   \n",
       "6  0.117676  0.142334  0.025452  0.028168 -4.902344  1.610352 -0.834473   \n",
       "7  0.000000  0.122925  0.039825  0.082397  3.693359 -0.244995 -2.193359   \n",
       "8  0.116577  0.144287  0.039856  0.024384 -4.925781  1.587891  0.075317   \n",
       "9  0.068237  0.012177  0.010292  0.098755 -1.089844  0.600586 -4.187500   \n",
       "\n",
       "       v_13      v_14  \n",
       "0  0.795410  0.914551  \n",
       "1 -1.722656  0.245483  \n",
       "2 -0.832520 -0.229980  \n",
       "3 -2.437500 -0.478760  \n",
       "4  2.833984  1.923828  \n",
       "5 -0.286865  0.206543  \n",
       "6 -1.996094 -0.103210  \n",
       "7  0.236694  0.195557  \n",
       "8 -1.550781  0.069458  \n",
       "9  0.198242 -1.025391  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:48:59.185232Z",
     "start_time": "2020-04-26T13:48:59.130252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SaleID</th>\n",
       "      <th>name</th>\n",
       "      <th>regDate</th>\n",
       "      <th>model</th>\n",
       "      <th>brand</th>\n",
       "      <th>bodyType</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>power</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>notRepairedDamage</th>\n",
       "      <th>regionCode</th>\n",
       "      <th>seller</th>\n",
       "      <th>offerType</th>\n",
       "      <th>creatDate</th>\n",
       "      <th>v_0</th>\n",
       "      <th>v_1</th>\n",
       "      <th>v_2</th>\n",
       "      <th>v_3</th>\n",
       "      <th>v_4</th>\n",
       "      <th>v_5</th>\n",
       "      <th>v_6</th>\n",
       "      <th>v_7</th>\n",
       "      <th>v_8</th>\n",
       "      <th>v_9</th>\n",
       "      <th>v_10</th>\n",
       "      <th>v_11</th>\n",
       "      <th>v_12</th>\n",
       "      <th>v_13</th>\n",
       "      <th>v_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>133777</td>\n",
       "      <td>20000501</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160308</td>\n",
       "      <td>42.15625</td>\n",
       "      <td>-3.095703</td>\n",
       "      <td>-0.721191</td>\n",
       "      <td>1.466797</td>\n",
       "      <td>1.009766</td>\n",
       "      <td>0.236572</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.105347</td>\n",
       "      <td>0.046234</td>\n",
       "      <td>0.094543</td>\n",
       "      <td>3.619141</td>\n",
       "      <td>-0.280518</td>\n",
       "      <td>-2.019531</td>\n",
       "      <td>0.979004</td>\n",
       "      <td>0.803223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>61206</td>\n",
       "      <td>19950211</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1505</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160310</td>\n",
       "      <td>43.90625</td>\n",
       "      <td>-3.244141</td>\n",
       "      <td>-0.766602</td>\n",
       "      <td>1.276367</td>\n",
       "      <td>-1.065430</td>\n",
       "      <td>0.261475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>0.046783</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>2.998047</td>\n",
       "      <td>-1.406250</td>\n",
       "      <td>-1.020508</td>\n",
       "      <td>-1.349609</td>\n",
       "      <td>-0.200562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>67829</td>\n",
       "      <td>20090606</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-</td>\n",
       "      <td>1776</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160309</td>\n",
       "      <td>45.37500</td>\n",
       "      <td>3.373047</td>\n",
       "      <td>-0.965332</td>\n",
       "      <td>-2.447266</td>\n",
       "      <td>0.624512</td>\n",
       "      <td>0.261719</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079651</td>\n",
       "      <td>0.073608</td>\n",
       "      <td>-3.951172</td>\n",
       "      <td>-0.433350</td>\n",
       "      <td>0.918945</td>\n",
       "      <td>1.634766</td>\n",
       "      <td>1.027344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>8892</td>\n",
       "      <td>20020601</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160314</td>\n",
       "      <td>42.78125</td>\n",
       "      <td>4.035156</td>\n",
       "      <td>-0.217407</td>\n",
       "      <td>1.708984</td>\n",
       "      <td>1.119141</td>\n",
       "      <td>0.236084</td>\n",
       "      <td>0.101807</td>\n",
       "      <td>0.098938</td>\n",
       "      <td>0.026825</td>\n",
       "      <td>0.096619</td>\n",
       "      <td>-2.847656</td>\n",
       "      <td>2.800781</td>\n",
       "      <td>-2.525391</td>\n",
       "      <td>1.077148</td>\n",
       "      <td>0.461670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>76998</td>\n",
       "      <td>20030301</td>\n",
       "      <td>46.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>738</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160306</td>\n",
       "      <td>43.65625</td>\n",
       "      <td>-3.134766</td>\n",
       "      <td>-1.133789</td>\n",
       "      <td>0.470215</td>\n",
       "      <td>0.134033</td>\n",
       "      <td>0.257080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066711</td>\n",
       "      <td>0.057770</td>\n",
       "      <td>0.068848</td>\n",
       "      <td>2.839844</td>\n",
       "      <td>-1.660156</td>\n",
       "      <td>-0.924316</td>\n",
       "      <td>0.199463</td>\n",
       "      <td>0.450928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200005</td>\n",
       "      <td>142813</td>\n",
       "      <td>19990006</td>\n",
       "      <td>37.0</td>\n",
       "      <td>18</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3393</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160404</td>\n",
       "      <td>43.65625</td>\n",
       "      <td>-3.130859</td>\n",
       "      <td>-2.519531</td>\n",
       "      <td>1.180664</td>\n",
       "      <td>1.295898</td>\n",
       "      <td>0.261475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.110718</td>\n",
       "      <td>2.646484</td>\n",
       "      <td>-2.441406</td>\n",
       "      <td>-2.255859</td>\n",
       "      <td>0.712402</td>\n",
       "      <td>-3.314453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200006</td>\n",
       "      <td>135370</td>\n",
       "      <td>19980503</td>\n",
       "      <td>36.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2244</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160305</td>\n",
       "      <td>41.90625</td>\n",
       "      <td>-3.117188</td>\n",
       "      <td>-2.884766</td>\n",
       "      <td>3.189453</td>\n",
       "      <td>0.860352</td>\n",
       "      <td>0.261963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018356</td>\n",
       "      <td>0.098389</td>\n",
       "      <td>3.501953</td>\n",
       "      <td>-1.248047</td>\n",
       "      <td>-4.574219</td>\n",
       "      <td>0.569824</td>\n",
       "      <td>1.083984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200007</td>\n",
       "      <td>7138</td>\n",
       "      <td>20040201</td>\n",
       "      <td>88.0</td>\n",
       "      <td>14</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160325</td>\n",
       "      <td>44.87500</td>\n",
       "      <td>4.542969</td>\n",
       "      <td>-0.659668</td>\n",
       "      <td>-0.196411</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.109070</td>\n",
       "      <td>0.027527</td>\n",
       "      <td>0.051117</td>\n",
       "      <td>0.108276</td>\n",
       "      <td>-4.433594</td>\n",
       "      <td>1.144531</td>\n",
       "      <td>-0.750977</td>\n",
       "      <td>1.530273</td>\n",
       "      <td>-0.903320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200008</td>\n",
       "      <td>7977</td>\n",
       "      <td>20110209</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1184</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160328</td>\n",
       "      <td>47.09375</td>\n",
       "      <td>4.433594</td>\n",
       "      <td>0.462402</td>\n",
       "      <td>-1.986328</td>\n",
       "      <td>0.339111</td>\n",
       "      <td>0.259277</td>\n",
       "      <td>0.109070</td>\n",
       "      <td>0.081909</td>\n",
       "      <td>0.076965</td>\n",
       "      <td>0.066772</td>\n",
       "      <td>-5.183594</td>\n",
       "      <td>0.286865</td>\n",
       "      <td>2.080078</td>\n",
       "      <td>0.625977</td>\n",
       "      <td>0.684570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200009</td>\n",
       "      <td>104001</td>\n",
       "      <td>19991012</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4874</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160317</td>\n",
       "      <td>41.50000</td>\n",
       "      <td>-3.134766</td>\n",
       "      <td>-1.010742</td>\n",
       "      <td>2.517578</td>\n",
       "      <td>0.414551</td>\n",
       "      <td>0.240112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110840</td>\n",
       "      <td>0.030624</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>3.958984</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>-3.095703</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.367188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SaleID    name   regDate  model  brand  bodyType  fuelType  gearbox  power  \\\n",
       "0  200000  133777  20000501   67.0      0       1.0       0.0      0.0    101   \n",
       "1  200001   61206  19950211   19.0      6       2.0       0.0      0.0     73   \n",
       "2  200002   67829  20090606    5.0      5       4.0       0.0      0.0    120   \n",
       "3  200003    8892  20020601   22.0      9       1.0       0.0      0.0     58   \n",
       "4  200004   76998  20030301   46.0      6       0.0       NaN      0.0    116   \n",
       "5  200005  142813  19990006   37.0     18       6.0       0.0      0.0    125   \n",
       "6  200006  135370  19980503   36.0      6       4.0       0.0      0.0     75   \n",
       "7  200007    7138  20040201   88.0     14       3.0       0.0      1.0    125   \n",
       "8  200008    7977  20110209   77.0      0       3.0       1.0      1.0    140   \n",
       "9  200009  104001  19991012   30.0      6       1.0       0.0      0.0     74   \n",
       "\n",
       "   kilometer notRepairedDamage  regionCode  seller  offerType  creatDate  \\\n",
       "0       15.0               0.0        5019       0          0   20160308   \n",
       "1        6.0               0.0        1505       0          0   20160310   \n",
       "2        5.0                 -        1776       0          0   20160309   \n",
       "3       15.0               0.0          26       0          0   20160314   \n",
       "4       15.0               0.0         738       0          0   20160306   \n",
       "5       15.0               0.0        3393       0          0   20160404   \n",
       "6       15.0               0.0        2244       0          0   20160305   \n",
       "7       15.0               0.0         155       0          0   20160325   \n",
       "8        7.0               0.0        1184       0          0   20160328   \n",
       "9       15.0               0.0        4874       0          0   20160317   \n",
       "\n",
       "        v_0       v_1       v_2       v_3       v_4       v_5       v_6  \\\n",
       "0  42.15625 -3.095703 -0.721191  1.466797  1.009766  0.236572  0.000241   \n",
       "1  43.90625 -3.244141 -0.766602  1.276367 -1.065430  0.261475  0.000000   \n",
       "2  45.37500  3.373047 -0.965332 -2.447266  0.624512  0.261719  0.090820   \n",
       "3  42.78125  4.035156 -0.217407  1.708984  1.119141  0.236084  0.101807   \n",
       "4  43.65625 -3.134766 -1.133789  0.470215  0.134033  0.257080  0.000000   \n",
       "5  43.65625 -3.130859 -2.519531  1.180664  1.295898  0.261475  0.000000   \n",
       "6  41.90625 -3.117188 -2.884766  3.189453  0.860352  0.261963  0.000000   \n",
       "7  44.87500  4.542969 -0.659668 -0.196411  1.531250  0.251465  0.109070   \n",
       "8  47.09375  4.433594  0.462402 -1.986328  0.339111  0.259277  0.109070   \n",
       "9  41.50000 -3.134766 -1.010742  2.517578  0.414551  0.240112  0.000000   \n",
       "\n",
       "        v_7       v_8       v_9      v_10      v_11      v_12      v_13  \\\n",
       "0  0.105347  0.046234  0.094543  3.619141 -0.280518 -2.019531  0.979004   \n",
       "1  0.120300  0.046783  0.035400  2.998047 -1.406250 -1.020508 -1.349609   \n",
       "2  0.000000  0.079651  0.073608 -3.951172 -0.433350  0.918945  1.634766   \n",
       "3  0.098938  0.026825  0.096619 -2.847656  2.800781 -2.525391  1.077148   \n",
       "4  0.066711  0.057770  0.068848  2.839844 -1.660156 -0.924316  0.199463   \n",
       "5  0.000000  0.046875  0.110718  2.646484 -2.441406 -2.255859  0.712402   \n",
       "6  0.000000  0.018356  0.098389  3.501953 -1.248047 -4.574219  0.569824   \n",
       "7  0.027527  0.051117  0.108276 -4.433594  1.144531 -0.750977  1.530273   \n",
       "8  0.081909  0.076965  0.066772 -5.183594  0.286865  2.080078  0.625977   \n",
       "9  0.110840  0.030624  0.080078  3.958984  0.066406 -3.095703  0.205444   \n",
       "\n",
       "       v_14  \n",
       "0  0.803223  \n",
       "1 -0.200562  \n",
       "2  1.027344  \n",
       "3  0.461670  \n",
       "4  0.450928  \n",
       "5 -3.314453  \n",
       "6  1.083984  \n",
       "7 -0.903320  \n",
       "8  0.684570  \n",
       "9  0.367188  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TestA_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:48:59.344129Z",
     "start_time": "2020-04-26T13:48:59.189218Z"
    }
   },
   "outputs": [],
   "source": [
    "# 合并数据集\n",
    "concat_data = pd.concat([Train_data, TestA_data],\n",
    "                        ignore_index=True) # 重新生成索引\n",
    "\n",
    "# 'notRepairedDamage'中的'-'用0替换\n",
    "concat_data['notRepairedDamage'] = concat_data['notRepairedDamage'].replace('-', 0).astype('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:49:00.013012Z",
     "start_time": "2020-04-26T13:48:59.348127Z"
    }
   },
   "outputs": [],
   "source": [
    "# 每列中的缺失值用每列的众数填充\n",
    "concat_data = concat_data.fillna(concat_data.mode().iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:49:00.026268Z",
     "start_time": "2020-04-26T13:49:00.013012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 31)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:49:00.084235Z",
     "start_time": "2020-04-26T13:49:00.029267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SaleID</th>\n",
       "      <th>bodyType</th>\n",
       "      <th>brand</th>\n",
       "      <th>creatDate</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>model</th>\n",
       "      <th>name</th>\n",
       "      <th>notRepairedDamage</th>\n",
       "      <th>offerType</th>\n",
       "      <th>power</th>\n",
       "      <th>price</th>\n",
       "      <th>regDate</th>\n",
       "      <th>regionCode</th>\n",
       "      <th>seller</th>\n",
       "      <th>v_0</th>\n",
       "      <th>v_1</th>\n",
       "      <th>v_10</th>\n",
       "      <th>v_11</th>\n",
       "      <th>v_12</th>\n",
       "      <th>v_13</th>\n",
       "      <th>v_14</th>\n",
       "      <th>v_2</th>\n",
       "      <th>v_3</th>\n",
       "      <th>v_4</th>\n",
       "      <th>v_5</th>\n",
       "      <th>v_6</th>\n",
       "      <th>v_7</th>\n",
       "      <th>v_8</th>\n",
       "      <th>v_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>20160404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>30.0</td>\n",
       "      <td>736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1850.0</td>\n",
       "      <td>20040402</td>\n",
       "      <td>1046</td>\n",
       "      <td>0</td>\n",
       "      <td>43.34375</td>\n",
       "      <td>3.966797</td>\n",
       "      <td>-2.880859</td>\n",
       "      <td>2.804688</td>\n",
       "      <td>-2.419922</td>\n",
       "      <td>0.795410</td>\n",
       "      <td>0.914551</td>\n",
       "      <td>0.050262</td>\n",
       "      <td>2.160156</td>\n",
       "      <td>1.143555</td>\n",
       "      <td>0.235718</td>\n",
       "      <td>0.101990</td>\n",
       "      <td>0.129517</td>\n",
       "      <td>0.022812</td>\n",
       "      <td>0.097473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20160309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>20030301</td>\n",
       "      <td>4366</td>\n",
       "      <td>0</td>\n",
       "      <td>45.31250</td>\n",
       "      <td>5.234375</td>\n",
       "      <td>-4.902344</td>\n",
       "      <td>2.095703</td>\n",
       "      <td>-1.030273</td>\n",
       "      <td>-1.722656</td>\n",
       "      <td>0.245483</td>\n",
       "      <td>0.137939</td>\n",
       "      <td>1.380859</td>\n",
       "      <td>-1.421875</td>\n",
       "      <td>0.264893</td>\n",
       "      <td>0.121033</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0.026596</td>\n",
       "      <td>0.020584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>20160402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>115.0</td>\n",
       "      <td>14874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>6222.0</td>\n",
       "      <td>20040403</td>\n",
       "      <td>2806</td>\n",
       "      <td>0</td>\n",
       "      <td>45.96875</td>\n",
       "      <td>4.824219</td>\n",
       "      <td>-4.847656</td>\n",
       "      <td>1.803711</td>\n",
       "      <td>1.565430</td>\n",
       "      <td>-0.832520</td>\n",
       "      <td>-0.229980</td>\n",
       "      <td>1.319336</td>\n",
       "      <td>-0.998535</td>\n",
       "      <td>-0.997070</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.114929</td>\n",
       "      <td>0.165161</td>\n",
       "      <td>0.062164</td>\n",
       "      <td>0.027069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>20160312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>71865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>193</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>19960908</td>\n",
       "      <td>434</td>\n",
       "      <td>0</td>\n",
       "      <td>45.68750</td>\n",
       "      <td>4.492188</td>\n",
       "      <td>-4.507812</td>\n",
       "      <td>1.286133</td>\n",
       "      <td>-0.501953</td>\n",
       "      <td>-2.437500</td>\n",
       "      <td>-0.478760</td>\n",
       "      <td>-0.050629</td>\n",
       "      <td>0.883789</td>\n",
       "      <td>-2.228516</td>\n",
       "      <td>0.274414</td>\n",
       "      <td>0.110291</td>\n",
       "      <td>0.121948</td>\n",
       "      <td>0.033386</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>20160313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>111080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>20120103</td>\n",
       "      <td>6977</td>\n",
       "      <td>0</td>\n",
       "      <td>44.37500</td>\n",
       "      <td>2.031250</td>\n",
       "      <td>-1.896484</td>\n",
       "      <td>0.910645</td>\n",
       "      <td>0.931152</td>\n",
       "      <td>2.833984</td>\n",
       "      <td>1.923828</td>\n",
       "      <td>0.572266</td>\n",
       "      <td>-1.571289</td>\n",
       "      <td>2.246094</td>\n",
       "      <td>0.228027</td>\n",
       "      <td>0.073181</td>\n",
       "      <td>0.091858</td>\n",
       "      <td>0.078796</td>\n",
       "      <td>0.121521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>20160319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>137642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>20090602</td>\n",
       "      <td>3690</td>\n",
       "      <td>0</td>\n",
       "      <td>46.31250</td>\n",
       "      <td>-3.228516</td>\n",
       "      <td>1.885742</td>\n",
       "      <td>-2.722656</td>\n",
       "      <td>2.457031</td>\n",
       "      <td>-0.286865</td>\n",
       "      <td>0.206543</td>\n",
       "      <td>0.156616</td>\n",
       "      <td>-1.727539</td>\n",
       "      <td>-0.345703</td>\n",
       "      <td>0.260254</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.119812</td>\n",
       "      <td>0.090942</td>\n",
       "      <td>0.048767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>20160317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>19990411</td>\n",
       "      <td>3073</td>\n",
       "      <td>0</td>\n",
       "      <td>46.09375</td>\n",
       "      <td>4.925781</td>\n",
       "      <td>-4.902344</td>\n",
       "      <td>1.610352</td>\n",
       "      <td>-0.834473</td>\n",
       "      <td>-1.996094</td>\n",
       "      <td>-0.103210</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>1.644531</td>\n",
       "      <td>-1.270508</td>\n",
       "      <td>0.268066</td>\n",
       "      <td>0.117676</td>\n",
       "      <td>0.142334</td>\n",
       "      <td>0.025452</td>\n",
       "      <td>0.028168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>20160326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>165346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>19990706</td>\n",
       "      <td>4000</td>\n",
       "      <td>0</td>\n",
       "      <td>42.25000</td>\n",
       "      <td>-3.167969</td>\n",
       "      <td>3.693359</td>\n",
       "      <td>-0.244995</td>\n",
       "      <td>-2.193359</td>\n",
       "      <td>0.236694</td>\n",
       "      <td>0.195557</td>\n",
       "      <td>-0.676758</td>\n",
       "      <td>1.942383</td>\n",
       "      <td>0.524414</td>\n",
       "      <td>0.239502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122925</td>\n",
       "      <td>0.039825</td>\n",
       "      <td>0.082397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20160326</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>2850.0</td>\n",
       "      <td>20030205</td>\n",
       "      <td>4679</td>\n",
       "      <td>0</td>\n",
       "      <td>46.09375</td>\n",
       "      <td>4.894531</td>\n",
       "      <td>-4.925781</td>\n",
       "      <td>1.587891</td>\n",
       "      <td>0.075317</td>\n",
       "      <td>-1.550781</td>\n",
       "      <td>0.069458</td>\n",
       "      <td>0.475342</td>\n",
       "      <td>0.556641</td>\n",
       "      <td>-1.262695</td>\n",
       "      <td>0.263916</td>\n",
       "      <td>0.116577</td>\n",
       "      <td>0.144287</td>\n",
       "      <td>0.039856</td>\n",
       "      <td>0.024384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "      <td>20160402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>82021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>650.0</td>\n",
       "      <td>19980101</td>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>43.06250</td>\n",
       "      <td>1.666016</td>\n",
       "      <td>-1.089844</td>\n",
       "      <td>0.600586</td>\n",
       "      <td>-4.187500</td>\n",
       "      <td>0.198242</td>\n",
       "      <td>-1.025391</td>\n",
       "      <td>-2.201172</td>\n",
       "      <td>3.097656</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.262451</td>\n",
       "      <td>0.068237</td>\n",
       "      <td>0.012177</td>\n",
       "      <td>0.010292</td>\n",
       "      <td>0.098755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SaleID  bodyType  brand  creatDate  fuelType  gearbox  kilometer  model  \\\n",
       "0       0       1.0      6   20160404       0.0      0.0       12.5   30.0   \n",
       "1       1       2.0      1   20160309       0.0      0.0       15.0   40.0   \n",
       "2       2       1.0     15   20160402       0.0      0.0       12.5  115.0   \n",
       "3       3       0.0     10   20160312       0.0      1.0       15.0  109.0   \n",
       "4       4       1.0      5   20160313       0.0      0.0        5.0  110.0   \n",
       "5       5       0.0     10   20160319       1.0      0.0       10.0   24.0   \n",
       "6       6       0.0      4   20160317       0.0      1.0       15.0   13.0   \n",
       "7       7       1.0     14   20160326       0.0      0.0       15.0   26.0   \n",
       "8       8       2.0      1   20160326       1.0      1.0       15.0   19.0   \n",
       "9       9       5.0      7   20160402       0.0      0.0       15.0    7.0   \n",
       "\n",
       "     name  notRepairedDamage  offerType  power   price   regDate  regionCode  \\\n",
       "0     736                0.0          0     60  1850.0  20040402        1046   \n",
       "1    2262                0.0          0      0  3600.0  20030301        4366   \n",
       "2   14874                0.0          0    163  6222.0  20040403        2806   \n",
       "3   71865                0.0          0    193  2400.0  19960908         434   \n",
       "4  111080                0.0          0     68  5200.0  20120103        6977   \n",
       "5  137642                0.0          0    109  8000.0  20090602        3690   \n",
       "6    2402                0.0          0    150  3500.0  19990411        3073   \n",
       "7  165346                0.0          0    101  1000.0  19990706        4000   \n",
       "8    2974                0.0          0    179  2850.0  20030205        4679   \n",
       "9   82021                0.0          0     88   650.0  19980101         302   \n",
       "\n",
       "   seller       v_0       v_1      v_10      v_11      v_12      v_13  \\\n",
       "0       0  43.34375  3.966797 -2.880859  2.804688 -2.419922  0.795410   \n",
       "1       0  45.31250  5.234375 -4.902344  2.095703 -1.030273 -1.722656   \n",
       "2       0  45.96875  4.824219 -4.847656  1.803711  1.565430 -0.832520   \n",
       "3       0  45.68750  4.492188 -4.507812  1.286133 -0.501953 -2.437500   \n",
       "4       0  44.37500  2.031250 -1.896484  0.910645  0.931152  2.833984   \n",
       "5       0  46.31250 -3.228516  1.885742 -2.722656  2.457031 -0.286865   \n",
       "6       0  46.09375  4.925781 -4.902344  1.610352 -0.834473 -1.996094   \n",
       "7       0  42.25000 -3.167969  3.693359 -0.244995 -2.193359  0.236694   \n",
       "8       0  46.09375  4.894531 -4.925781  1.587891  0.075317 -1.550781   \n",
       "9       0  43.06250  1.666016 -1.089844  0.600586 -4.187500  0.198242   \n",
       "\n",
       "       v_14       v_2       v_3       v_4       v_5       v_6       v_7  \\\n",
       "0  0.914551  0.050262  2.160156  1.143555  0.235718  0.101990  0.129517   \n",
       "1  0.245483  0.137939  1.380859 -1.421875  0.264893  0.121033  0.135742   \n",
       "2 -0.229980  1.319336 -0.998535 -0.997070  0.251465  0.114929  0.165161   \n",
       "3 -0.478760 -0.050629  0.883789 -2.228516  0.274414  0.110291  0.121948   \n",
       "4  1.923828  0.572266 -1.571289  2.246094  0.228027  0.073181  0.091858   \n",
       "5  0.206543  0.156616 -1.727539 -0.345703  0.260254  0.000518  0.119812   \n",
       "6 -0.103210  0.113281  1.644531 -1.270508  0.268066  0.117676  0.142334   \n",
       "7  0.195557 -0.676758  1.942383  0.524414  0.239502  0.000000  0.122925   \n",
       "8  0.069458  0.475342  0.556641 -1.262695  0.263916  0.116577  0.144287   \n",
       "9 -1.025391 -2.201172  3.097656  0.843750  0.262451  0.068237  0.012177   \n",
       "\n",
       "        v_8       v_9  \n",
       "0  0.022812  0.097473  \n",
       "1  0.026596  0.020584  \n",
       "2  0.062164  0.027069  \n",
       "3  0.033386  0.000000  \n",
       "4  0.078796  0.121521  \n",
       "5  0.090942  0.048767  \n",
       "6  0.025452  0.028168  \n",
       "7  0.039825  0.082397  \n",
       "8  0.039856  0.024384  \n",
       "9  0.010292  0.098755  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:49:00.142201Z",
     "start_time": "2020-04-26T13:49:00.087234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SaleID</th>\n",
       "      <th>bodyType</th>\n",
       "      <th>brand</th>\n",
       "      <th>creatDate</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>model</th>\n",
       "      <th>name</th>\n",
       "      <th>notRepairedDamage</th>\n",
       "      <th>offerType</th>\n",
       "      <th>power</th>\n",
       "      <th>price</th>\n",
       "      <th>regDate</th>\n",
       "      <th>regionCode</th>\n",
       "      <th>seller</th>\n",
       "      <th>v_0</th>\n",
       "      <th>v_1</th>\n",
       "      <th>v_10</th>\n",
       "      <th>v_11</th>\n",
       "      <th>v_12</th>\n",
       "      <th>v_13</th>\n",
       "      <th>v_14</th>\n",
       "      <th>v_2</th>\n",
       "      <th>v_3</th>\n",
       "      <th>v_4</th>\n",
       "      <th>v_5</th>\n",
       "      <th>v_6</th>\n",
       "      <th>v_7</th>\n",
       "      <th>v_8</th>\n",
       "      <th>v_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199990</th>\n",
       "      <td>249990</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>20160331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>61395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>500.0</td>\n",
       "      <td>19990906</td>\n",
       "      <td>3160</td>\n",
       "      <td>0</td>\n",
       "      <td>43.09375</td>\n",
       "      <td>-3.195312</td>\n",
       "      <td>3.181641</td>\n",
       "      <td>-1.394531</td>\n",
       "      <td>-2.490234</td>\n",
       "      <td>-1.065430</td>\n",
       "      <td>-0.442627</td>\n",
       "      <td>-1.672852</td>\n",
       "      <td>2.144531</td>\n",
       "      <td>-0.656738</td>\n",
       "      <td>0.265137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070984</td>\n",
       "      <td>0.033417</td>\n",
       "      <td>0.051117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199991</th>\n",
       "      <td>249991</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10</td>\n",
       "      <td>20160403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>72277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>500.0</td>\n",
       "      <td>20031106</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>45.96875</td>\n",
       "      <td>-3.212891</td>\n",
       "      <td>1.997070</td>\n",
       "      <td>-2.519531</td>\n",
       "      <td>2.419922</td>\n",
       "      <td>-1.568359</td>\n",
       "      <td>0.269043</td>\n",
       "      <td>0.245117</td>\n",
       "      <td>-1.666992</td>\n",
       "      <td>-1.884766</td>\n",
       "      <td>0.268066</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.137573</td>\n",
       "      <td>0.086853</td>\n",
       "      <td>0.002478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199992</th>\n",
       "      <td>249992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>20160331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>29738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>500.0</td>\n",
       "      <td>20071009</td>\n",
       "      <td>3929</td>\n",
       "      <td>0</td>\n",
       "      <td>43.37500</td>\n",
       "      <td>3.027344</td>\n",
       "      <td>-2.238281</td>\n",
       "      <td>2.027344</td>\n",
       "      <td>-1.245117</td>\n",
       "      <td>2.113281</td>\n",
       "      <td>1.605469</td>\n",
       "      <td>0.086121</td>\n",
       "      <td>0.496582</td>\n",
       "      <td>1.914062</td>\n",
       "      <td>0.230103</td>\n",
       "      <td>0.087463</td>\n",
       "      <td>0.096252</td>\n",
       "      <td>0.047455</td>\n",
       "      <td>0.116638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199993</th>\n",
       "      <td>249993</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>20160328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>193</td>\n",
       "      <td>500.0</td>\n",
       "      <td>19970712</td>\n",
       "      <td>3258</td>\n",
       "      <td>0</td>\n",
       "      <td>44.71875</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>-3.392578</td>\n",
       "      <td>1.723633</td>\n",
       "      <td>-1.783203</td>\n",
       "      <td>-2.158203</td>\n",
       "      <td>-0.417480</td>\n",
       "      <td>-0.223511</td>\n",
       "      <td>2.148438</td>\n",
       "      <td>-1.631836</td>\n",
       "      <td>0.266113</td>\n",
       "      <td>0.100708</td>\n",
       "      <td>0.136597</td>\n",
       "      <td>0.018753</td>\n",
       "      <td>0.018051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199994</th>\n",
       "      <td>249994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>20160330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>41919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>500.0</td>\n",
       "      <td>20050807</td>\n",
       "      <td>5640</td>\n",
       "      <td>0</td>\n",
       "      <td>46.40625</td>\n",
       "      <td>2.798828</td>\n",
       "      <td>-3.435547</td>\n",
       "      <td>-0.013077</td>\n",
       "      <td>1.506836</td>\n",
       "      <td>-0.947266</td>\n",
       "      <td>0.359863</td>\n",
       "      <td>0.323975</td>\n",
       "      <td>-1.174805</td>\n",
       "      <td>-1.170898</td>\n",
       "      <td>0.267090</td>\n",
       "      <td>0.085876</td>\n",
       "      <td>0.109436</td>\n",
       "      <td>0.067200</td>\n",
       "      <td>0.024033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>249995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>20160309</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>500.0</td>\n",
       "      <td>20041005</td>\n",
       "      <td>5564</td>\n",
       "      <td>0</td>\n",
       "      <td>46.31250</td>\n",
       "      <td>-3.304688</td>\n",
       "      <td>2.072266</td>\n",
       "      <td>-2.531250</td>\n",
       "      <td>1.716797</td>\n",
       "      <td>-1.063477</td>\n",
       "      <td>0.326660</td>\n",
       "      <td>0.073364</td>\n",
       "      <td>-0.622559</td>\n",
       "      <td>-0.778320</td>\n",
       "      <td>0.263672</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.141846</td>\n",
       "      <td>0.076416</td>\n",
       "      <td>0.039276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>249996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20160323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>152834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>500.0</td>\n",
       "      <td>20130409</td>\n",
       "      <td>5220</td>\n",
       "      <td>0</td>\n",
       "      <td>48.09375</td>\n",
       "      <td>-3.318359</td>\n",
       "      <td>1.358398</td>\n",
       "      <td>-3.291016</td>\n",
       "      <td>4.269531</td>\n",
       "      <td>0.140503</td>\n",
       "      <td>0.556152</td>\n",
       "      <td>0.965820</td>\n",
       "      <td>-2.671875</td>\n",
       "      <td>0.357422</td>\n",
       "      <td>0.255371</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.155884</td>\n",
       "      <td>0.108398</td>\n",
       "      <td>0.067871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>249997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>20160316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>132531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>500.0</td>\n",
       "      <td>20041211</td>\n",
       "      <td>3795</td>\n",
       "      <td>0</td>\n",
       "      <td>46.15625</td>\n",
       "      <td>-3.304688</td>\n",
       "      <td>2.166016</td>\n",
       "      <td>-2.417969</td>\n",
       "      <td>1.371094</td>\n",
       "      <td>-1.073242</td>\n",
       "      <td>0.270508</td>\n",
       "      <td>-0.015282</td>\n",
       "      <td>-0.288330</td>\n",
       "      <td>-0.687012</td>\n",
       "      <td>0.262939</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.141846</td>\n",
       "      <td>0.071960</td>\n",
       "      <td>0.042969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>249998</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20160327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>143405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>500.0</td>\n",
       "      <td>20020702</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>45.50000</td>\n",
       "      <td>-3.197266</td>\n",
       "      <td>2.029297</td>\n",
       "      <td>-2.939453</td>\n",
       "      <td>0.568848</td>\n",
       "      <td>-1.717773</td>\n",
       "      <td>0.316406</td>\n",
       "      <td>-1.141602</td>\n",
       "      <td>-0.434814</td>\n",
       "      <td>-1.844727</td>\n",
       "      <td>0.282227</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.067505</td>\n",
       "      <td>0.067505</td>\n",
       "      <td>0.009003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>249999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>20160401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>78202</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>20090708</td>\n",
       "      <td>4158</td>\n",
       "      <td>0</td>\n",
       "      <td>44.28125</td>\n",
       "      <td>4.179688</td>\n",
       "      <td>-3.689453</td>\n",
       "      <td>2.033203</td>\n",
       "      <td>0.109131</td>\n",
       "      <td>2.203125</td>\n",
       "      <td>0.847656</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>-0.775879</td>\n",
       "      <td>1.790039</td>\n",
       "      <td>0.231445</td>\n",
       "      <td>0.103943</td>\n",
       "      <td>0.096008</td>\n",
       "      <td>0.062317</td>\n",
       "      <td>0.110168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SaleID  bodyType  brand  creatDate  fuelType  gearbox  kilometer  \\\n",
       "199990  249990       3.0      5   20160331       0.0      0.0       15.0   \n",
       "199991  249991       2.0     10   20160403       0.0      1.0       15.0   \n",
       "199992  249992       1.0      6   20160331       0.0      0.0        8.0   \n",
       "199993  249993       2.0      4   20160328       0.0      1.0       15.0   \n",
       "199994  249994       0.0      4   20160330       0.0      0.0       15.0   \n",
       "199995  249995       0.0      4   20160309       0.0      1.0       15.0   \n",
       "199996  249996       0.0      1   20160323       0.0      0.0        4.0   \n",
       "199997  249997       0.0      4   20160316       0.0      1.0       12.5   \n",
       "199998  249998       4.0      1   20160327       0.0      1.0       15.0   \n",
       "199999  249999       1.0      8   20160401       0.0      0.0        3.0   \n",
       "\n",
       "        model    name  notRepairedDamage  offerType  power  price   regDate  \\\n",
       "199990   19.0   61395                0.0          0    132  500.0  19990906   \n",
       "199991   17.0   72277                0.0          0    224  500.0  20031106   \n",
       "199992   41.0   29738                0.0          0     60  500.0  20071009   \n",
       "199993   13.0      35                0.0          0    193  500.0  19970712   \n",
       "199994    4.0   41919                0.0          0    150  500.0  20050807   \n",
       "199995    4.0  111443                0.0          0    150  500.0  20041005   \n",
       "199996   65.0  152834                0.0          0    179  500.0  20130409   \n",
       "199997    4.0  132531                0.0          0    147  500.0  20041211   \n",
       "199998   40.0  143405                0.0          0    176  500.0  20020702   \n",
       "199999   32.0   78202                0.0          0      0  500.0  20090708   \n",
       "\n",
       "        regionCode  seller       v_0       v_1      v_10      v_11      v_12  \\\n",
       "199990        3160       0  43.09375 -3.195312  3.181641 -1.394531 -2.490234   \n",
       "199991         163       0  45.96875 -3.212891  1.997070 -2.519531  2.419922   \n",
       "199992        3929       0  43.37500  3.027344 -2.238281  2.027344 -1.245117   \n",
       "199993        3258       0  44.71875  3.812500 -3.392578  1.723633 -1.783203   \n",
       "199994        5640       0  46.40625  2.798828 -3.435547 -0.013077  1.506836   \n",
       "199995        5564       0  46.31250 -3.304688  2.072266 -2.531250  1.716797   \n",
       "199996        5220       0  48.09375 -3.318359  1.358398 -3.291016  4.269531   \n",
       "199997        3795       0  46.15625 -3.304688  2.166016 -2.417969  1.371094   \n",
       "199998          61       0  45.50000 -3.197266  2.029297 -2.939453  0.568848   \n",
       "199999        4158       0  44.28125  4.179688 -3.689453  2.033203  0.109131   \n",
       "\n",
       "            v_13      v_14       v_2       v_3       v_4       v_5       v_6  \\\n",
       "199990 -1.065430 -0.442627 -1.672852  2.144531 -0.656738  0.265137  0.000000   \n",
       "199991 -1.568359  0.269043  0.245117 -1.666992 -1.884766  0.268066  0.000387   \n",
       "199992  2.113281  1.605469  0.086121  0.496582  1.914062  0.230103  0.087463   \n",
       "199993 -2.158203 -0.417480 -0.223511  2.148438 -1.631836  0.266113  0.100708   \n",
       "199994 -0.947266  0.359863  0.323975 -1.174805 -1.170898  0.267090  0.085876   \n",
       "199995 -1.063477  0.326660  0.073364 -0.622559 -0.778320  0.263672  0.000292   \n",
       "199996  0.140503  0.556152  0.965820 -2.671875  0.357422  0.255371  0.000991   \n",
       "199997 -1.073242  0.270508 -0.015282 -0.288330 -0.687012  0.262939  0.000318   \n",
       "199998 -1.717773  0.316406 -1.141602 -0.434814 -1.844727  0.282227  0.000023   \n",
       "199999  2.203125  0.847656  0.546875 -0.775879  1.790039  0.231445  0.103943   \n",
       "\n",
       "             v_7       v_8       v_9  \n",
       "199990  0.070984  0.033417  0.051117  \n",
       "199991  0.137573  0.086853  0.002478  \n",
       "199992  0.096252  0.047455  0.116638  \n",
       "199993  0.136597  0.018753  0.018051  \n",
       "199994  0.109436  0.067200  0.024033  \n",
       "199995  0.141846  0.076416  0.039276  \n",
       "199996  0.155884  0.108398  0.067871  \n",
       "199997  0.141846  0.071960  0.042969  \n",
       "199998  0.067505  0.067505  0.009003  \n",
       "199999  0.096008  0.062317  0.110168  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:49:09.322856Z",
     "start_time": "2020-04-26T13:49:00.145203Z"
    }
   },
   "outputs": [],
   "source": [
    "# 处理异常值\n",
    "concat_data['power'][concat_data['power'] > 600] = 600\n",
    "concat_data['power'][concat_data['power'] < 1] = 1\n",
    "\n",
    "concat_data['v_13'][concat_data['v_13'] > 6] = 6\n",
    "concat_data['v_14'][concat_data['v_14'] > 4] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:49:10.601354Z",
     "start_time": "2020-04-26T13:49:09.322856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 353)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v系列特征之间相加\n",
    "for j in ['v_' + str(i) for i in range(14)]:\n",
    "    for k in ['v_' + str(m) for m in range(14)]:\n",
    "        concat_data[j + '+' + k] = concat_data[j] + concat_data[k]\n",
    "\n",
    "# 原始特征与v系列特征之间相乘\n",
    "for i in ['model', 'brand', 'bodyType', 'fuelType', 'gearbox', 'power', 'kilometer', 'notRepairedDamage', 'regionCode']:\n",
    "    for j in ['v_' + str(k) for k in range(14)]:\n",
    "        concat_data[i + '*' + j] = concat_data[i] * concat_data[j]\n",
    "    \n",
    "concat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:49:11.760791Z",
     "start_time": "2020-04-26T13:49:10.603354Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200000, 361)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提取日期信息\n",
    "date_cols = ['regDate', 'creatDate']\n",
    "concat_data = date_tran(concat_data, date_cols)\n",
    "\n",
    "concat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:49:12.058612Z",
     "start_time": "2020-04-26T13:49:11.762781Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SaleID', 'bodyType', 'brand', 'creatDate', 'fuelType', 'gearbox',\n",
       "       'kilometer', 'model', 'name', 'notRepairedDamage',\n",
       "       ...\n",
       "       'regionCode*v_12', 'regionCode*v_13', 'regDate_year', 'regDate_month',\n",
       "       'regDate_day', 'regDate_dayofweek', 'creatDate_year', 'creatDate_month',\n",
       "       'creatDate_day', 'creatDate_dayofweek'],\n",
       "      dtype='object', length=361)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = concat_data.copy()\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:49:12.347444Z",
     "start_time": "2020-04-26T13:49:12.060610Z"
    }
   },
   "outputs": [],
   "source": [
    "# count编码\n",
    "count_list = ['regDate', 'creatDate', 'model', 'brand', 'regionCode', 'bodyType', 'fuelType', 'name',\n",
    "              'regDate_year', 'regDate_month', 'regDate_day', 'regDate_dayofweek',\n",
    "              'creatDate_month', 'creatDate_day', 'creatDate_dayofweek', 'kilometer']\n",
    "data = count_coding(data, count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:49:12.479521Z",
     "start_time": "2020-04-26T13:49:12.349444Z"
    }
   },
   "outputs": [],
   "source": [
    "# 特征构造\n",
    "# 使用时间：data['creatDate'] - data['regDate']，反应汽车使用时间，一般来说价格与使用时间成反比\n",
    "# 不过要注意，数据里有时间出错的格式，所以我们需要 errors='coerce'\n",
    "data['used_time1'] = (pd.to_datetime(data['creatDate'], format='%Y%m%d', errors='coerce') - \n",
    "                      pd.to_datetime(data['regDate'], format='%Y%m%d', errors='coerce')).dt.days\n",
    "data['used_time2'] = (pd.datetime.now() - pd.to_datetime(data['regDate'], format='%Y%m%d', errors='coerce')).dt.days                        \n",
    "data['used_time3'] = (pd.datetime.now() - pd.to_datetime(data['creatDate'], format='%Y%m%d', errors='coerce') ).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:49:12.574159Z",
     "start_time": "2020-04-26T13:49:12.481210Z"
    }
   },
   "outputs": [],
   "source": [
    "# 分桶，注意：kilometer应该是已经离散化了的\n",
    "cut_cols = ['power'] + ['used_time1', 'used_time2', 'used_time3']\n",
    "data = cut_group(data, cut_cols, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:49:27.052070Z",
     "start_time": "2020-04-26T13:49:12.576156Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\n",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]\n",
      " 14%|████████████                                                                        | 1/7 [00:01<00:09,  1.60s/it]\n",
      " 29%|████████████████████████                                                            | 2/7 [00:02<00:06,  1.25s/it]\n",
      " 43%|████████████████████████████████████                                                | 3/7 [00:02<00:04,  1.00s/it]\n",
      " 57%|████████████████████████████████████████████████                                    | 4/7 [00:02<00:02,  1.17it/s]\n",
      " 71%|████████████████████████████████████████████████████████████                        | 5/7 [00:03<00:01,  1.38it/s]\n",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [00:03<00:00,  1.56it/s]\n",
      " 33%|████████████████████████████                                                        | 1/3 [00:04<00:08,  4.35s/it]\n",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]\n",
      " 14%|████████████                                                                        | 1/7 [00:01<00:11,  1.95s/it]\n",
      " 29%|████████████████████████                                                            | 2/7 [00:02<00:07,  1.49s/it]\n",
      " 43%|████████████████████████████████████                                                | 3/7 [00:02<00:04,  1.17s/it]\n",
      " 57%|████████████████████████████████████████████████                                    | 4/7 [00:03<00:02,  1.05it/s]\n",
      " 71%|████████████████████████████████████████████████████████████                        | 5/7 [00:03<00:01,  1.25it/s]\n",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [00:04<00:00,  1.45it/s]\n",
      " 67%|████████████████████████████████████████████████████████                            | 2/3 [00:09<00:04,  4.49s/it]\n",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]\n",
      " 14%|████████████                                                                        | 1/7 [00:02<00:13,  2.27s/it]\n",
      " 29%|████████████████████████                                                            | 2/7 [00:02<00:08,  1.71s/it]\n",
      " 43%|████████████████████████████████████                                                | 3/7 [00:03<00:05,  1.33s/it]\n",
      " 57%|████████████████████████████████████████████████                                    | 4/7 [00:03<00:03,  1.06s/it]\n",
      " 71%|████████████████████████████████████████████████████████████                        | 5/7 [00:03<00:01,  1.15it/s]\n",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [00:04<00:00,  1.32it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.71s/it]\n"
     ]
    }
   ],
   "source": [
    "# 用数值特征对类别特征做统计刻画，随便挑了几个跟price相关性最高的匿名特征\n",
    "cross_cat = ['model', 'brand','regDate_year']\n",
    "cross_num = ['v_0','v_3', 'v_4', 'v_8', 'v_12','power', 'used_time1']\n",
    "data = cross_cat_num(data, cross_num, cross_cat) # 一阶交叉\n",
    "# data = cross_qua_cat_num(data) # 二阶交叉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:49:27.066066Z",
     "start_time": "2020-04-26T13:49:27.057071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SaleID', 'bodyType', 'brand', 'creatDate', 'fuelType', 'gearbox',\n",
      "       'kilometer', 'model', 'name', 'notRepairedDamage',\n",
      "       ...\n",
      "       'regDate_year_v_8_median', 'regDate_year_v_12_max',\n",
      "       'regDate_year_v_12_min', 'regDate_year_v_12_median',\n",
      "       'regDate_year_power_max', 'regDate_year_power_min',\n",
      "       'regDate_year_power_median', 'regDate_year_used_time1_max',\n",
      "       'regDate_year_used_time1_min', 'regDate_year_used_time1_median'],\n",
      "      dtype='object', length=447)\n"
     ]
    }
   ],
   "source": [
    "# 选择特征列\n",
    "numerical_cols = data.columns\n",
    "print(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:49:27.712669Z",
     "start_time": "2020-04-26T13:49:27.071060Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_fea = ['SaleID', 'offerType', 'seller']\n",
    "feature_cols = [col for col in numerical_cols if col not in cat_fea]\n",
    "feature_cols = [col for col in feature_cols if col not in ['price']]\n",
    "\n",
    "# 将训练集和测试集分开\n",
    "X_data = data.iloc[:len(Train_data), :][feature_cols]\n",
    "Y_data = Train_data['price']\n",
    "X_test  = data.iloc[len(Train_data):, :][feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:49:27.762331Z",
     "start_time": "2020-04-26T13:49:27.712669Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from itertools import product\n",
    "class MeanEncoder:\n",
    "    def __init__(self, categorical_features, n_splits=10, target_type='classification', prior_weight_func=None):\n",
    "        \"\"\"\n",
    "        :param categorical_features: list of str, the name of the categorical columns to encode\n",
    " \n",
    "        :param n_splits: the number of splits used in mean encoding\n",
    " \n",
    "        :param target_type: str, 'regression' or 'classification'\n",
    " \n",
    "        :param prior_weight_func:\n",
    "        a function that takes in the number of observations, and outputs prior weight\n",
    "        when a dict is passed, the default exponential decay function will be used:\n",
    "        k: the number of observations needed for the posterior to be weighted equally as the prior\n",
    "        f: larger f --> smaller slope\n",
    "        \"\"\"\n",
    " \n",
    "        self.categorical_features = categorical_features\n",
    "        self.n_splits = n_splits\n",
    "        self.learned_stats = {}\n",
    " \n",
    "        if target_type == 'classification':\n",
    "            self.target_type = target_type\n",
    "            self.target_values = []\n",
    "        else:\n",
    "            self.target_type = 'regression'\n",
    "            self.target_values = None\n",
    " \n",
    "        if isinstance(prior_weight_func, dict):\n",
    "            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n",
    "        elif callable(prior_weight_func):\n",
    "            self.prior_weight_func = prior_weight_func\n",
    "        else:\n",
    "            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n",
    " \n",
    "    @staticmethod\n",
    "    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n",
    "        X_train = X_train[[variable]].copy()\n",
    "        X_test = X_test[[variable]].copy()\n",
    " \n",
    "        if target is not None:\n",
    "            nf_name = '{}_pred_{}'.format(variable, target)\n",
    "            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n",
    "        else:\n",
    "            nf_name = '{}_pred'.format(variable)\n",
    "            X_train['pred_temp'] = y_train  # regression\n",
    "        prior = X_train['pred_temp'].mean()\n",
    " \n",
    "        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg(['mean', 'size'])\n",
    "        col_avg_y['size'] = prior_weight_func(col_avg_y['size'])\n",
    "        col_avg_y[nf_name] = col_avg_y['size'] * prior + (1 - col_avg_y['size']) * col_avg_y['mean']\n",
    "        col_avg_y.drop(['size', 'mean'], axis=1, inplace=True)\n",
    " \n",
    "        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n",
    "        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n",
    " \n",
    "        return nf_train, nf_test, prior, col_avg_y\n",
    " \n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :param y: pandas Series or numpy array, n_samples\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "        if self.target_type == 'classification':\n",
    "            skf = StratifiedKFold(self.n_splits)\n",
    "        else:\n",
    "            skf = KFold(self.n_splits)\n",
    " \n",
    "        if self.target_type == 'classification':\n",
    "            self.target_values = sorted(set(y))\n",
    "            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n",
    "                                  product(self.categorical_features, self.target_values)}\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(X, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        else:\n",
    "            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(X, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        return X_new\n",
    " \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    " \n",
    "        if self.target_type == 'classification':\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "        else:\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    " \n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:49:38.178559Z",
     "start_time": "2020-04-26T13:49:27.770326Z"
    }
   },
   "outputs": [],
   "source": [
    "class_list = ['model', 'brand', 'name', 'regionCode'] + date_cols\n",
    "MeanEncodeFeature = class_list\n",
    "ME = MeanEncoder(categorical_features=MeanEncodeFeature, n_splits=5, target_type='regression', prior_weight_func=None)\n",
    "X_data = ME.fit_transform(X_data, Y_data)\n",
    "X_test = ME.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:49:38.187781Z",
     "start_time": "2020-04-26T13:49:38.178559Z"
    }
   },
   "outputs": [],
   "source": [
    "X_data['price'] = Train_data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:50:07.188417Z",
     "start_time": "2020-04-26T13:49:38.190779Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:28<00:00,  4.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# target encoding目标编码，回归场景相对来说做目标编码的选择更多，不仅可以做均值编码，还可以做标准差编码、中位数编码等\n",
    "enc_cols = []\n",
    "stats_default_dict = {\n",
    "    'max': X_data['price'].max(),\n",
    "    'min': X_data['price'].min(),\n",
    "    'median': X_data['price'].median(),\n",
    "    'mean': X_data['price'].mean(),\n",
    "    'sum': X_data['price'].sum(),\n",
    "    'std': X_data['price'].std(),\n",
    "    'skew': X_data['price'].skew(), # 偏度\n",
    "    'kurt': X_data['price'].kurt(), # 峰度\n",
    "    'mad': X_data['price'].mad() # mean absolute deviation 平均绝对偏差\n",
    "}\n",
    "\n",
    "# 暂且选择这三种编码\n",
    "enc_stats = ['max', 'min', 'mean']\n",
    "skf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for f in tqdm(['regionCode', 'brand', 'regDate_year' ,'creatDate_year', 'kilometer', 'model']):\n",
    "    enc_dict = {}\n",
    "    for stat in enc_stats:\n",
    "        enc_dict['{}_target_{}'.format(f, stat)] = stat\n",
    "        X_data['{}_target_{}'.format(f, stat)] = 0\n",
    "        X_test['{}_target_{}'.format(f, stat)] = 0\n",
    "        enc_cols.append('{}_target_{}'.format(f, stat))\n",
    "    for i, (trn_idx, val_idx) in enumerate(skf.split(X_data, Y_data)):\n",
    "        trn_x, val_x = X_data.iloc[trn_idx].reset_index(drop=True), X_data.iloc[val_idx].reset_index(drop=True)\n",
    "        enc_df = trn_x.groupby(f, as_index=False)['price'].agg(enc_dict)\n",
    "        val_x = val_x[[f]].merge(enc_df, on=f, how='left')\n",
    "        test_x = X_test[[f]].merge(enc_df, on=f, how='left')\n",
    "        for stat in enc_stats:\n",
    "            val_x['{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].fillna(stats_default_dict[stat])\n",
    "            test_x['{}_target_{}'.format(f, stat)] = test_x['{}_target_{}'.format(f, stat)].fillna(stats_default_dict[stat])\n",
    "            X_data.loc[val_idx, '{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].values \n",
    "            X_test['{}_target_{}'.format(f, stat)] += test_x['{}_target_{}'.format(f, stat)].values / skf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:50:07.482248Z",
     "start_time": "2020-04-26T13:50:07.190416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 463)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_list = ['regDate', 'creatDate', 'brand_power_min', 'regDate_year_power_min']\n",
    "x_train = X_data.drop(drop_list + ['price'], axis=1)\n",
    "x_test = X_test.drop(drop_list, axis=1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:50:10.215306Z",
     "start_time": "2020-04-26T13:50:07.484247Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:50:12.026373Z",
     "start_time": "2020-04-26T13:50:10.215306Z"
    }
   },
   "outputs": [],
   "source": [
    "# 特征归一化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(pd.concat([x_train, x_test]).values)\n",
    "all_data = min_max_scaler.transform(pd.concat([x_train, x_test]).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:50:28.290994Z",
     "start_time": "2020-04-26T13:50:12.029371Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=146)\n",
    "all_pca = pca.fit_transform(all_data)\n",
    "X_pca = all_pca[:len(x_train)]\n",
    "test = all_pca[len(x_train):]\n",
    "y = Train_data['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:50:31.527528Z",
     "start_time": "2020-04-26T13:50:28.290994Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, Activation, MaxPool1D, Flatten, Dense\n",
    "from keras.layers import Input, Dense, Concatenate, Reshape, Dropout, merge, Add\n",
    "def NN_model(input_dim):\n",
    "    init = keras.initializers.glorot_uniform(seed=1)\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Dense(units=300, input_dim=input_dim, kernel_initializer=init, activation='softplus'))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=300, kernel_initializer=init, activation='softplus'))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=64, kernel_initializer=init, activation='softplus'))\n",
    "    model.add(Dense(units=32, kernel_initializer=init, activation='softplus'))\n",
    "    model.add(Dense(units=8, kernel_initializer=init, activation='softplus'))\n",
    "    model.add(Dense(units=1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:50:31.547504Z",
     "start_time": "2020-04-26T13:50:31.529511Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, EarlyStopping\n",
    "class Metric(Callback):\n",
    "    def __init__(self, model, callbacks, data):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.callbacks = callbacks\n",
    "        self.data = data\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_train_begin(logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_train_end(logs)\n",
    "\n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "        X_train, y_train = self.data[0][0], self.data[0][1]\n",
    "        y_pred3 = self.model.predict(X_train)\n",
    "        y_pred = np.zeros((len(y_pred3), ))\n",
    "        y_true = np.zeros((len(y_pred3), ))\n",
    "        for i in range(len(y_pred3)):\n",
    "            y_pred[i] = y_pred3[i]\n",
    "        for i in range(len(y_pred3)):\n",
    "            y_true[i] = y_train[i]\n",
    "        trn_s = mean_absolute_error(y_true, y_pred)\n",
    "        logs['trn_score'] = trn_s\n",
    "        \n",
    "        X_val, y_val = self.data[1][0], self.data[1][1]\n",
    "        y_pred3 = self.model.predict(X_val)\n",
    "        y_pred = np.zeros((len(y_pred3), ))\n",
    "        y_true = np.zeros((len(y_pred3), ))\n",
    "        for i in range(len(y_pred3)):\n",
    "            y_pred[i] = y_pred3[i]\n",
    "        for i in range(len(y_pred3)):\n",
    "            y_true[i] = y_val[i]\n",
    "        val_s = mean_absolute_error(y_true, y_pred)\n",
    "        logs['val_score'] = val_s\n",
    "        print('trn_score', trn_s, 'val_score', val_s)\n",
    "\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_epoch_end(batch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T13:50:31.561494Z",
     "start_time": "2020-04-26T13:50:31.552501Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "  \n",
    "def scheduler(epoch):\n",
    "    # 每隔100个epoch，学习率减小为原来的1/10\n",
    "    if epoch % 20 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.6)\n",
    "        print(\"lr changed to {}\".format(lr * 0.6))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "reduce_lr = LearningRateScheduler(scheduler)\n",
    "# model.fit(train_x, train_y, batch_size=32, epochs=5, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T14:45:07.670228Z",
     "start_time": "2020-04-26T13:50:31.567495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "WARNING:tensorflow:From C:\\Users\\z\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\z\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 125000 samples, validate on 25000 samples\n",
      "Epoch 1/145\n",
      " - 4s - loss: 2543.2775 - mae: 2543.2776 - val_loss: 982.1495 - val_mae: 982.1495\n",
      "Epoch 2/145\n",
      " - 4s - loss: 828.4164 - mae: 828.4164 - val_loss: 712.2494 - val_mae: 712.2494\n",
      "Epoch 3/145\n",
      " - 3s - loss: 695.1878 - mae: 695.1879 - val_loss: 619.7988 - val_mae: 619.7987\n",
      "Epoch 4/145\n",
      " - 3s - loss: 631.8862 - mae: 631.8863 - val_loss: 590.0787 - val_mae: 590.0788\n",
      "Epoch 5/145\n",
      " - 3s - loss: 628.2687 - mae: 628.2688 - val_loss: 653.2979 - val_mae: 653.2980\n",
      "Epoch 6/145\n",
      " - 3s - loss: 645.8541 - mae: 645.8541 - val_loss: 591.9103 - val_mae: 591.9103\n",
      "Epoch 7/145\n",
      " - 3s - loss: 566.0573 - mae: 566.0572 - val_loss: 582.6709 - val_mae: 582.6709\n",
      "Epoch 8/145\n",
      " - 3s - loss: 623.5933 - mae: 623.5933 - val_loss: 572.7327 - val_mae: 572.7327\n",
      "Epoch 9/145\n",
      " - 3s - loss: 541.3239 - mae: 541.3239 - val_loss: 534.9603 - val_mae: 534.9603\n",
      "Epoch 10/145\n",
      " - 3s - loss: 525.9399 - mae: 525.9400 - val_loss: 514.2498 - val_mae: 514.2498\n",
      "Epoch 11/145\n",
      " - 4s - loss: 530.2441 - mae: 530.2441 - val_loss: 525.5604 - val_mae: 525.5605\n",
      "Epoch 12/145\n",
      " - 3s - loss: 526.2972 - mae: 526.2972 - val_loss: 523.9621 - val_mae: 523.9621\n",
      "Epoch 13/145\n",
      " - 3s - loss: 512.5548 - mae: 512.5548 - val_loss: 501.9149 - val_mae: 501.9149\n",
      "Epoch 14/145\n",
      " - 4s - loss: 509.1708 - mae: 509.1707 - val_loss: 497.8773 - val_mae: 497.8773\n",
      "Epoch 15/145\n",
      " - 4s - loss: 525.8289 - mae: 525.8289 - val_loss: 482.7569 - val_mae: 482.7569\n",
      "Epoch 16/145\n",
      " - 4s - loss: 491.4656 - mae: 491.4656 - val_loss: 528.0990 - val_mae: 528.0991\n",
      "Epoch 17/145\n",
      " - 4s - loss: 511.5519 - mae: 511.5520 - val_loss: 514.6524 - val_mae: 514.6523\n",
      "Epoch 18/145\n",
      " - 6s - loss: 504.7354 - mae: 504.7352 - val_loss: 500.7430 - val_mae: 500.7430\n",
      "Epoch 19/145\n",
      " - 6s - loss: 492.1232 - mae: 492.1232 - val_loss: 466.5905 - val_mae: 466.5905\n",
      "Epoch 20/145\n",
      " - 5s - loss: 476.3042 - mae: 476.3042 - val_loss: 486.7871 - val_mae: 486.7871\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      " - 4s - loss: 458.4716 - mae: 458.4716 - val_loss: 458.0553 - val_mae: 458.0552\n",
      "Epoch 22/145\n",
      " - 4s - loss: 447.7478 - mae: 447.7479 - val_loss: 450.7820 - val_mae: 450.7820\n",
      "Epoch 23/145\n",
      " - 3s - loss: 448.2619 - mae: 448.2620 - val_loss: 461.6666 - val_mae: 461.6666\n",
      "Epoch 24/145\n",
      " - 4s - loss: 451.2162 - mae: 451.2162 - val_loss: 458.9594 - val_mae: 458.9594\n",
      "Epoch 25/145\n",
      " - 6s - loss: 444.4269 - mae: 444.4269 - val_loss: 458.4848 - val_mae: 458.4848\n",
      "Epoch 26/145\n",
      " - 4s - loss: 454.7045 - mae: 454.7045 - val_loss: 551.6351 - val_mae: 551.6351\n",
      "Epoch 27/145\n",
      " - 4s - loss: 478.6888 - mae: 478.6888 - val_loss: 463.0713 - val_mae: 463.0713\n",
      "Epoch 28/145\n",
      " - 4s - loss: 442.5277 - mae: 442.5277 - val_loss: 452.3354 - val_mae: 452.3354\n",
      "Epoch 29/145\n",
      " - 3s - loss: 443.0082 - mae: 443.0083 - val_loss: 450.4675 - val_mae: 450.4675\n",
      "Epoch 30/145\n",
      " - 4s - loss: 439.2661 - mae: 439.2661 - val_loss: 448.7954 - val_mae: 448.7954\n",
      "Epoch 31/145\n",
      " - 3s - loss: 435.9034 - mae: 435.9034 - val_loss: 460.9203 - val_mae: 460.9203\n",
      "Epoch 32/145\n",
      " - 3s - loss: 438.1784 - mae: 438.1784 - val_loss: 447.2660 - val_mae: 447.2661\n",
      "Epoch 33/145\n",
      " - 4s - loss: 440.4273 - mae: 440.4273 - val_loss: 468.4152 - val_mae: 468.4153\n",
      "Epoch 34/145\n",
      " - 4s - loss: 435.4901 - mae: 435.4901 - val_loss: 446.1349 - val_mae: 446.1349\n",
      "Epoch 35/145\n",
      " - 4s - loss: 438.5125 - mae: 438.5125 - val_loss: 486.8919 - val_mae: 486.8918\n",
      "Epoch 36/145\n",
      " - 3s - loss: 438.0790 - mae: 438.0790 - val_loss: 453.5467 - val_mae: 453.5466\n",
      "Epoch 37/145\n",
      " - 3s - loss: 438.0250 - mae: 438.0250 - val_loss: 445.0906 - val_mae: 445.0906\n",
      "Epoch 38/145\n",
      " - 4s - loss: 430.1049 - mae: 430.1049 - val_loss: 444.5422 - val_mae: 444.5421\n",
      "Epoch 39/145\n",
      " - 4s - loss: 430.4019 - mae: 430.4019 - val_loss: 451.0908 - val_mae: 451.0909\n",
      "Epoch 40/145\n",
      " - 4s - loss: 445.5298 - mae: 445.5298 - val_loss: 457.1758 - val_mae: 457.1758\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      " - 4s - loss: 419.2854 - mae: 419.2854 - val_loss: 436.3844 - val_mae: 436.3844\n",
      "Epoch 42/145\n",
      " - 4s - loss: 415.0857 - mae: 415.0856 - val_loss: 432.2885 - val_mae: 432.2885\n",
      "Epoch 43/145\n",
      " - 3s - loss: 416.9954 - mae: 416.9955 - val_loss: 438.3179 - val_mae: 438.3178\n",
      "Epoch 44/145\n",
      " - 3s - loss: 414.6516 - mae: 414.6516 - val_loss: 443.0922 - val_mae: 443.0922\n",
      "Epoch 45/145\n",
      " - 3s - loss: 412.2561 - mae: 412.2561 - val_loss: 435.5802 - val_mae: 435.5802\n",
      "Epoch 46/145\n",
      " - 3s - loss: 413.2255 - mae: 413.2256 - val_loss: 433.3374 - val_mae: 433.3373\n",
      "Epoch 47/145\n",
      " - 3s - loss: 416.4827 - mae: 416.4828 - val_loss: 437.5410 - val_mae: 437.5410\n",
      "Epoch 48/145\n",
      " - 4s - loss: 411.1414 - mae: 411.1414 - val_loss: 438.3813 - val_mae: 438.3813\n",
      "Epoch 49/145\n",
      " - 3s - loss: 432.8332 - mae: 432.8333 - val_loss: 475.1180 - val_mae: 475.1180\n",
      "Epoch 50/145\n",
      " - 3s - loss: 418.6859 - mae: 418.6859 - val_loss: 459.4751 - val_mae: 459.4750\n",
      "Epoch 51/145\n",
      " - 4s - loss: 409.4758 - mae: 409.4757 - val_loss: 438.2167 - val_mae: 438.2167\n",
      "Epoch 52/145\n",
      " - 4s - loss: 409.3017 - mae: 409.3018 - val_loss: 429.0512 - val_mae: 429.0513\n",
      "Epoch 53/145\n",
      " - 4s - loss: 409.5830 - mae: 409.5830 - val_loss: 433.6828 - val_mae: 433.6829\n",
      "Epoch 54/145\n",
      " - 4s - loss: 406.7002 - mae: 406.7002 - val_loss: 433.5147 - val_mae: 433.5147\n",
      "Epoch 55/145\n",
      " - 3s - loss: 406.3156 - mae: 406.3157 - val_loss: 431.5533 - val_mae: 431.5534\n",
      "Epoch 56/145\n",
      " - 3s - loss: 406.4380 - mae: 406.4380 - val_loss: 430.3511 - val_mae: 430.3511\n",
      "Epoch 57/145\n",
      " - 3s - loss: 405.1056 - mae: 405.1056 - val_loss: 429.9750 - val_mae: 429.9750\n",
      "Epoch 58/145\n",
      " - 3s - loss: 407.6929 - mae: 407.6930 - val_loss: 447.6100 - val_mae: 447.6100\n",
      "Epoch 59/145\n",
      " - 3s - loss: 415.0700 - mae: 415.0701 - val_loss: 435.8906 - val_mae: 435.8906\n",
      "Epoch 60/145\n",
      " - 3s - loss: 406.1733 - mae: 406.1732 - val_loss: 432.8096 - val_mae: 432.8096\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      " - 3s - loss: 399.0002 - mae: 399.0002 - val_loss: 425.6222 - val_mae: 425.6222\n",
      "Epoch 62/145\n",
      " - 3s - loss: 396.3903 - mae: 396.3903 - val_loss: 429.7987 - val_mae: 429.7987\n",
      "Epoch 63/145\n",
      " - 3s - loss: 394.3361 - mae: 394.3362 - val_loss: 429.0824 - val_mae: 429.0823\n",
      "Epoch 64/145\n",
      " - 4s - loss: 394.3269 - mae: 394.3268 - val_loss: 426.2136 - val_mae: 426.2135\n",
      "Epoch 65/145\n",
      " - 4s - loss: 393.2317 - mae: 393.2317 - val_loss: 424.1441 - val_mae: 424.1440\n",
      "Epoch 66/145\n",
      " - 4s - loss: 395.0164 - mae: 395.0164 - val_loss: 428.8764 - val_mae: 428.8764\n",
      "Epoch 67/145\n",
      " - 4s - loss: 393.5123 - mae: 393.5123 - val_loss: 424.5316 - val_mae: 424.5316\n",
      "Epoch 68/145\n",
      " - 4s - loss: 392.3293 - mae: 392.3293 - val_loss: 425.9217 - val_mae: 425.9217\n",
      "Epoch 69/145\n",
      " - 4s - loss: 392.9745 - mae: 392.9745 - val_loss: 428.0243 - val_mae: 428.0243\n",
      "Epoch 70/145\n",
      " - 4s - loss: 392.2142 - mae: 392.2142 - val_loss: 429.8229 - val_mae: 429.8230\n",
      "Epoch 71/145\n",
      " - 4s - loss: 396.0516 - mae: 396.0515 - val_loss: 424.4166 - val_mae: 424.4166\n",
      "Epoch 72/145\n",
      " - 4s - loss: 391.8593 - mae: 391.8593 - val_loss: 433.7114 - val_mae: 433.7114\n",
      "Epoch 73/145\n",
      " - 3s - loss: 390.1002 - mae: 390.1002 - val_loss: 435.9938 - val_mae: 435.9938\n",
      "Epoch 74/145\n",
      " - 3s - loss: 393.9486 - mae: 393.9487 - val_loss: 428.7532 - val_mae: 428.7532\n",
      "Epoch 75/145\n",
      " - 3s - loss: 393.2112 - mae: 393.2111 - val_loss: 425.4539 - val_mae: 425.4539\n",
      "Epoch 76/145\n",
      " - 4s - loss: 390.5266 - mae: 390.5265 - val_loss: 423.4966 - val_mae: 423.4966\n",
      "Epoch 77/145\n",
      " - 4s - loss: 390.1361 - mae: 390.1362 - val_loss: 431.8931 - val_mae: 431.8932\n",
      "Epoch 78/145\n",
      " - 4s - loss: 389.1966 - mae: 389.1965 - val_loss: 426.2017 - val_mae: 426.2017\n",
      "Epoch 79/145\n",
      " - 4s - loss: 389.8784 - mae: 389.8784 - val_loss: 424.3444 - val_mae: 424.3444\n",
      "Epoch 80/145\n",
      " - 3s - loss: 389.1205 - mae: 389.1204 - val_loss: 426.4925 - val_mae: 426.4925\n",
      "Epoch 81/145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr changed to 0.0019439998548477888\n",
      " - 4s - loss: 383.8487 - mae: 383.8487 - val_loss: 421.6221 - val_mae: 421.6221\n",
      "Epoch 82/145\n",
      " - 3s - loss: 383.5858 - mae: 383.5858 - val_loss: 423.3402 - val_mae: 423.3402\n",
      "Epoch 83/145\n",
      " - 3s - loss: 383.1038 - mae: 383.1038 - val_loss: 422.6194 - val_mae: 422.6194\n",
      "Epoch 84/145\n",
      " - 4s - loss: 381.8845 - mae: 381.8846 - val_loss: 422.3480 - val_mae: 422.3480\n",
      "Epoch 85/145\n",
      " - 4s - loss: 384.9330 - mae: 384.9330 - val_loss: 421.7282 - val_mae: 421.7282\n",
      "Epoch 86/145\n",
      " - 4s - loss: 381.3084 - mae: 381.3085 - val_loss: 421.8086 - val_mae: 421.8086\n",
      "Epoch 87/145\n",
      " - 4s - loss: 380.9888 - mae: 380.9890 - val_loss: 422.0096 - val_mae: 422.0096\n",
      "Epoch 88/145\n",
      " - 4s - loss: 383.7313 - mae: 383.7313 - val_loss: 421.9014 - val_mae: 421.9014\n",
      "Epoch 89/145\n",
      " - 4s - loss: 381.1947 - mae: 381.1947 - val_loss: 429.5747 - val_mae: 429.5747\n",
      "Epoch 90/145\n",
      " - 4s - loss: 381.7575 - mae: 381.7574 - val_loss: 424.2885 - val_mae: 424.2885\n",
      "Epoch 91/145\n",
      " - 5s - loss: 381.0875 - mae: 381.0875 - val_loss: 423.6227 - val_mae: 423.6227\n",
      "Epoch 92/145\n",
      " - 4s - loss: 380.3750 - mae: 380.3750 - val_loss: 421.1351 - val_mae: 421.1350\n",
      "Epoch 93/145\n",
      " - 4s - loss: 381.3848 - mae: 381.3848 - val_loss: 426.1400 - val_mae: 426.1400\n",
      "Epoch 94/145\n",
      " - 5s - loss: 380.9282 - mae: 380.9283 - val_loss: 423.5808 - val_mae: 423.5807\n",
      "Epoch 95/145\n",
      " - 4s - loss: 380.9016 - mae: 380.9016 - val_loss: 420.9263 - val_mae: 420.9263\n",
      "Epoch 96/145\n",
      " - 4s - loss: 379.0962 - mae: 379.0962 - val_loss: 424.0442 - val_mae: 424.0442\n",
      "Epoch 97/145\n",
      " - 3s - loss: 380.6668 - mae: 380.6667 - val_loss: 422.7037 - val_mae: 422.7037\n",
      "Epoch 98/145\n",
      " - 4s - loss: 378.8423 - mae: 378.8423 - val_loss: 425.1688 - val_mae: 425.1688\n",
      "Epoch 99/145\n",
      " - 4s - loss: 377.2187 - mae: 377.2187 - val_loss: 420.3086 - val_mae: 420.3087\n",
      "Epoch 100/145\n",
      " - 4s - loss: 377.8423 - mae: 377.8423 - val_loss: 421.7724 - val_mae: 421.7724\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      " - 3s - loss: 375.3878 - mae: 375.3878 - val_loss: 420.0109 - val_mae: 420.0110\n",
      "Epoch 102/145\n",
      " - 3s - loss: 376.0256 - mae: 376.0256 - val_loss: 420.8809 - val_mae: 420.8809\n",
      "Epoch 103/145\n",
      " - 4s - loss: 374.2123 - mae: 374.2124 - val_loss: 419.6102 - val_mae: 419.6102\n",
      "Epoch 104/145\n",
      " - 3s - loss: 373.7324 - mae: 373.7324 - val_loss: 420.1604 - val_mae: 420.1604\n",
      "Epoch 105/145\n",
      " - 3s - loss: 373.4902 - mae: 373.4902 - val_loss: 420.4731 - val_mae: 420.4732\n",
      "Epoch 106/145\n",
      " - 3s - loss: 374.3535 - mae: 374.3535 - val_loss: 421.0213 - val_mae: 421.0213\n",
      "Epoch 107/145\n",
      " - 4s - loss: 373.4415 - mae: 373.4415 - val_loss: 420.6443 - val_mae: 420.6443\n",
      "Epoch 108/145\n",
      " - 3s - loss: 373.0834 - mae: 373.0833 - val_loss: 420.1548 - val_mae: 420.1548\n",
      "Epoch 109/145\n",
      " - 3s - loss: 373.2561 - mae: 373.2561 - val_loss: 422.9664 - val_mae: 422.9664\n",
      "Epoch 110/145\n",
      " - 3s - loss: 373.0116 - mae: 373.0115 - val_loss: 419.5709 - val_mae: 419.5710\n",
      "Epoch 111/145\n",
      " - 3s - loss: 371.8722 - mae: 371.8722 - val_loss: 420.0051 - val_mae: 420.0050\n",
      "Epoch 112/145\n",
      " - 3s - loss: 372.1642 - mae: 372.1641 - val_loss: 419.5404 - val_mae: 419.5403\n",
      "Epoch 113/145\n",
      " - 4s - loss: 373.3350 - mae: 373.3349 - val_loss: 419.6580 - val_mae: 419.6580\n",
      "Epoch 114/145\n",
      " - 3s - loss: 371.6320 - mae: 371.6320 - val_loss: 419.7458 - val_mae: 419.7458\n",
      "Epoch 115/145\n",
      " - 4s - loss: 372.0173 - mae: 372.0173 - val_loss: 421.1103 - val_mae: 421.1102\n",
      "Epoch 116/145\n",
      " - 3s - loss: 371.0540 - mae: 371.0540 - val_loss: 420.4068 - val_mae: 420.4067\n",
      "Epoch 117/145\n",
      " - 3s - loss: 371.3109 - mae: 371.3110 - val_loss: 421.9214 - val_mae: 421.9214\n",
      "Epoch 118/145\n",
      " - 3s - loss: 371.3911 - mae: 371.3911 - val_loss: 420.8317 - val_mae: 420.8317\n",
      "Epoch 119/145\n",
      " - 3s - loss: 370.3334 - mae: 370.3334 - val_loss: 419.7337 - val_mae: 419.7338\n",
      "Epoch 120/145\n",
      " - 3s - loss: 370.1273 - mae: 370.1273 - val_loss: 420.3913 - val_mae: 420.3914\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      " - 3s - loss: 368.9338 - mae: 368.9338 - val_loss: 420.2044 - val_mae: 420.2043\n",
      "Epoch 122/145\n",
      " - 3s - loss: 368.8588 - mae: 368.8588 - val_loss: 418.8163 - val_mae: 418.8163\n",
      "Epoch 123/145\n",
      " - 3s - loss: 368.1011 - mae: 368.1010 - val_loss: 418.8144 - val_mae: 418.8144\n",
      "Epoch 124/145\n",
      " - 3s - loss: 368.4088 - mae: 368.4088 - val_loss: 419.2497 - val_mae: 419.2497\n",
      "Epoch 125/145\n",
      " - 3s - loss: 368.2271 - mae: 368.2272 - val_loss: 419.0814 - val_mae: 419.0814\n",
      "Epoch 126/145\n",
      " - 3s - loss: 367.7371 - mae: 367.7371 - val_loss: 420.0130 - val_mae: 420.0130\n",
      "Epoch 127/145\n",
      " - 3s - loss: 368.0324 - mae: 368.0324 - val_loss: 419.5967 - val_mae: 419.5967\n",
      "Epoch 128/145\n",
      " - 4s - loss: 367.7180 - mae: 367.7181 - val_loss: 418.9483 - val_mae: 418.9482\n",
      "Epoch 129/145\n",
      " - 3s - loss: 367.4842 - mae: 367.4841 - val_loss: 418.8042 - val_mae: 418.8042\n",
      "Epoch 130/145\n",
      " - 3s - loss: 367.3565 - mae: 367.3564 - val_loss: 420.0178 - val_mae: 420.0178\n",
      "Epoch 131/145\n",
      " - 3s - loss: 367.0741 - mae: 367.0742 - val_loss: 420.4570 - val_mae: 420.4570\n",
      "Epoch 132/145\n",
      " - 3s - loss: 366.9663 - mae: 366.9663 - val_loss: 419.8805 - val_mae: 419.8804\n",
      "Epoch 133/145\n",
      " - 3s - loss: 367.0088 - mae: 367.0087 - val_loss: 418.5625 - val_mae: 418.5624\n",
      "Epoch 134/145\n",
      " - 4s - loss: 367.0052 - mae: 367.0052 - val_loss: 418.8602 - val_mae: 418.8601\n",
      "Epoch 135/145\n",
      " - 3s - loss: 366.5331 - mae: 366.5331 - val_loss: 419.3002 - val_mae: 419.3003\n",
      "Epoch 136/145\n",
      " - 3s - loss: 366.0408 - mae: 366.0407 - val_loss: 419.1148 - val_mae: 419.1148\n",
      "Epoch 137/145\n",
      " - 4s - loss: 366.4844 - mae: 366.4844 - val_loss: 420.7664 - val_mae: 420.7664\n",
      "Epoch 138/145\n",
      " - 3s - loss: 366.0310 - mae: 366.0310 - val_loss: 420.4273 - val_mae: 420.4273\n",
      "Epoch 139/145\n",
      " - 3s - loss: 365.7985 - mae: 365.7985 - val_loss: 418.3766 - val_mae: 418.3766\n",
      "Epoch 140/145\n",
      " - 3s - loss: 365.5558 - mae: 365.5558 - val_loss: 418.2977 - val_mae: 418.2977\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      " - 3s - loss: 365.0785 - mae: 365.0784 - val_loss: 419.2062 - val_mae: 419.2061\n",
      "Epoch 142/145\n",
      " - 3s - loss: 364.6127 - mae: 364.6127 - val_loss: 418.6662 - val_mae: 418.6662\n",
      "Epoch 143/145\n",
      " - 3s - loss: 364.6438 - mae: 364.6437 - val_loss: 419.6297 - val_mae: 419.6297\n",
      "Epoch 144/145\n",
      " - 3s - loss: 364.3729 - mae: 364.3730 - val_loss: 418.8123 - val_mae: 418.8123\n",
      "Epoch 145/145\n",
      " - 3s - loss: 364.4522 - mae: 364.4521 - val_loss: 418.3781 - val_mae: 418.3781\n",
      "\n",
      "val_mae is:418.37808696231843\n",
      "\n",
      "fold: 1\n",
      "Train on 125000 samples, validate on 25000 samples\n",
      "Epoch 1/145\n",
      " - 4s - loss: 2465.8079 - mae: 2465.8081 - val_loss: 871.3519 - val_mae: 871.3519\n",
      "Epoch 2/145\n",
      " - 3s - loss: 809.0494 - mae: 809.0493 - val_loss: 671.8816 - val_mae: 671.8816\n",
      "Epoch 3/145\n",
      " - 3s - loss: 671.1922 - mae: 671.1923 - val_loss: 603.1979 - val_mae: 603.1979\n",
      "Epoch 4/145\n",
      " - 3s - loss: 616.0398 - mae: 616.0397 - val_loss: 573.0899 - val_mae: 573.0898\n",
      "Epoch 5/145\n",
      " - 3s - loss: 591.9430 - mae: 591.9431 - val_loss: 540.7202 - val_mae: 540.7203\n",
      "Epoch 6/145\n",
      " - 3s - loss: 574.9603 - mae: 574.9603 - val_loss: 522.2009 - val_mae: 522.2009\n",
      "Epoch 7/145\n",
      " - 3s - loss: 560.4660 - mae: 560.4660 - val_loss: 538.2785 - val_mae: 538.2784\n",
      "Epoch 8/145\n",
      " - 4s - loss: 532.4610 - mae: 532.4609 - val_loss: 501.4309 - val_mae: 501.4309\n",
      "Epoch 9/145\n",
      " - 4s - loss: 517.8555 - mae: 517.8555 - val_loss: 494.0707 - val_mae: 494.0707\n",
      "Epoch 10/145\n",
      " - 4s - loss: 517.8258 - mae: 517.8257 - val_loss: 526.1656 - val_mae: 526.1656\n",
      "Epoch 11/145\n",
      " - 3s - loss: 506.6381 - mae: 506.6381 - val_loss: 513.8180 - val_mae: 513.8180\n",
      "Epoch 12/145\n",
      " - 3s - loss: 502.8170 - mae: 502.8170 - val_loss: 500.6138 - val_mae: 500.6138\n",
      "Epoch 13/145\n",
      " - 4s - loss: 501.7245 - mae: 501.7245 - val_loss: 520.6736 - val_mae: 520.6736\n",
      "Epoch 14/145\n",
      " - 3s - loss: 522.4055 - mae: 522.4056 - val_loss: 472.7999 - val_mae: 472.7999\n",
      "Epoch 15/145\n",
      " - 3s - loss: 487.9083 - mae: 487.9083 - val_loss: 468.5734 - val_mae: 468.5735\n",
      "Epoch 16/145\n",
      " - 3s - loss: 489.1964 - mae: 489.1965 - val_loss: 475.5544 - val_mae: 475.5545\n",
      "Epoch 17/145\n",
      " - 3s - loss: 486.9041 - mae: 486.9042 - val_loss: 487.5956 - val_mae: 487.5956\n",
      "Epoch 18/145\n",
      " - 4s - loss: 482.3977 - mae: 482.3977 - val_loss: 476.5546 - val_mae: 476.5546\n",
      "Epoch 19/145\n",
      " - 4s - loss: 492.4846 - mae: 492.4847 - val_loss: 518.2175 - val_mae: 518.2175\n",
      "Epoch 20/145\n",
      " - 4s - loss: 580.9611 - mae: 580.9611 - val_loss: 471.5404 - val_mae: 471.5404\n",
      "Epoch 21/145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr changed to 0.008999999798834323\n",
      " - 4s - loss: 451.3580 - mae: 451.3580 - val_loss: 441.4983 - val_mae: 441.4983\n",
      "Epoch 22/145\n",
      " - 4s - loss: 446.8190 - mae: 446.8190 - val_loss: 447.9727 - val_mae: 447.9727\n",
      "Epoch 23/145\n",
      " - 4s - loss: 443.6126 - mae: 443.6126 - val_loss: 441.4638 - val_mae: 441.4637\n",
      "Epoch 24/145\n",
      " - 4s - loss: 445.6428 - mae: 445.6428 - val_loss: 442.0935 - val_mae: 442.0935\n",
      "Epoch 25/145\n",
      " - 4s - loss: 444.2401 - mae: 444.2400 - val_loss: 444.4611 - val_mae: 444.4611\n",
      "Epoch 26/145\n",
      " - 4s - loss: 447.9747 - mae: 447.9747 - val_loss: 463.6964 - val_mae: 463.6964\n",
      "Epoch 27/145\n",
      " - 4s - loss: 446.3767 - mae: 446.3766 - val_loss: 444.4838 - val_mae: 444.4838\n",
      "Epoch 28/145\n",
      " - 4s - loss: 446.3893 - mae: 446.3893 - val_loss: 449.8667 - val_mae: 449.8667\n",
      "Epoch 29/145\n",
      " - 4s - loss: 444.1332 - mae: 444.1333 - val_loss: 441.1736 - val_mae: 441.1736\n",
      "Epoch 30/145\n",
      " - 4s - loss: 450.0307 - mae: 450.0307 - val_loss: 439.4200 - val_mae: 439.4200\n",
      "Epoch 31/145\n",
      " - 4s - loss: 458.3963 - mae: 458.3962 - val_loss: 446.1510 - val_mae: 446.1510\n",
      "Epoch 32/145\n",
      " - 4s - loss: 437.7899 - mae: 437.7899 - val_loss: 434.8920 - val_mae: 434.8920\n",
      "Epoch 33/145\n",
      " - 4s - loss: 438.8224 - mae: 438.8223 - val_loss: 439.5681 - val_mae: 439.5681\n",
      "Epoch 34/145\n",
      " - 4s - loss: 438.8933 - mae: 438.8933 - val_loss: 441.1078 - val_mae: 441.1078\n",
      "Epoch 35/145\n",
      " - 4s - loss: 466.6559 - mae: 466.6559 - val_loss: 455.7168 - val_mae: 455.7168\n",
      "Epoch 36/145\n",
      " - 4s - loss: 436.0458 - mae: 436.0459 - val_loss: 446.8573 - val_mae: 446.8573\n",
      "Epoch 37/145\n",
      " - 4s - loss: 434.8153 - mae: 434.8152 - val_loss: 531.3583 - val_mae: 531.3583\n",
      "Epoch 38/145\n",
      " - 4s - loss: 465.0848 - mae: 465.0849 - val_loss: 432.3427 - val_mae: 432.3428\n",
      "Epoch 39/145\n",
      " - 4s - loss: 435.1179 - mae: 435.1180 - val_loss: 434.3057 - val_mae: 434.3057\n",
      "Epoch 40/145\n",
      " - 3s - loss: 450.9687 - mae: 450.9688 - val_loss: 519.4489 - val_mae: 519.4489\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      " - 4s - loss: 443.1869 - mae: 443.1869 - val_loss: 425.6390 - val_mae: 425.6390\n",
      "Epoch 42/145\n",
      " - 4s - loss: 417.6558 - mae: 417.6558 - val_loss: 427.9875 - val_mae: 427.9874\n",
      "Epoch 43/145\n",
      " - 4s - loss: 417.5447 - mae: 417.5447 - val_loss: 426.2271 - val_mae: 426.2271\n",
      "Epoch 44/145\n",
      " - 3s - loss: 417.6336 - mae: 417.6336 - val_loss: 431.6414 - val_mae: 431.6414\n",
      "Epoch 45/145\n",
      " - 3s - loss: 416.2013 - mae: 416.2012 - val_loss: 426.1365 - val_mae: 426.1365\n",
      "Epoch 46/145\n",
      " - 3s - loss: 418.5988 - mae: 418.5988 - val_loss: 436.6590 - val_mae: 436.6590\n",
      "Epoch 47/145\n",
      " - 3s - loss: 425.1583 - mae: 425.1584 - val_loss: 424.0118 - val_mae: 424.0118\n",
      "Epoch 48/145\n",
      " - 4s - loss: 414.7146 - mae: 414.7146 - val_loss: 422.2242 - val_mae: 422.2241\n",
      "Epoch 49/145\n",
      " - 4s - loss: 413.4451 - mae: 413.4450 - val_loss: 428.6053 - val_mae: 428.6053\n",
      "Epoch 50/145\n",
      " - 4s - loss: 411.4207 - mae: 411.4207 - val_loss: 423.5508 - val_mae: 423.5508\n",
      "Epoch 51/145\n",
      " - 4s - loss: 412.3676 - mae: 412.3676 - val_loss: 425.2513 - val_mae: 425.2513\n",
      "Epoch 52/145\n",
      " - 3s - loss: 411.7822 - mae: 411.7822 - val_loss: 421.2639 - val_mae: 421.2639\n",
      "Epoch 53/145\n",
      " - 3s - loss: 412.6551 - mae: 412.6550 - val_loss: 428.4533 - val_mae: 428.4532\n",
      "Epoch 54/145\n",
      " - 3s - loss: 410.6480 - mae: 410.6480 - val_loss: 422.5680 - val_mae: 422.5680\n",
      "Epoch 55/145\n",
      " - 3s - loss: 410.3774 - mae: 410.3774 - val_loss: 423.8217 - val_mae: 423.8217\n",
      "Epoch 56/145\n",
      " - 3s - loss: 420.8460 - mae: 420.8460 - val_loss: 444.4392 - val_mae: 444.4391\n",
      "Epoch 57/145\n",
      " - 3s - loss: 431.3596 - mae: 431.3596 - val_loss: 428.1005 - val_mae: 428.1005\n",
      "Epoch 58/145\n",
      " - 3s - loss: 410.2405 - mae: 410.2405 - val_loss: 423.4642 - val_mae: 423.4642\n",
      "Epoch 59/145\n",
      " - 3s - loss: 408.8498 - mae: 408.8497 - val_loss: 428.9411 - val_mae: 428.9411\n",
      "Epoch 60/145\n",
      " - 3s - loss: 414.3248 - mae: 414.3246 - val_loss: 419.4010 - val_mae: 419.4010\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      " - 3s - loss: 399.6530 - mae: 399.6529 - val_loss: 415.3883 - val_mae: 415.3883\n",
      "Epoch 62/145\n",
      " - 3s - loss: 399.2132 - mae: 399.2133 - val_loss: 418.3684 - val_mae: 418.3683\n",
      "Epoch 63/145\n",
      " - 3s - loss: 401.9809 - mae: 401.9808 - val_loss: 419.4129 - val_mae: 419.4130\n",
      "Epoch 64/145\n",
      " - 3s - loss: 400.2663 - mae: 400.2664 - val_loss: 417.4985 - val_mae: 417.4985\n",
      "Epoch 65/145\n",
      " - 3s - loss: 403.9715 - mae: 403.9714 - val_loss: 417.0105 - val_mae: 417.0104\n",
      "Epoch 66/145\n",
      " - 3s - loss: 398.2596 - mae: 398.2596 - val_loss: 421.1919 - val_mae: 421.1919\n",
      "Epoch 67/145\n",
      " - 3s - loss: 401.8662 - mae: 401.8661 - val_loss: 415.2357 - val_mae: 415.2357\n",
      "Epoch 68/145\n",
      " - 3s - loss: 398.2252 - mae: 398.2253 - val_loss: 423.6303 - val_mae: 423.6303\n",
      "Epoch 69/145\n",
      " - 3s - loss: 407.2694 - mae: 407.2694 - val_loss: 419.0804 - val_mae: 419.0804\n",
      "Epoch 70/145\n",
      " - 3s - loss: 399.0707 - mae: 399.0706 - val_loss: 414.9862 - val_mae: 414.9862\n",
      "Epoch 71/145\n",
      " - 4s - loss: 396.9079 - mae: 396.9079 - val_loss: 416.0488 - val_mae: 416.0488\n",
      "Epoch 72/145\n",
      " - 4s - loss: 397.2112 - mae: 397.2113 - val_loss: 419.4111 - val_mae: 419.4112\n",
      "Epoch 73/145\n",
      " - 3s - loss: 398.8263 - mae: 398.8263 - val_loss: 415.4727 - val_mae: 415.4727\n",
      "Epoch 74/145\n",
      " - 3s - loss: 395.5347 - mae: 395.5347 - val_loss: 415.8837 - val_mae: 415.8837\n",
      "Epoch 75/145\n",
      " - 3s - loss: 395.0077 - mae: 395.0078 - val_loss: 418.4920 - val_mae: 418.4920\n",
      "Epoch 76/145\n",
      " - 3s - loss: 394.9037 - mae: 394.9037 - val_loss: 414.5539 - val_mae: 414.5539\n",
      "Epoch 77/145\n",
      " - 3s - loss: 393.0703 - mae: 393.0704 - val_loss: 413.6725 - val_mae: 413.6725\n",
      "Epoch 78/145\n",
      " - 3s - loss: 393.1518 - mae: 393.1519 - val_loss: 428.1182 - val_mae: 428.1182\n",
      "Epoch 79/145\n",
      " - 3s - loss: 403.9112 - mae: 403.9112 - val_loss: 419.8420 - val_mae: 419.8419\n",
      "Epoch 80/145\n",
      " - 3s - loss: 392.1128 - mae: 392.1129 - val_loss: 423.3350 - val_mae: 423.3350\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      " - 3s - loss: 389.7365 - mae: 389.7366 - val_loss: 412.6402 - val_mae: 412.6402\n",
      "Epoch 82/145\n",
      " - 3s - loss: 388.9474 - mae: 388.9474 - val_loss: 413.7670 - val_mae: 413.7670\n",
      "Epoch 83/145\n",
      " - 3s - loss: 387.5348 - mae: 387.5348 - val_loss: 414.6495 - val_mae: 414.6495\n",
      "Epoch 84/145\n",
      " - 3s - loss: 387.1213 - mae: 387.1213 - val_loss: 413.4125 - val_mae: 413.4125\n",
      "Epoch 85/145\n",
      " - 3s - loss: 386.0233 - mae: 386.0233 - val_loss: 410.2263 - val_mae: 410.2263\n",
      "Epoch 86/145\n",
      " - 3s - loss: 385.9087 - mae: 385.9087 - val_loss: 411.3952 - val_mae: 411.3952\n",
      "Epoch 87/145\n",
      " - 3s - loss: 385.5873 - mae: 385.5872 - val_loss: 411.3450 - val_mae: 411.3450\n",
      "Epoch 88/145\n",
      " - 3s - loss: 386.0933 - mae: 386.0933 - val_loss: 410.6058 - val_mae: 410.6058\n",
      "Epoch 89/145\n",
      " - 3s - loss: 386.4442 - mae: 386.4442 - val_loss: 419.7091 - val_mae: 419.7091\n",
      "Epoch 90/145\n",
      " - 3s - loss: 388.1060 - mae: 388.1060 - val_loss: 412.4736 - val_mae: 412.4737\n",
      "Epoch 91/145\n",
      " - 3s - loss: 385.4221 - mae: 385.4221 - val_loss: 410.6620 - val_mae: 410.6620\n",
      "Epoch 92/145\n",
      " - 3s - loss: 383.6834 - mae: 383.6832 - val_loss: 410.3905 - val_mae: 410.3904\n",
      "Epoch 93/145\n",
      " - 4s - loss: 384.3850 - mae: 384.3850 - val_loss: 410.3860 - val_mae: 410.3860\n",
      "Epoch 94/145\n",
      " - 4s - loss: 385.0668 - mae: 385.0668 - val_loss: 410.9119 - val_mae: 410.9119\n",
      "Epoch 95/145\n",
      " - 4s - loss: 384.3025 - mae: 384.3025 - val_loss: 411.6933 - val_mae: 411.6933\n",
      "Epoch 96/145\n",
      " - 3s - loss: 384.9486 - mae: 384.9486 - val_loss: 410.3372 - val_mae: 410.3372\n",
      "Epoch 97/145\n",
      " - 3s - loss: 384.4939 - mae: 384.4939 - val_loss: 411.5562 - val_mae: 411.5562\n",
      "Epoch 98/145\n",
      " - 3s - loss: 383.2484 - mae: 383.2484 - val_loss: 411.4546 - val_mae: 411.4546\n",
      "Epoch 99/145\n",
      " - 4s - loss: 382.9931 - mae: 382.9931 - val_loss: 409.7057 - val_mae: 409.7057\n",
      "Epoch 100/145\n",
      " - 3s - loss: 382.1487 - mae: 382.1487 - val_loss: 411.5397 - val_mae: 411.5398\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      " - 3s - loss: 379.6154 - mae: 379.6154 - val_loss: 409.0027 - val_mae: 409.0027\n",
      "Epoch 102/145\n",
      " - 4s - loss: 379.6602 - mae: 379.6602 - val_loss: 412.0664 - val_mae: 412.0664\n",
      "Epoch 103/145\n",
      " - 4s - loss: 379.6392 - mae: 379.6392 - val_loss: 408.7795 - val_mae: 408.7796\n",
      "Epoch 104/145\n",
      " - 4s - loss: 379.0607 - mae: 379.0607 - val_loss: 411.7449 - val_mae: 411.7449\n",
      "Epoch 105/145\n",
      " - 3s - loss: 378.8719 - mae: 378.8719 - val_loss: 409.1629 - val_mae: 409.1629\n",
      "Epoch 106/145\n",
      " - 3s - loss: 377.6771 - mae: 377.6771 - val_loss: 409.6805 - val_mae: 409.6805\n",
      "Epoch 107/145\n",
      " - 3s - loss: 378.4870 - mae: 378.4870 - val_loss: 409.2780 - val_mae: 409.2780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/145\n",
      " - 4s - loss: 378.0807 - mae: 378.0806 - val_loss: 408.9386 - val_mae: 408.9386\n",
      "Epoch 109/145\n",
      " - 4s - loss: 377.5718 - mae: 377.5717 - val_loss: 409.1025 - val_mae: 409.1025\n",
      "Epoch 110/145\n",
      " - 4s - loss: 376.8961 - mae: 376.8960 - val_loss: 409.1984 - val_mae: 409.1984\n",
      "Epoch 111/145\n",
      " - 4s - loss: 377.2177 - mae: 377.2177 - val_loss: 409.2765 - val_mae: 409.2765\n",
      "Epoch 112/145\n",
      " - 4s - loss: 376.7788 - mae: 376.7787 - val_loss: 408.6479 - val_mae: 408.6479\n",
      "Epoch 113/145\n",
      " - 4s - loss: 376.3006 - mae: 376.3006 - val_loss: 408.8626 - val_mae: 408.8625\n",
      "Epoch 114/145\n",
      " - 4s - loss: 377.6703 - mae: 377.6702 - val_loss: 409.9219 - val_mae: 409.9219\n",
      "Epoch 115/145\n",
      " - 4s - loss: 376.8480 - mae: 376.8480 - val_loss: 410.4693 - val_mae: 410.4694\n",
      "Epoch 116/145\n",
      " - 4s - loss: 377.0518 - mae: 377.0518 - val_loss: 411.7636 - val_mae: 411.7636\n",
      "Epoch 117/145\n",
      " - 4s - loss: 377.5921 - mae: 377.5921 - val_loss: 410.2352 - val_mae: 410.2352\n",
      "Epoch 118/145\n",
      " - 4s - loss: 375.6181 - mae: 375.6182 - val_loss: 410.8869 - val_mae: 410.8869\n",
      "Epoch 119/145\n",
      " - 4s - loss: 376.4001 - mae: 376.4001 - val_loss: 409.8250 - val_mae: 409.8250\n",
      "Epoch 120/145\n",
      " - 4s - loss: 376.0620 - mae: 376.0620 - val_loss: 409.6809 - val_mae: 409.6809\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      " - 4s - loss: 373.3536 - mae: 373.3535 - val_loss: 407.6184 - val_mae: 407.6184\n",
      "Epoch 122/145\n",
      " - 4s - loss: 372.9999 - mae: 372.9999 - val_loss: 407.1845 - val_mae: 407.1844\n",
      "Epoch 123/145\n",
      " - 4s - loss: 372.6042 - mae: 372.6042 - val_loss: 407.7350 - val_mae: 407.7350\n",
      "Epoch 124/145\n",
      " - 4s - loss: 372.9775 - mae: 372.9774 - val_loss: 408.7292 - val_mae: 408.7292\n",
      "Epoch 125/145\n",
      " - 4s - loss: 373.1458 - mae: 373.1458 - val_loss: 407.8546 - val_mae: 407.8546\n",
      "Epoch 126/145\n",
      " - 4s - loss: 372.3555 - mae: 372.3555 - val_loss: 407.9208 - val_mae: 407.9207\n",
      "Epoch 127/145\n",
      " - 4s - loss: 372.3724 - mae: 372.3724 - val_loss: 408.3691 - val_mae: 408.3692\n",
      "Epoch 128/145\n",
      " - 4s - loss: 372.4702 - mae: 372.4703 - val_loss: 407.9307 - val_mae: 407.9307\n",
      "Epoch 129/145\n",
      " - 4s - loss: 371.6730 - mae: 371.6730 - val_loss: 407.5116 - val_mae: 407.5116\n",
      "Epoch 130/145\n",
      " - 4s - loss: 372.4993 - mae: 372.4994 - val_loss: 408.4356 - val_mae: 408.4356\n",
      "Epoch 131/145\n",
      " - 4s - loss: 371.9961 - mae: 371.9961 - val_loss: 408.3292 - val_mae: 408.3293\n",
      "Epoch 132/145\n",
      " - 4s - loss: 371.8260 - mae: 371.8260 - val_loss: 407.6457 - val_mae: 407.6458\n",
      "Epoch 133/145\n",
      " - 5s - loss: 371.1218 - mae: 371.1218 - val_loss: 408.0275 - val_mae: 408.0276\n",
      "Epoch 134/145\n",
      " - 4s - loss: 371.8863 - mae: 371.8864 - val_loss: 407.3191 - val_mae: 407.3191\n",
      "Epoch 135/145\n",
      " - 4s - loss: 371.2576 - mae: 371.2576 - val_loss: 407.7240 - val_mae: 407.7240\n",
      "Epoch 136/145\n",
      " - 4s - loss: 371.6335 - mae: 371.6335 - val_loss: 407.4411 - val_mae: 407.4411\n",
      "Epoch 137/145\n",
      " - 4s - loss: 370.5862 - mae: 370.5863 - val_loss: 407.5051 - val_mae: 407.5050\n",
      "Epoch 138/145\n",
      " - 5s - loss: 370.2698 - mae: 370.2698 - val_loss: 407.8750 - val_mae: 407.8750\n",
      "Epoch 139/145\n",
      " - 5s - loss: 370.6083 - mae: 370.6083 - val_loss: 407.6294 - val_mae: 407.6294\n",
      "Epoch 140/145\n",
      " - 5s - loss: 370.5074 - mae: 370.5073 - val_loss: 407.8137 - val_mae: 407.8136\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      " - 5s - loss: 369.1707 - mae: 369.1707 - val_loss: 407.8887 - val_mae: 407.8887\n",
      "Epoch 142/145\n",
      " - 5s - loss: 369.3349 - mae: 369.3349 - val_loss: 406.9821 - val_mae: 406.9821\n",
      "Epoch 143/145\n",
      " - 5s - loss: 369.0604 - mae: 369.0604 - val_loss: 407.3605 - val_mae: 407.3606\n",
      "Epoch 144/145\n",
      " - 4s - loss: 368.6799 - mae: 368.6799 - val_loss: 407.5409 - val_mae: 407.5408\n",
      "Epoch 145/145\n",
      " - 4s - loss: 368.7713 - mae: 368.7713 - val_loss: 407.5821 - val_mae: 407.5820\n",
      "\n",
      "val_mae is:407.5820529640198\n",
      "\n",
      "fold: 2\n",
      "Train on 125000 samples, validate on 25000 samples\n",
      "Epoch 1/145\n",
      " - 4s - loss: 2387.2429 - mae: 2387.2429 - val_loss: 879.3644 - val_mae: 879.3645\n",
      "Epoch 2/145\n",
      " - 4s - loss: 792.3087 - mae: 792.3085 - val_loss: 733.3006 - val_mae: 733.3005\n",
      "Epoch 3/145\n",
      " - 4s - loss: 684.4481 - mae: 684.4481 - val_loss: 824.1959 - val_mae: 824.1959\n",
      "Epoch 4/145\n",
      " - 4s - loss: 677.0251 - mae: 677.0252 - val_loss: 606.1370 - val_mae: 606.1370\n",
      "Epoch 5/145\n",
      " - 4s - loss: 588.7676 - mae: 588.7678 - val_loss: 624.8909 - val_mae: 624.8909\n",
      "Epoch 6/145\n",
      " - 4s - loss: 557.8129 - mae: 557.8128 - val_loss: 535.0370 - val_mae: 535.0370\n",
      "Epoch 7/145\n",
      " - 4s - loss: 543.6408 - mae: 543.6407 - val_loss: 522.4910 - val_mae: 522.4910\n",
      "Epoch 8/145\n",
      " - 4s - loss: 570.7887 - mae: 570.7888 - val_loss: 653.4816 - val_mae: 653.4816\n",
      "Epoch 9/145\n",
      " - 4s - loss: 526.8136 - mae: 526.8135 - val_loss: 634.2244 - val_mae: 634.2244\n",
      "Epoch 10/145\n",
      " - 4s - loss: 570.4854 - mae: 570.4854 - val_loss: 552.2477 - val_mae: 552.2476\n",
      "Epoch 11/145\n",
      " - 4s - loss: 527.8025 - mae: 527.8026 - val_loss: 495.3048 - val_mae: 495.3047\n",
      "Epoch 12/145\n",
      " - 4s - loss: 522.1168 - mae: 522.1169 - val_loss: 516.0953 - val_mae: 516.0953\n",
      "Epoch 13/145\n",
      " - 4s - loss: 497.3839 - mae: 497.3838 - val_loss: 556.4982 - val_mae: 556.4982\n",
      "Epoch 14/145\n",
      " - 4s - loss: 547.9498 - mae: 547.9498 - val_loss: 490.6643 - val_mae: 490.6644\n",
      "Epoch 15/145\n",
      " - 3s - loss: 488.6217 - mae: 488.6217 - val_loss: 482.3628 - val_mae: 482.3628\n",
      "Epoch 16/145\n",
      " - 3s - loss: 475.1314 - mae: 475.1314 - val_loss: 544.1334 - val_mae: 544.1334\n",
      "Epoch 17/145\n",
      " - 3s - loss: 547.9027 - mae: 547.9027 - val_loss: 472.1113 - val_mae: 472.1113\n",
      "Epoch 18/145\n",
      " - 3s - loss: 499.6779 - mae: 499.6779 - val_loss: 492.6282 - val_mae: 492.6282\n",
      "Epoch 19/145\n",
      " - 3s - loss: 471.9944 - mae: 471.9943 - val_loss: 476.5539 - val_mae: 476.5539\n",
      "Epoch 20/145\n",
      " - 3s - loss: 482.4571 - mae: 482.4571 - val_loss: 480.5445 - val_mae: 480.5445\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      " - 3s - loss: 449.0298 - mae: 449.0298 - val_loss: 458.6836 - val_mae: 458.6836\n",
      "Epoch 22/145\n",
      " - 3s - loss: 444.5600 - mae: 444.5600 - val_loss: 463.8516 - val_mae: 463.8516\n",
      "Epoch 23/145\n",
      " - 3s - loss: 443.5280 - mae: 443.5280 - val_loss: 448.5745 - val_mae: 448.5746\n",
      "Epoch 24/145\n",
      " - 3s - loss: 443.9591 - mae: 443.9591 - val_loss: 452.6684 - val_mae: 452.6684\n",
      "Epoch 25/145\n",
      " - 4s - loss: 441.5477 - mae: 441.5477 - val_loss: 490.1095 - val_mae: 490.1094\n",
      "Epoch 26/145\n",
      " - 3s - loss: 467.2827 - mae: 467.2827 - val_loss: 497.7529 - val_mae: 497.7528\n",
      "Epoch 27/145\n",
      " - 3s - loss: 473.2299 - mae: 473.2299 - val_loss: 495.7698 - val_mae: 495.7698\n",
      "Epoch 28/145\n",
      " - 3s - loss: 449.9607 - mae: 449.9607 - val_loss: 444.6810 - val_mae: 444.6810\n",
      "Epoch 29/145\n",
      " - 3s - loss: 455.2363 - mae: 455.2362 - val_loss: 449.2002 - val_mae: 449.2002\n",
      "Epoch 30/145\n",
      " - 3s - loss: 433.7642 - mae: 433.7642 - val_loss: 442.0750 - val_mae: 442.0750\n",
      "Epoch 31/145\n",
      " - 3s - loss: 435.2365 - mae: 435.2364 - val_loss: 449.2882 - val_mae: 449.2882\n",
      "Epoch 32/145\n",
      " - 3s - loss: 436.2624 - mae: 436.2624 - val_loss: 444.9154 - val_mae: 444.9154\n",
      "Epoch 33/145\n",
      " - 3s - loss: 436.5382 - mae: 436.5383 - val_loss: 445.6227 - val_mae: 445.6228\n",
      "Epoch 34/145\n",
      " - 3s - loss: 434.9850 - mae: 434.9850 - val_loss: 447.1110 - val_mae: 447.1111\n",
      "Epoch 35/145\n",
      " - 3s - loss: 434.8852 - mae: 434.8852 - val_loss: 442.0525 - val_mae: 442.0524\n",
      "Epoch 36/145\n",
      " - 3s - loss: 459.3887 - mae: 459.3887 - val_loss: 552.9067 - val_mae: 552.9067\n",
      "Epoch 37/145\n",
      " - 3s - loss: 434.8385 - mae: 434.8384 - val_loss: 444.9792 - val_mae: 444.9792\n",
      "Epoch 38/145\n",
      " - 3s - loss: 436.4553 - mae: 436.4553 - val_loss: 479.0509 - val_mae: 479.0509\n",
      "Epoch 39/145\n",
      " - 3s - loss: 434.4004 - mae: 434.4005 - val_loss: 436.1128 - val_mae: 436.1128\n",
      "Epoch 40/145\n",
      " - 3s - loss: 429.8663 - mae: 429.8664 - val_loss: 441.4320 - val_mae: 441.4320\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      " - 3s - loss: 415.7776 - mae: 415.7776 - val_loss: 436.3028 - val_mae: 436.3028\n",
      "Epoch 42/145\n",
      " - 3s - loss: 413.4172 - mae: 413.4171 - val_loss: 443.9373 - val_mae: 443.9373\n",
      "Epoch 43/145\n",
      " - 3s - loss: 410.7896 - mae: 410.7897 - val_loss: 434.2416 - val_mae: 434.2416\n",
      "Epoch 44/145\n",
      " - 4s - loss: 410.5916 - mae: 410.5916 - val_loss: 433.6433 - val_mae: 433.6433\n",
      "Epoch 45/145\n",
      " - 4s - loss: 413.4572 - mae: 413.4572 - val_loss: 431.5272 - val_mae: 431.5271\n",
      "Epoch 46/145\n",
      " - 4s - loss: 411.9342 - mae: 411.9341 - val_loss: 436.6673 - val_mae: 436.6674\n",
      "Epoch 47/145\n",
      " - 4s - loss: 408.7666 - mae: 408.7666 - val_loss: 428.4390 - val_mae: 428.4390\n",
      "Epoch 48/145\n",
      " - 4s - loss: 434.0292 - mae: 434.0292 - val_loss: 458.1323 - val_mae: 458.1324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/145\n",
      " - 4s - loss: 428.2805 - mae: 428.2804 - val_loss: 434.5818 - val_mae: 434.5818\n",
      "Epoch 50/145\n",
      " - 4s - loss: 407.1694 - mae: 407.1694 - val_loss: 429.8830 - val_mae: 429.8830\n",
      "Epoch 51/145\n",
      " - 4s - loss: 407.9571 - mae: 407.9571 - val_loss: 434.7908 - val_mae: 434.7908\n",
      "Epoch 52/145\n",
      " - 4s - loss: 404.9249 - mae: 404.9249 - val_loss: 426.9457 - val_mae: 426.9457\n",
      "Epoch 53/145\n",
      " - 4s - loss: 404.8383 - mae: 404.8384 - val_loss: 427.1888 - val_mae: 427.1888\n",
      "Epoch 54/145\n",
      " - 4s - loss: 404.1496 - mae: 404.1496 - val_loss: 428.8906 - val_mae: 428.8906\n",
      "Epoch 55/145\n",
      " - 4s - loss: 406.6566 - mae: 406.6566 - val_loss: 424.6260 - val_mae: 424.6260\n",
      "Epoch 56/145\n",
      " - 4s - loss: 406.5206 - mae: 406.5205 - val_loss: 427.3555 - val_mae: 427.3555\n",
      "Epoch 57/145\n",
      " - 4s - loss: 402.9935 - mae: 402.9934 - val_loss: 429.8791 - val_mae: 429.8791\n",
      "Epoch 58/145\n",
      " - 4s - loss: 407.3115 - mae: 407.3115 - val_loss: 428.5092 - val_mae: 428.5092\n",
      "Epoch 59/145\n",
      " - 4s - loss: 426.0610 - mae: 426.0611 - val_loss: 440.6740 - val_mae: 440.6740\n",
      "Epoch 60/145\n",
      " - 4s - loss: 402.4611 - mae: 402.4612 - val_loss: 434.0936 - val_mae: 434.0936\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      " - 4s - loss: 397.0023 - mae: 397.0022 - val_loss: 425.9352 - val_mae: 425.9352\n",
      "Epoch 62/145\n",
      " - 4s - loss: 393.3936 - mae: 393.3937 - val_loss: 421.8043 - val_mae: 421.8043\n",
      "Epoch 63/145\n",
      " - 4s - loss: 393.1143 - mae: 393.1142 - val_loss: 421.3316 - val_mae: 421.3316\n",
      "Epoch 64/145\n",
      " - 4s - loss: 390.6683 - mae: 390.6683 - val_loss: 426.0002 - val_mae: 426.0002\n",
      "Epoch 65/145\n",
      " - 4s - loss: 393.9067 - mae: 393.9067 - val_loss: 422.3362 - val_mae: 422.3362\n",
      "Epoch 66/145\n",
      " - 4s - loss: 390.9238 - mae: 390.9238 - val_loss: 422.4557 - val_mae: 422.4556\n",
      "Epoch 67/145\n",
      " - 4s - loss: 389.5955 - mae: 389.5954 - val_loss: 422.8653 - val_mae: 422.8652\n",
      "Epoch 68/145\n",
      " - 4s - loss: 389.6193 - mae: 389.6193 - val_loss: 419.1993 - val_mae: 419.1993\n",
      "Epoch 69/145\n",
      " - 4s - loss: 389.4052 - mae: 389.4052 - val_loss: 423.0330 - val_mae: 423.0330\n",
      "Epoch 70/145\n",
      " - 4s - loss: 387.7358 - mae: 387.7358 - val_loss: 424.0180 - val_mae: 424.0180\n",
      "Epoch 71/145\n",
      " - 5s - loss: 389.5489 - mae: 389.5489 - val_loss: 421.9522 - val_mae: 421.9522\n",
      "Epoch 72/145\n",
      " - 5s - loss: 391.2571 - mae: 391.2571 - val_loss: 425.8829 - val_mae: 425.8828\n",
      "Epoch 73/145\n",
      " - 4s - loss: 390.3341 - mae: 390.3341 - val_loss: 423.4609 - val_mae: 423.4609\n",
      "Epoch 74/145\n",
      " - 4s - loss: 386.9985 - mae: 386.9985 - val_loss: 421.6877 - val_mae: 421.6877\n",
      "Epoch 75/145\n",
      " - 4s - loss: 386.1143 - mae: 386.1143 - val_loss: 420.5487 - val_mae: 420.5487\n",
      "Epoch 76/145\n",
      " - 4s - loss: 387.3977 - mae: 387.3976 - val_loss: 429.3669 - val_mae: 429.3669\n",
      "Epoch 77/145\n",
      " - 4s - loss: 387.5840 - mae: 387.5840 - val_loss: 423.3585 - val_mae: 423.3585\n",
      "Epoch 78/145\n",
      " - 4s - loss: 386.7820 - mae: 386.7819 - val_loss: 430.8841 - val_mae: 430.8840\n",
      "Epoch 79/145\n",
      " - 4s - loss: 389.8301 - mae: 389.8302 - val_loss: 419.8505 - val_mae: 419.8505\n",
      "Epoch 80/145\n",
      " - 4s - loss: 392.0058 - mae: 392.0058 - val_loss: 422.3397 - val_mae: 422.3397\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      " - 4s - loss: 380.5522 - mae: 380.5522 - val_loss: 419.5772 - val_mae: 419.5772\n",
      "Epoch 82/145\n",
      " - 4s - loss: 380.1270 - mae: 380.1270 - val_loss: 421.3553 - val_mae: 421.3553\n",
      "Epoch 83/145\n",
      " - 4s - loss: 379.3547 - mae: 379.3548 - val_loss: 421.3011 - val_mae: 421.3011\n",
      "Epoch 84/145\n",
      " - 4s - loss: 378.0418 - mae: 378.0418 - val_loss: 418.3859 - val_mae: 418.3859\n",
      "Epoch 85/145\n",
      " - 4s - loss: 379.0774 - mae: 379.0774 - val_loss: 419.9609 - val_mae: 419.9609\n",
      "Epoch 86/145\n",
      " - 4s - loss: 378.1433 - mae: 378.1434 - val_loss: 418.2565 - val_mae: 418.2565\n",
      "Epoch 87/145\n",
      " - 4s - loss: 376.7759 - mae: 376.7759 - val_loss: 418.7832 - val_mae: 418.7832\n",
      "Epoch 88/145\n",
      " - 4s - loss: 376.9539 - mae: 376.9540 - val_loss: 423.8040 - val_mae: 423.8040\n",
      "Epoch 89/145\n",
      " - 4s - loss: 377.9988 - mae: 377.9988 - val_loss: 420.7035 - val_mae: 420.7036\n",
      "Epoch 90/145\n",
      " - 4s - loss: 377.1352 - mae: 377.1351 - val_loss: 428.4827 - val_mae: 428.4827\n",
      "Epoch 91/145\n",
      " - 4s - loss: 379.7927 - mae: 379.7927 - val_loss: 420.4239 - val_mae: 420.4240\n",
      "Epoch 92/145\n",
      " - 4s - loss: 375.3464 - mae: 375.3463 - val_loss: 419.3426 - val_mae: 419.3426\n",
      "Epoch 93/145\n",
      " - 4s - loss: 374.9062 - mae: 374.9063 - val_loss: 424.6406 - val_mae: 424.6406\n",
      "Epoch 94/145\n",
      " - 4s - loss: 375.7649 - mae: 375.7649 - val_loss: 419.1434 - val_mae: 419.1434\n",
      "Epoch 95/145\n",
      " - 4s - loss: 375.1869 - mae: 375.1869 - val_loss: 417.5217 - val_mae: 417.5216\n",
      "Epoch 96/145\n",
      " - 4s - loss: 374.3838 - mae: 374.3839 - val_loss: 418.5874 - val_mae: 418.5874\n",
      "Epoch 97/145\n",
      " - 4s - loss: 374.2322 - mae: 374.2322 - val_loss: 418.2447 - val_mae: 418.2447\n",
      "Epoch 98/145\n",
      " - 4s - loss: 373.6158 - mae: 373.6159 - val_loss: 421.6374 - val_mae: 421.6374\n",
      "Epoch 99/145\n",
      " - 4s - loss: 374.3942 - mae: 374.3942 - val_loss: 418.7271 - val_mae: 418.7271\n",
      "Epoch 100/145\n",
      " - 4s - loss: 373.1306 - mae: 373.1306 - val_loss: 419.5079 - val_mae: 419.5078\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      " - 4s - loss: 370.9417 - mae: 370.9417 - val_loss: 418.5375 - val_mae: 418.5375\n",
      "Epoch 102/145\n",
      " - 4s - loss: 369.9711 - mae: 369.9711 - val_loss: 417.2079 - val_mae: 417.2079\n",
      "Epoch 103/145\n",
      " - 4s - loss: 369.9627 - mae: 369.9627 - val_loss: 416.9921 - val_mae: 416.9921\n",
      "Epoch 104/145\n",
      " - 4s - loss: 369.5787 - mae: 369.5788 - val_loss: 419.7032 - val_mae: 419.7032\n",
      "Epoch 105/145\n",
      " - 4s - loss: 369.1540 - mae: 369.1541 - val_loss: 417.7632 - val_mae: 417.7632\n",
      "Epoch 106/145\n",
      " - 4s - loss: 369.5078 - mae: 369.5078 - val_loss: 416.6019 - val_mae: 416.6019\n",
      "Epoch 107/145\n",
      " - 4s - loss: 369.0864 - mae: 369.0863 - val_loss: 417.7990 - val_mae: 417.7990\n",
      "Epoch 108/145\n",
      " - 4s - loss: 368.2410 - mae: 368.2409 - val_loss: 415.9623 - val_mae: 415.9624\n",
      "Epoch 109/145\n",
      " - 4s - loss: 368.0146 - mae: 368.0146 - val_loss: 417.0926 - val_mae: 417.0926\n",
      "Epoch 110/145\n",
      " - 4s - loss: 367.9201 - mae: 367.9201 - val_loss: 417.8229 - val_mae: 417.8228\n",
      "Epoch 111/145\n",
      " - 4s - loss: 367.4477 - mae: 367.4478 - val_loss: 417.4168 - val_mae: 417.4168\n",
      "Epoch 112/145\n",
      " - 4s - loss: 367.9409 - mae: 367.9409 - val_loss: 418.2198 - val_mae: 418.2198\n",
      "Epoch 113/145\n",
      " - 4s - loss: 368.0625 - mae: 368.0624 - val_loss: 420.0068 - val_mae: 420.0068\n",
      "Epoch 114/145\n",
      " - 4s - loss: 367.1434 - mae: 367.1434 - val_loss: 417.4777 - val_mae: 417.4777\n",
      "Epoch 115/145\n",
      " - 4s - loss: 366.8162 - mae: 366.8161 - val_loss: 416.8200 - val_mae: 416.8199\n",
      "Epoch 116/145\n",
      " - 4s - loss: 366.8924 - mae: 366.8925 - val_loss: 418.2488 - val_mae: 418.2488\n",
      "Epoch 117/145\n",
      " - 5s - loss: 366.7783 - mae: 366.7784 - val_loss: 416.4968 - val_mae: 416.4968\n",
      "Epoch 118/145\n",
      " - 4s - loss: 366.5977 - mae: 366.5977 - val_loss: 419.2417 - val_mae: 419.2417\n",
      "Epoch 119/145\n",
      " - 4s - loss: 366.6328 - mae: 366.6328 - val_loss: 420.6091 - val_mae: 420.6091\n",
      "Epoch 120/145\n",
      " - 4s - loss: 366.4334 - mae: 366.4334 - val_loss: 419.6792 - val_mae: 419.6792\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      " - 4s - loss: 363.6261 - mae: 363.6261 - val_loss: 416.2236 - val_mae: 416.2236\n",
      "Epoch 122/145\n",
      " - 4s - loss: 363.2554 - mae: 363.2555 - val_loss: 416.7224 - val_mae: 416.7224\n",
      "Epoch 123/145\n",
      " - 4s - loss: 363.2798 - mae: 363.2798 - val_loss: 416.5782 - val_mae: 416.5781\n",
      "Epoch 124/145\n",
      " - 4s - loss: 362.9180 - mae: 362.9181 - val_loss: 415.6275 - val_mae: 415.6276\n",
      "Epoch 125/145\n",
      " - 4s - loss: 362.6446 - mae: 362.6446 - val_loss: 415.7908 - val_mae: 415.7907\n",
      "Epoch 126/145\n",
      " - 4s - loss: 362.4716 - mae: 362.4716 - val_loss: 416.6675 - val_mae: 416.6675\n",
      "Epoch 127/145\n",
      " - 4s - loss: 362.2226 - mae: 362.2227 - val_loss: 415.9816 - val_mae: 415.9816\n",
      "Epoch 128/145\n",
      " - 4s - loss: 362.8727 - mae: 362.8727 - val_loss: 418.6857 - val_mae: 418.6857\n",
      "Epoch 129/145\n",
      " - 4s - loss: 362.4223 - mae: 362.4223 - val_loss: 415.8762 - val_mae: 415.8763\n",
      "Epoch 130/145\n",
      " - 4s - loss: 361.9666 - mae: 361.9666 - val_loss: 417.3034 - val_mae: 417.3034\n",
      "Epoch 131/145\n",
      " - 4s - loss: 361.9709 - mae: 361.9709 - val_loss: 416.0183 - val_mae: 416.0183\n",
      "Epoch 132/145\n",
      " - 4s - loss: 361.4680 - mae: 361.4680 - val_loss: 417.0386 - val_mae: 417.0386\n",
      "Epoch 133/145\n",
      " - 4s - loss: 362.0029 - mae: 362.0029 - val_loss: 417.0451 - val_mae: 417.0451\n",
      "Epoch 134/145\n",
      " - 4s - loss: 361.1906 - mae: 361.1906 - val_loss: 416.0407 - val_mae: 416.0407\n",
      "Epoch 135/145\n",
      " - 4s - loss: 360.8323 - mae: 360.8322 - val_loss: 417.1595 - val_mae: 417.1595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/145\n",
      " - 4s - loss: 361.0679 - mae: 361.0679 - val_loss: 417.3618 - val_mae: 417.3618\n",
      "Epoch 137/145\n",
      " - 4s - loss: 360.8285 - mae: 360.8286 - val_loss: 416.4869 - val_mae: 416.4868\n",
      "Epoch 138/145\n",
      " - 4s - loss: 360.6264 - mae: 360.6263 - val_loss: 416.8362 - val_mae: 416.8362\n",
      "Epoch 139/145\n",
      " - 4s - loss: 360.6740 - mae: 360.6740 - val_loss: 416.7460 - val_mae: 416.7459\n",
      "Epoch 140/145\n",
      " - 4s - loss: 360.5180 - mae: 360.5179 - val_loss: 418.2436 - val_mae: 418.2436\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      " - 4s - loss: 358.9741 - mae: 358.9740 - val_loss: 415.7488 - val_mae: 415.7487\n",
      "Epoch 142/145\n",
      " - 4s - loss: 359.0799 - mae: 359.0800 - val_loss: 415.8293 - val_mae: 415.8293\n",
      "Epoch 143/145\n",
      " - 4s - loss: 358.9807 - mae: 358.9808 - val_loss: 415.8667 - val_mae: 415.8668\n",
      "Epoch 144/145\n",
      " - 4s - loss: 358.8614 - mae: 358.8614 - val_loss: 416.0872 - val_mae: 416.0872\n",
      "Epoch 145/145\n",
      " - 4s - loss: 358.6652 - mae: 358.6652 - val_loss: 415.4259 - val_mae: 415.4259\n",
      "\n",
      "val_mae is:415.4259248913574\n",
      "\n",
      "fold: 3\n",
      "Train on 125000 samples, validate on 25000 samples\n",
      "Epoch 1/145\n",
      " - 5s - loss: 3103.5222 - mae: 3103.5220 - val_loss: 1015.6876 - val_mae: 1015.6876\n",
      "Epoch 2/145\n",
      " - 4s - loss: 862.6153 - mae: 862.6154 - val_loss: 758.0780 - val_mae: 758.0780\n",
      "Epoch 3/145\n",
      " - 4s - loss: 695.9590 - mae: 695.9589 - val_loss: 654.5665 - val_mae: 654.5665\n",
      "Epoch 4/145\n",
      " - 4s - loss: 626.6420 - mae: 626.6419 - val_loss: 630.4439 - val_mae: 630.4439\n",
      "Epoch 5/145\n",
      " - 4s - loss: 582.7547 - mae: 582.7547 - val_loss: 605.3452 - val_mae: 605.3452\n",
      "Epoch 6/145\n",
      " - 4s - loss: 563.8075 - mae: 563.8075 - val_loss: 572.6840 - val_mae: 572.6840\n",
      "Epoch 7/145\n",
      " - 4s - loss: 540.3621 - mae: 540.3621 - val_loss: 594.6979 - val_mae: 594.6979\n",
      "Epoch 8/145\n",
      " - 4s - loss: 594.0480 - mae: 594.0480 - val_loss: 565.8536 - val_mae: 565.8536\n",
      "Epoch 9/145\n",
      " - 4s - loss: 541.2373 - mae: 541.2374 - val_loss: 523.5787 - val_mae: 523.5786\n",
      "Epoch 10/145\n",
      " - 4s - loss: 517.0914 - mae: 517.0915 - val_loss: 557.6079 - val_mae: 557.6079\n",
      "Epoch 11/145\n",
      " - 4s - loss: 529.1941 - mae: 529.1941 - val_loss: 616.0958 - val_mae: 616.0957\n",
      "Epoch 12/145\n",
      " - 4s - loss: 533.9983 - mae: 533.9984 - val_loss: 511.4943 - val_mae: 511.4943\n",
      "Epoch 13/145\n",
      " - 4s - loss: 495.2842 - mae: 495.2842 - val_loss: 509.9366 - val_mae: 509.9366\n",
      "Epoch 14/145\n",
      " - 4s - loss: 496.6192 - mae: 496.6193 - val_loss: 504.7675 - val_mae: 504.7675\n",
      "Epoch 15/145\n",
      " - 4s - loss: 495.5852 - mae: 495.5852 - val_loss: 499.0677 - val_mae: 499.0677\n",
      "Epoch 16/145\n",
      " - 4s - loss: 534.3779 - mae: 534.3780 - val_loss: 500.9989 - val_mae: 500.9988\n",
      "Epoch 17/145\n",
      " - 4s - loss: 480.0157 - mae: 480.0157 - val_loss: 504.5153 - val_mae: 504.5153\n",
      "Epoch 18/145\n",
      " - 4s - loss: 473.5311 - mae: 473.5310 - val_loss: 516.7649 - val_mae: 516.7649\n",
      "Epoch 19/145\n",
      " - 4s - loss: 476.2091 - mae: 476.2090 - val_loss: 485.6147 - val_mae: 485.6147\n",
      "Epoch 20/145\n",
      " - 4s - loss: 469.9676 - mae: 469.9675 - val_loss: 486.7680 - val_mae: 486.7679\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      " - 4s - loss: 450.4422 - mae: 450.4421 - val_loss: 472.8582 - val_mae: 472.8582\n",
      "Epoch 22/145\n",
      " - 4s - loss: 445.0995 - mae: 445.0995 - val_loss: 471.1569 - val_mae: 471.1569\n",
      "Epoch 23/145\n",
      " - 4s - loss: 446.6772 - mae: 446.6772 - val_loss: 469.8544 - val_mae: 469.8544\n",
      "Epoch 24/145\n",
      " - 4s - loss: 443.8366 - mae: 443.8367 - val_loss: 470.9251 - val_mae: 470.9251\n",
      "Epoch 25/145\n",
      " - 4s - loss: 443.2321 - mae: 443.2321 - val_loss: 466.4472 - val_mae: 466.4472\n",
      "Epoch 26/145\n",
      " - 4s - loss: 445.6048 - mae: 445.6049 - val_loss: 465.6805 - val_mae: 465.6805\n",
      "Epoch 27/145\n",
      " - 5s - loss: 442.7993 - mae: 442.7993 - val_loss: 498.0331 - val_mae: 498.0331\n",
      "Epoch 28/145\n",
      " - 4s - loss: 450.1239 - mae: 450.1238 - val_loss: 463.2528 - val_mae: 463.2528\n",
      "Epoch 29/145\n",
      " - 4s - loss: 468.8863 - mae: 468.8863 - val_loss: 464.4616 - val_mae: 464.4616\n",
      "Epoch 30/145\n",
      " - 4s - loss: 434.5551 - mae: 434.5551 - val_loss: 461.0218 - val_mae: 461.0219\n",
      "Epoch 31/145\n",
      " - 4s - loss: 435.1392 - mae: 435.1391 - val_loss: 458.8014 - val_mae: 458.8014\n",
      "Epoch 32/145\n",
      " - 4s - loss: 433.8558 - mae: 433.8557 - val_loss: 459.0489 - val_mae: 459.0489\n",
      "Epoch 33/145\n",
      " - 4s - loss: 429.4943 - mae: 429.4943 - val_loss: 461.6526 - val_mae: 461.6526\n",
      "Epoch 34/145\n",
      " - 4s - loss: 433.2181 - mae: 433.2180 - val_loss: 457.3281 - val_mae: 457.3280\n",
      "Epoch 35/145\n",
      " - 4s - loss: 428.1095 - mae: 428.1095 - val_loss: 450.7899 - val_mae: 450.7898\n",
      "Epoch 36/145\n",
      " - 5s - loss: 423.6884 - mae: 423.6884 - val_loss: 453.2689 - val_mae: 453.2689\n",
      "Epoch 37/145\n",
      " - 6s - loss: 430.7896 - mae: 430.7896 - val_loss: 458.1568 - val_mae: 458.1567\n",
      "Epoch 38/145\n",
      " - 4s - loss: 429.2731 - mae: 429.2730 - val_loss: 461.4847 - val_mae: 461.4847\n",
      "Epoch 39/145\n",
      " - 4s - loss: 438.9685 - mae: 438.9684 - val_loss: 463.7099 - val_mae: 463.7098\n",
      "Epoch 40/145\n",
      " - 4s - loss: 426.0009 - mae: 426.0009 - val_loss: 445.8371 - val_mae: 445.8370\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      " - 4s - loss: 413.0120 - mae: 413.0120 - val_loss: 443.9635 - val_mae: 443.9635\n",
      "Epoch 42/145\n",
      " - 4s - loss: 411.2053 - mae: 411.2053 - val_loss: 442.0593 - val_mae: 442.0593\n",
      "Epoch 43/145\n",
      " - 4s - loss: 408.5108 - mae: 408.5108 - val_loss: 444.6653 - val_mae: 444.6653\n",
      "Epoch 44/145\n",
      " - 4s - loss: 408.5850 - mae: 408.5849 - val_loss: 443.6794 - val_mae: 443.6794\n",
      "Epoch 45/145\n",
      " - 4s - loss: 409.3736 - mae: 409.3735 - val_loss: 447.4239 - val_mae: 447.4240\n",
      "Epoch 46/145\n",
      " - 4s - loss: 410.3383 - mae: 410.3382 - val_loss: 471.9504 - val_mae: 471.9504\n",
      "Epoch 47/145\n",
      " - 4s - loss: 414.5770 - mae: 414.5770 - val_loss: 446.0098 - val_mae: 446.0098\n",
      "Epoch 48/145\n",
      " - 4s - loss: 413.6663 - mae: 413.6663 - val_loss: 443.3395 - val_mae: 443.3396\n",
      "Epoch 49/145\n",
      " - 4s - loss: 405.6774 - mae: 405.6774 - val_loss: 442.8085 - val_mae: 442.8085\n",
      "Epoch 50/145\n",
      " - 4s - loss: 403.7337 - mae: 403.7338 - val_loss: 442.3873 - val_mae: 442.3873\n",
      "Epoch 51/145\n",
      " - 4s - loss: 405.0852 - mae: 405.0852 - val_loss: 444.1568 - val_mae: 444.1568\n",
      "Epoch 52/145\n",
      " - 4s - loss: 412.1291 - mae: 412.1292 - val_loss: 445.6175 - val_mae: 445.6174\n",
      "Epoch 53/145\n",
      " - 4s - loss: 413.4131 - mae: 413.4131 - val_loss: 452.2544 - val_mae: 452.2544\n",
      "Epoch 54/145\n",
      " - 4s - loss: 405.6263 - mae: 405.6262 - val_loss: 440.8642 - val_mae: 440.8642\n",
      "Epoch 55/145\n",
      " - 4s - loss: 402.6577 - mae: 402.6576 - val_loss: 445.8764 - val_mae: 445.8764\n",
      "Epoch 56/145\n",
      " - 4s - loss: 402.7840 - mae: 402.7840 - val_loss: 448.4094 - val_mae: 448.4095\n",
      "Epoch 57/145\n",
      " - 4s - loss: 404.0055 - mae: 404.0054 - val_loss: 439.5538 - val_mae: 439.5538\n",
      "Epoch 58/145\n",
      " - 4s - loss: 401.4404 - mae: 401.4403 - val_loss: 444.3638 - val_mae: 444.3637\n",
      "Epoch 59/145\n",
      " - 4s - loss: 402.5923 - mae: 402.5923 - val_loss: 438.3849 - val_mae: 438.3849\n",
      "Epoch 60/145\n",
      " - 4s - loss: 399.7486 - mae: 399.7486 - val_loss: 439.0663 - val_mae: 439.0663\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      " - 4s - loss: 393.6707 - mae: 393.6707 - val_loss: 437.1706 - val_mae: 437.1706\n",
      "Epoch 62/145\n",
      " - 4s - loss: 393.4190 - mae: 393.4190 - val_loss: 437.6946 - val_mae: 437.6946\n",
      "Epoch 63/145\n",
      " - 4s - loss: 391.8311 - mae: 391.8311 - val_loss: 435.6927 - val_mae: 435.6927\n",
      "Epoch 64/145\n",
      " - 4s - loss: 394.2069 - mae: 394.2069 - val_loss: 436.0650 - val_mae: 436.0650\n",
      "Epoch 65/145\n",
      " - 4s - loss: 392.6228 - mae: 392.6228 - val_loss: 439.1510 - val_mae: 439.1510\n",
      "Epoch 66/145\n",
      " - 4s - loss: 392.6766 - mae: 392.6766 - val_loss: 436.1699 - val_mae: 436.1698\n",
      "Epoch 67/145\n",
      " - 4s - loss: 391.2189 - mae: 391.2190 - val_loss: 434.8311 - val_mae: 434.8311\n",
      "Epoch 68/145\n",
      " - 4s - loss: 390.5166 - mae: 390.5167 - val_loss: 436.3720 - val_mae: 436.3720\n",
      "Epoch 69/145\n",
      " - 4s - loss: 390.5407 - mae: 390.5407 - val_loss: 435.1237 - val_mae: 435.1237\n",
      "Epoch 70/145\n",
      " - 4s - loss: 389.0679 - mae: 389.0680 - val_loss: 435.0239 - val_mae: 435.0239\n",
      "Epoch 71/145\n",
      " - 4s - loss: 390.9681 - mae: 390.9681 - val_loss: 435.5504 - val_mae: 435.5504\n",
      "Epoch 72/145\n",
      " - 4s - loss: 390.4358 - mae: 390.4359 - val_loss: 436.6648 - val_mae: 436.6648\n",
      "Epoch 73/145\n",
      " - 4s - loss: 388.0201 - mae: 388.0201 - val_loss: 433.4422 - val_mae: 433.4422\n",
      "Epoch 74/145\n",
      " - 4s - loss: 387.2957 - mae: 387.2957 - val_loss: 434.9976 - val_mae: 434.9977\n",
      "Epoch 75/145\n",
      " - 4s - loss: 390.4563 - mae: 390.4564 - val_loss: 434.5879 - val_mae: 434.5879\n",
      "Epoch 76/145\n",
      " - 4s - loss: 391.7891 - mae: 391.7891 - val_loss: 433.7144 - val_mae: 433.7144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/145\n",
      " - 4s - loss: 386.2156 - mae: 386.2157 - val_loss: 434.2686 - val_mae: 434.2686\n",
      "Epoch 78/145\n",
      " - 4s - loss: 387.2894 - mae: 387.2895 - val_loss: 436.2154 - val_mae: 436.2154\n",
      "Epoch 79/145\n",
      " - 4s - loss: 386.1818 - mae: 386.1817 - val_loss: 433.8536 - val_mae: 433.8536\n",
      "Epoch 80/145\n",
      " - 4s - loss: 385.9157 - mae: 385.9157 - val_loss: 436.3356 - val_mae: 436.3356\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      " - 4s - loss: 383.0362 - mae: 383.0363 - val_loss: 431.8729 - val_mae: 431.8730\n",
      "Epoch 82/145\n",
      " - 4s - loss: 381.8215 - mae: 381.8215 - val_loss: 431.8538 - val_mae: 431.8538\n",
      "Epoch 83/145\n",
      " - 4s - loss: 381.3619 - mae: 381.3618 - val_loss: 432.4404 - val_mae: 432.4404\n",
      "Epoch 84/145\n",
      " - 5s - loss: 382.0029 - mae: 382.0030 - val_loss: 432.1397 - val_mae: 432.1397\n",
      "Epoch 85/145\n",
      " - 4s - loss: 381.5610 - mae: 381.5610 - val_loss: 432.7401 - val_mae: 432.7401\n",
      "Epoch 86/145\n",
      " - 4s - loss: 381.5317 - mae: 381.5316 - val_loss: 433.5587 - val_mae: 433.5587\n",
      "Epoch 87/145\n",
      " - 4s - loss: 380.9450 - mae: 380.9449 - val_loss: 432.1979 - val_mae: 432.1980\n",
      "Epoch 88/145\n",
      " - 4s - loss: 380.5895 - mae: 380.5894 - val_loss: 432.8659 - val_mae: 432.8659\n",
      "Epoch 89/145\n",
      " - 4s - loss: 380.7424 - mae: 380.7425 - val_loss: 432.7695 - val_mae: 432.7695\n",
      "Epoch 90/145\n",
      " - 4s - loss: 380.1210 - mae: 380.1210 - val_loss: 433.3295 - val_mae: 433.3294\n",
      "Epoch 91/145\n",
      " - 4s - loss: 380.2007 - mae: 380.2007 - val_loss: 432.3393 - val_mae: 432.3393\n",
      "Epoch 92/145\n",
      " - 4s - loss: 380.2848 - mae: 380.2848 - val_loss: 432.6082 - val_mae: 432.6082\n",
      "Epoch 93/145\n",
      " - 4s - loss: 379.4029 - mae: 379.4030 - val_loss: 432.4306 - val_mae: 432.4306\n",
      "Epoch 94/145\n",
      " - 4s - loss: 379.3206 - mae: 379.3206 - val_loss: 430.8481 - val_mae: 430.8481\n",
      "Epoch 95/145\n",
      " - 4s - loss: 377.9165 - mae: 377.9165 - val_loss: 430.9911 - val_mae: 430.9911\n",
      "Epoch 96/145\n",
      " - 4s - loss: 379.8540 - mae: 379.8540 - val_loss: 431.3543 - val_mae: 431.3543\n",
      "Epoch 97/145\n",
      " - 4s - loss: 380.7918 - mae: 380.7917 - val_loss: 432.5512 - val_mae: 432.5511\n",
      "Epoch 98/145\n",
      " - 4s - loss: 378.4290 - mae: 378.4290 - val_loss: 431.8972 - val_mae: 431.8972\n",
      "Epoch 99/145\n",
      " - 4s - loss: 378.3646 - mae: 378.3645 - val_loss: 435.5332 - val_mae: 435.5332\n",
      "Epoch 100/145\n",
      " - 4s - loss: 378.6879 - mae: 378.6879 - val_loss: 431.4204 - val_mae: 431.4204\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      " - 4s - loss: 375.4307 - mae: 375.4306 - val_loss: 430.7848 - val_mae: 430.7849\n",
      "Epoch 102/145\n",
      " - 4s - loss: 375.9784 - mae: 375.9784 - val_loss: 431.0912 - val_mae: 431.0912\n",
      "Epoch 103/145\n",
      " - 4s - loss: 375.0613 - mae: 375.0613 - val_loss: 429.7488 - val_mae: 429.7488\n",
      "Epoch 104/145\n",
      " - 4s - loss: 375.7131 - mae: 375.7131 - val_loss: 431.8781 - val_mae: 431.8781\n",
      "Epoch 105/145\n",
      " - 4s - loss: 374.6752 - mae: 374.6752 - val_loss: 431.8297 - val_mae: 431.8298\n",
      "Epoch 106/145\n",
      " - 4s - loss: 374.5889 - mae: 374.5889 - val_loss: 436.3714 - val_mae: 436.3715\n",
      "Epoch 107/145\n",
      " - 4s - loss: 374.4910 - mae: 374.4910 - val_loss: 431.9609 - val_mae: 431.9610\n",
      "Epoch 108/145\n",
      " - 4s - loss: 374.7671 - mae: 374.7671 - val_loss: 431.3949 - val_mae: 431.3949\n",
      "Epoch 109/145\n",
      " - 4s - loss: 374.2690 - mae: 374.2690 - val_loss: 429.7011 - val_mae: 429.7012\n",
      "Epoch 110/145\n",
      " - 4s - loss: 373.9718 - mae: 373.9718 - val_loss: 430.5483 - val_mae: 430.5483\n",
      "Epoch 111/145\n",
      " - 4s - loss: 374.8040 - mae: 374.8040 - val_loss: 431.0829 - val_mae: 431.0829\n",
      "Epoch 112/145\n",
      " - 4s - loss: 373.8001 - mae: 373.8002 - val_loss: 430.6806 - val_mae: 430.6806\n",
      "Epoch 113/145\n",
      " - 4s - loss: 373.7641 - mae: 373.7640 - val_loss: 430.8218 - val_mae: 430.8218\n",
      "Epoch 114/145\n",
      " - 5s - loss: 373.5435 - mae: 373.5435 - val_loss: 430.1317 - val_mae: 430.1317\n",
      "Epoch 115/145\n",
      " - 4s - loss: 372.7974 - mae: 372.7974 - val_loss: 430.5622 - val_mae: 430.5622\n",
      "Epoch 116/145\n",
      " - 4s - loss: 372.9259 - mae: 372.9260 - val_loss: 431.1831 - val_mae: 431.1831\n",
      "Epoch 117/145\n",
      " - 4s - loss: 372.8487 - mae: 372.8488 - val_loss: 430.0262 - val_mae: 430.0262\n",
      "Epoch 118/145\n",
      " - 3s - loss: 373.2973 - mae: 373.2973 - val_loss: 431.1427 - val_mae: 431.1427\n",
      "Epoch 119/145\n",
      " - 4s - loss: 372.7293 - mae: 372.7294 - val_loss: 431.5128 - val_mae: 431.5128\n",
      "Epoch 120/145\n",
      " - 4s - loss: 372.7189 - mae: 372.7190 - val_loss: 430.2469 - val_mae: 430.2469\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      " - 4s - loss: 371.0372 - mae: 371.0371 - val_loss: 430.0415 - val_mae: 430.0416\n",
      "Epoch 122/145\n",
      " - 4s - loss: 370.7274 - mae: 370.7272 - val_loss: 430.5699 - val_mae: 430.5699\n",
      "Epoch 123/145\n",
      " - 4s - loss: 370.9304 - mae: 370.9304 - val_loss: 430.5626 - val_mae: 430.5626\n",
      "Epoch 124/145\n",
      " - 4s - loss: 370.5971 - mae: 370.5970 - val_loss: 430.3571 - val_mae: 430.3571\n",
      "Epoch 125/145\n",
      " - 4s - loss: 370.8079 - mae: 370.8079 - val_loss: 429.8313 - val_mae: 429.8313\n",
      "Epoch 126/145\n",
      " - 4s - loss: 370.3176 - mae: 370.3176 - val_loss: 429.8854 - val_mae: 429.8853\n",
      "Epoch 127/145\n",
      " - 4s - loss: 369.9803 - mae: 369.9802 - val_loss: 430.7282 - val_mae: 430.7281\n",
      "Epoch 128/145\n",
      " - 4s - loss: 370.3081 - mae: 370.3081 - val_loss: 431.3353 - val_mae: 431.3353\n",
      "Epoch 129/145\n",
      " - 4s - loss: 370.3420 - mae: 370.3420 - val_loss: 430.5667 - val_mae: 430.5667\n",
      "Epoch 130/145\n",
      " - 4s - loss: 370.3650 - mae: 370.3649 - val_loss: 430.5386 - val_mae: 430.5386\n",
      "Epoch 131/145\n",
      " - 4s - loss: 369.6601 - mae: 369.6601 - val_loss: 430.0481 - val_mae: 430.0480\n",
      "Epoch 132/145\n",
      " - 4s - loss: 369.8252 - mae: 369.8252 - val_loss: 430.6875 - val_mae: 430.6875\n",
      "Epoch 133/145\n",
      " - 4s - loss: 369.6803 - mae: 369.6803 - val_loss: 430.2568 - val_mae: 430.2568\n",
      "Epoch 134/145\n",
      " - 4s - loss: 369.9056 - mae: 369.9056 - val_loss: 430.4974 - val_mae: 430.4974\n",
      "Epoch 135/145\n",
      " - 4s - loss: 369.8838 - mae: 369.8838 - val_loss: 430.7938 - val_mae: 430.7938\n",
      "Epoch 136/145\n",
      " - 3s - loss: 370.0321 - mae: 370.0321 - val_loss: 431.9155 - val_mae: 431.9155\n",
      "Epoch 137/145\n",
      " - 3s - loss: 369.9004 - mae: 369.9005 - val_loss: 431.2502 - val_mae: 431.2502\n",
      "Epoch 138/145\n",
      " - 3s - loss: 369.3330 - mae: 369.3331 - val_loss: 429.5533 - val_mae: 429.5534\n",
      "Epoch 139/145\n",
      " - 3s - loss: 368.9468 - mae: 368.9468 - val_loss: 429.8649 - val_mae: 429.8649\n",
      "Epoch 140/145\n",
      " - 4s - loss: 368.9145 - mae: 368.9145 - val_loss: 430.0084 - val_mae: 430.0085\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      " - 3s - loss: 368.2797 - mae: 368.2798 - val_loss: 429.5616 - val_mae: 429.5616\n",
      "Epoch 142/145\n",
      " - 3s - loss: 367.9634 - mae: 367.9634 - val_loss: 429.9807 - val_mae: 429.9807\n",
      "Epoch 143/145\n",
      " - 3s - loss: 367.9709 - mae: 367.9709 - val_loss: 430.0557 - val_mae: 430.0558\n",
      "Epoch 144/145\n",
      " - 3s - loss: 368.3135 - mae: 368.3136 - val_loss: 429.8762 - val_mae: 429.8762\n",
      "Epoch 145/145\n",
      " - 3s - loss: 367.8223 - mae: 367.8223 - val_loss: 429.6456 - val_mae: 429.6457\n",
      "\n",
      "val_mae is:429.64565713222504\n",
      "\n",
      "fold: 4\n",
      "Train on 125000 samples, validate on 25000 samples\n",
      "Epoch 1/145\n",
      " - 4s - loss: 4140.7333 - mae: 4140.7329 - val_loss: 1249.4872 - val_mae: 1249.4872\n",
      "Epoch 2/145\n",
      " - 4s - loss: 978.7427 - mae: 978.7427 - val_loss: 860.8871 - val_mae: 860.8870\n",
      "Epoch 3/145\n",
      " - 3s - loss: 798.4854 - mae: 798.4854 - val_loss: 707.3770 - val_mae: 707.3770\n",
      "Epoch 4/145\n",
      " - 3s - loss: 660.8187 - mae: 660.8187 - val_loss: 592.6544 - val_mae: 592.6544\n",
      "Epoch 5/145\n",
      " - 3s - loss: 598.7172 - mae: 598.7172 - val_loss: 566.3465 - val_mae: 566.3465\n",
      "Epoch 6/145\n",
      " - 3s - loss: 584.0906 - mae: 584.0906 - val_loss: 565.2538 - val_mae: 565.2538\n",
      "Epoch 7/145\n",
      " - 3s - loss: 540.5708 - mae: 540.5709 - val_loss: 550.0815 - val_mae: 550.0815\n",
      "Epoch 8/145\n",
      " - 3s - loss: 524.9046 - mae: 524.9047 - val_loss: 531.4178 - val_mae: 531.4178\n",
      "Epoch 9/145\n",
      " - 4s - loss: 517.9689 - mae: 517.9689 - val_loss: 508.0188 - val_mae: 508.0188\n",
      "Epoch 10/145\n",
      " - 4s - loss: 506.2133 - mae: 506.2133 - val_loss: 525.2995 - val_mae: 525.2995\n",
      "Epoch 11/145\n",
      " - 4s - loss: 526.1975 - mae: 526.1974 - val_loss: 561.1712 - val_mae: 561.1713\n",
      "Epoch 12/145\n",
      " - 4s - loss: 499.7950 - mae: 499.7950 - val_loss: 473.9099 - val_mae: 473.9099\n",
      "Epoch 13/145\n",
      " - 3s - loss: 485.0113 - mae: 485.0114 - val_loss: 483.8714 - val_mae: 483.8714\n",
      "Epoch 14/145\n",
      " - 3s - loss: 526.9116 - mae: 526.9116 - val_loss: 501.4906 - val_mae: 501.4906\n",
      "Epoch 15/145\n",
      " - 3s - loss: 494.5271 - mae: 494.5271 - val_loss: 486.8177 - val_mae: 486.8176\n",
      "Epoch 16/145\n",
      " - 3s - loss: 468.0129 - mae: 468.0129 - val_loss: 457.8988 - val_mae: 457.8988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/145\n",
      " - 3s - loss: 469.5185 - mae: 469.5185 - val_loss: 459.3752 - val_mae: 459.3752\n",
      "Epoch 18/145\n",
      " - 3s - loss: 464.4343 - mae: 464.4343 - val_loss: 474.6750 - val_mae: 474.6750\n",
      "Epoch 19/145\n",
      " - 3s - loss: 464.2584 - mae: 464.2584 - val_loss: 452.3513 - val_mae: 452.3513\n",
      "Epoch 20/145\n",
      " - 3s - loss: 451.5213 - mae: 451.5213 - val_loss: 464.5713 - val_mae: 464.5713\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      " - 3s - loss: 440.0547 - mae: 440.0547 - val_loss: 440.4808 - val_mae: 440.4808\n",
      "Epoch 22/145\n",
      " - 3s - loss: 439.8500 - mae: 439.8500 - val_loss: 438.4206 - val_mae: 438.4206\n",
      "Epoch 23/145\n",
      " - 3s - loss: 435.0113 - mae: 435.0111 - val_loss: 440.4934 - val_mae: 440.4934\n",
      "Epoch 24/145\n",
      " - 3s - loss: 433.3361 - mae: 433.3362 - val_loss: 441.4137 - val_mae: 441.4138\n",
      "Epoch 25/145\n",
      " - 3s - loss: 434.9581 - mae: 434.9581 - val_loss: 447.5729 - val_mae: 447.5729\n",
      "Epoch 26/145\n",
      " - 4s - loss: 432.4048 - mae: 432.4049 - val_loss: 435.8342 - val_mae: 435.8342\n",
      "Epoch 27/145\n",
      " - 4s - loss: 426.6905 - mae: 426.6904 - val_loss: 432.6423 - val_mae: 432.6422\n",
      "Epoch 28/145\n",
      " - 4s - loss: 424.9842 - mae: 424.9842 - val_loss: 431.0390 - val_mae: 431.0390\n",
      "Epoch 29/145\n",
      " - 3s - loss: 422.6299 - mae: 422.6300 - val_loss: 429.1300 - val_mae: 429.1300\n",
      "Epoch 30/145\n",
      " - 3s - loss: 423.4231 - mae: 423.4231 - val_loss: 432.8174 - val_mae: 432.8174\n",
      "Epoch 31/145\n",
      " - 4s - loss: 421.5361 - mae: 421.5362 - val_loss: 447.1845 - val_mae: 447.1846\n",
      "Epoch 32/145\n",
      " - 4s - loss: 428.0111 - mae: 428.0110 - val_loss: 430.5862 - val_mae: 430.5862\n",
      "Epoch 33/145\n",
      " - 4s - loss: 420.9261 - mae: 420.9261 - val_loss: 436.0688 - val_mae: 436.0688\n",
      "Epoch 34/145\n",
      " - 4s - loss: 422.7812 - mae: 422.7812 - val_loss: 428.6843 - val_mae: 428.6843\n",
      "Epoch 35/145\n",
      " - 4s - loss: 427.7792 - mae: 427.7792 - val_loss: 440.8470 - val_mae: 440.8470\n",
      "Epoch 36/145\n",
      " - 3s - loss: 421.3842 - mae: 421.3842 - val_loss: 429.1350 - val_mae: 429.1350\n",
      "Epoch 37/145\n",
      " - 3s - loss: 416.8244 - mae: 416.8243 - val_loss: 434.6045 - val_mae: 434.6045\n",
      "Epoch 38/145\n",
      " - 3s - loss: 416.7951 - mae: 416.7952 - val_loss: 424.5022 - val_mae: 424.5022\n",
      "Epoch 39/145\n",
      " - 3s - loss: 414.9243 - mae: 414.9244 - val_loss: 432.0944 - val_mae: 432.0944\n",
      "Epoch 40/145\n",
      " - 3s - loss: 411.8946 - mae: 411.8946 - val_loss: 422.9548 - val_mae: 422.9548\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      " - 3s - loss: 402.5688 - mae: 402.5688 - val_loss: 420.1088 - val_mae: 420.1088\n",
      "Epoch 42/145\n",
      " - 3s - loss: 401.8308 - mae: 401.8308 - val_loss: 423.7456 - val_mae: 423.7456\n",
      "Epoch 43/145\n",
      " - 3s - loss: 401.2811 - mae: 401.2811 - val_loss: 421.0394 - val_mae: 421.0394\n",
      "Epoch 44/145\n",
      " - 3s - loss: 399.9755 - mae: 399.9755 - val_loss: 421.7286 - val_mae: 421.7285\n",
      "Epoch 45/145\n",
      " - 3s - loss: 398.2464 - mae: 398.2463 - val_loss: 418.9209 - val_mae: 418.9209\n",
      "Epoch 46/145\n",
      " - 3s - loss: 399.4806 - mae: 399.4805 - val_loss: 423.3994 - val_mae: 423.3994\n",
      "Epoch 47/145\n",
      " - 3s - loss: 401.0027 - mae: 401.0028 - val_loss: 422.1076 - val_mae: 422.1076\n",
      "Epoch 48/145\n",
      " - 3s - loss: 398.9992 - mae: 398.9992 - val_loss: 423.3828 - val_mae: 423.3828\n",
      "Epoch 49/145\n",
      " - 3s - loss: 396.2218 - mae: 396.2217 - val_loss: 421.3050 - val_mae: 421.3050\n",
      "Epoch 50/145\n",
      " - 3s - loss: 397.1813 - mae: 397.1812 - val_loss: 420.7188 - val_mae: 420.7188\n",
      "Epoch 51/145\n",
      " - 3s - loss: 394.7997 - mae: 394.7996 - val_loss: 421.0379 - val_mae: 421.0379\n",
      "Epoch 52/145\n",
      " - 4s - loss: 395.2794 - mae: 395.2794 - val_loss: 419.2520 - val_mae: 419.2520\n",
      "Epoch 53/145\n",
      " - 3s - loss: 394.3645 - mae: 394.3644 - val_loss: 424.3817 - val_mae: 424.3818\n",
      "Epoch 54/145\n",
      " - 3s - loss: 394.8974 - mae: 394.8975 - val_loss: 431.6607 - val_mae: 431.6607\n",
      "Epoch 55/145\n",
      " - 3s - loss: 395.8123 - mae: 395.8123 - val_loss: 434.5925 - val_mae: 434.5925\n",
      "Epoch 56/145\n",
      " - 3s - loss: 392.3106 - mae: 392.3105 - val_loss: 419.3854 - val_mae: 419.3854\n",
      "Epoch 57/145\n",
      " - 3s - loss: 392.6268 - mae: 392.6267 - val_loss: 417.5996 - val_mae: 417.5996\n",
      "Epoch 58/145\n",
      " - 3s - loss: 393.3027 - mae: 393.3026 - val_loss: 429.3729 - val_mae: 429.3728\n",
      "Epoch 59/145\n",
      " - 3s - loss: 398.9412 - mae: 398.9412 - val_loss: 420.9217 - val_mae: 420.9217\n",
      "Epoch 60/145\n",
      " - 3s - loss: 393.0455 - mae: 393.0455 - val_loss: 418.0091 - val_mae: 418.0091\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      " - 3s - loss: 383.4207 - mae: 383.4207 - val_loss: 417.0439 - val_mae: 417.0439\n",
      "Epoch 62/145\n",
      " - 4s - loss: 382.4962 - mae: 382.4962 - val_loss: 415.8948 - val_mae: 415.8948\n",
      "Epoch 63/145\n",
      " - 3s - loss: 382.3143 - mae: 382.3143 - val_loss: 418.0575 - val_mae: 418.0575\n",
      "Epoch 64/145\n",
      " - 3s - loss: 382.3149 - mae: 382.3150 - val_loss: 420.0200 - val_mae: 420.0200\n",
      "Epoch 65/145\n",
      " - 3s - loss: 381.7741 - mae: 381.7740 - val_loss: 418.1959 - val_mae: 418.1959\n",
      "Epoch 66/145\n",
      " - 3s - loss: 382.1602 - mae: 382.1602 - val_loss: 422.4322 - val_mae: 422.4322\n",
      "Epoch 67/145\n",
      " - 3s - loss: 382.1284 - mae: 382.1285 - val_loss: 418.5470 - val_mae: 418.5469\n",
      "Epoch 68/145\n",
      " - 3s - loss: 380.9083 - mae: 380.9084 - val_loss: 416.9197 - val_mae: 416.9197\n",
      "Epoch 69/145\n",
      " - 3s - loss: 381.6158 - mae: 381.6158 - val_loss: 423.9036 - val_mae: 423.9036\n",
      "Epoch 70/145\n",
      " - 3s - loss: 379.7230 - mae: 379.7230 - val_loss: 422.0418 - val_mae: 422.0418\n",
      "Epoch 71/145\n",
      " - 3s - loss: 382.4264 - mae: 382.4264 - val_loss: 416.7006 - val_mae: 416.7005\n",
      "Epoch 72/145\n",
      " - 3s - loss: 381.0372 - mae: 381.0372 - val_loss: 417.2219 - val_mae: 417.2220\n",
      "Epoch 73/145\n",
      " - 3s - loss: 379.7256 - mae: 379.7257 - val_loss: 424.1920 - val_mae: 424.1920\n",
      "Epoch 74/145\n",
      " - 3s - loss: 379.2242 - mae: 379.2242 - val_loss: 417.1544 - val_mae: 417.1544\n",
      "Epoch 75/145\n",
      " - 3s - loss: 377.5436 - mae: 377.5436 - val_loss: 418.0577 - val_mae: 418.0578\n",
      "Epoch 76/145\n",
      " - 3s - loss: 377.2588 - mae: 377.2588 - val_loss: 418.6260 - val_mae: 418.6260\n",
      "Epoch 77/145\n",
      " - 3s - loss: 375.9101 - mae: 375.9102 - val_loss: 417.9168 - val_mae: 417.9168\n",
      "Epoch 78/145\n",
      " - 3s - loss: 378.1292 - mae: 378.1292 - val_loss: 417.0565 - val_mae: 417.0565\n",
      "Epoch 79/145\n",
      " - 3s - loss: 374.3800 - mae: 374.3799 - val_loss: 417.9208 - val_mae: 417.9207\n",
      "Epoch 80/145\n",
      " - 3s - loss: 375.8459 - mae: 375.8459 - val_loss: 428.8209 - val_mae: 428.8208\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      " - 4s - loss: 373.0613 - mae: 373.0612 - val_loss: 416.3394 - val_mae: 416.3394\n",
      "Epoch 82/145\n",
      " - 4s - loss: 371.0530 - mae: 371.0529 - val_loss: 415.4050 - val_mae: 415.4050\n",
      "Epoch 83/145\n",
      " - 4s - loss: 370.6029 - mae: 370.6030 - val_loss: 416.2647 - val_mae: 416.2648\n",
      "Epoch 84/145\n",
      " - 4s - loss: 370.1260 - mae: 370.1259 - val_loss: 415.8742 - val_mae: 415.8742\n",
      "Epoch 85/145\n",
      " - 4s - loss: 369.5559 - mae: 369.5559 - val_loss: 415.8272 - val_mae: 415.8272\n",
      "Epoch 86/145\n",
      " - 4s - loss: 369.0946 - mae: 369.0946 - val_loss: 417.8611 - val_mae: 417.8611\n",
      "Epoch 87/145\n",
      " - 4s - loss: 369.2623 - mae: 369.2623 - val_loss: 416.3276 - val_mae: 416.3276\n",
      "Epoch 88/145\n",
      " - 4s - loss: 369.1915 - mae: 369.1915 - val_loss: 415.6437 - val_mae: 415.6437\n",
      "Epoch 89/145\n",
      " - 4s - loss: 368.3623 - mae: 368.3623 - val_loss: 415.3106 - val_mae: 415.3106\n",
      "Epoch 90/145\n",
      " - 4s - loss: 368.4330 - mae: 368.4330 - val_loss: 417.1116 - val_mae: 417.1117\n",
      "Epoch 91/145\n",
      " - 3s - loss: 367.6852 - mae: 367.6852 - val_loss: 418.6123 - val_mae: 418.6124\n",
      "Epoch 92/145\n",
      " - 3s - loss: 368.9228 - mae: 368.9229 - val_loss: 416.3384 - val_mae: 416.3383\n",
      "Epoch 93/145\n",
      " - 3s - loss: 367.3288 - mae: 367.3288 - val_loss: 416.9113 - val_mae: 416.9113\n",
      "Epoch 94/145\n",
      " - 3s - loss: 368.0205 - mae: 368.0204 - val_loss: 418.0835 - val_mae: 418.0835\n",
      "Epoch 95/145\n",
      " - 4s - loss: 366.4572 - mae: 366.4571 - val_loss: 415.6629 - val_mae: 415.6630\n",
      "Epoch 96/145\n",
      " - 3s - loss: 366.1066 - mae: 366.1067 - val_loss: 415.7121 - val_mae: 415.7121\n",
      "Epoch 97/145\n",
      " - 3s - loss: 365.4587 - mae: 365.4587 - val_loss: 414.7948 - val_mae: 414.7947\n",
      "Epoch 98/145\n",
      " - 4s - loss: 365.5066 - mae: 365.5066 - val_loss: 416.1878 - val_mae: 416.1878\n",
      "Epoch 99/145\n",
      " - 3s - loss: 365.7719 - mae: 365.7719 - val_loss: 416.4925 - val_mae: 416.4926\n",
      "Epoch 100/145\n",
      " - 3s - loss: 366.5353 - mae: 366.5353 - val_loss: 417.5077 - val_mae: 417.5077\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      " - 3s - loss: 362.8763 - mae: 362.8763 - val_loss: 415.9434 - val_mae: 415.9435\n",
      "Epoch 102/145\n",
      " - 3s - loss: 362.6511 - mae: 362.6511 - val_loss: 415.8022 - val_mae: 415.8022\n",
      "Epoch 103/145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 362.3731 - mae: 362.3731 - val_loss: 414.6740 - val_mae: 414.6740\n",
      "Epoch 104/145\n",
      " - 3s - loss: 362.1257 - mae: 362.1257 - val_loss: 415.9922 - val_mae: 415.9922\n",
      "Epoch 105/145\n",
      " - 3s - loss: 362.2246 - mae: 362.2247 - val_loss: 415.8966 - val_mae: 415.8966\n",
      "Epoch 106/145\n",
      " - 3s - loss: 361.8215 - mae: 361.8215 - val_loss: 415.8130 - val_mae: 415.8130\n",
      "Epoch 107/145\n",
      " - 3s - loss: 362.0878 - mae: 362.0879 - val_loss: 415.6389 - val_mae: 415.6389\n",
      "Epoch 108/145\n",
      " - 3s - loss: 361.1616 - mae: 361.1616 - val_loss: 416.6011 - val_mae: 416.6011\n",
      "Epoch 109/145\n",
      " - 3s - loss: 361.4950 - mae: 361.4950 - val_loss: 415.2746 - val_mae: 415.2746\n",
      "Epoch 110/145\n",
      " - 3s - loss: 360.7192 - mae: 360.7193 - val_loss: 416.0886 - val_mae: 416.0886\n",
      "Epoch 111/145\n",
      " - 3s - loss: 360.7200 - mae: 360.7200 - val_loss: 415.7953 - val_mae: 415.7953\n",
      "Epoch 112/145\n",
      " - 3s - loss: 360.5541 - mae: 360.5541 - val_loss: 415.7767 - val_mae: 415.7767\n",
      "Epoch 113/145\n",
      " - 4s - loss: 359.9712 - mae: 359.9712 - val_loss: 415.6098 - val_mae: 415.6098\n",
      "Epoch 114/145\n",
      " - 3s - loss: 360.0671 - mae: 360.0671 - val_loss: 417.2466 - val_mae: 417.2466\n",
      "Epoch 115/145\n",
      " - 3s - loss: 359.7481 - mae: 359.7482 - val_loss: 415.6172 - val_mae: 415.6171\n",
      "Epoch 116/145\n",
      " - 3s - loss: 359.7495 - mae: 359.7495 - val_loss: 415.4519 - val_mae: 415.4519\n",
      "Epoch 117/145\n",
      " - 3s - loss: 359.2352 - mae: 359.2352 - val_loss: 416.3099 - val_mae: 416.3098\n",
      "Epoch 118/145\n",
      " - 3s - loss: 358.9593 - mae: 358.9594 - val_loss: 415.5459 - val_mae: 415.5458\n",
      "Epoch 119/145\n",
      " - 3s - loss: 358.7800 - mae: 358.7800 - val_loss: 415.1140 - val_mae: 415.1140\n",
      "Epoch 120/145\n",
      " - 3s - loss: 358.7607 - mae: 358.7607 - val_loss: 415.9863 - val_mae: 415.9863\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      " - 4s - loss: 357.2640 - mae: 357.2640 - val_loss: 415.8252 - val_mae: 415.8252\n",
      "Epoch 122/145\n",
      " - 3s - loss: 357.1228 - mae: 357.1228 - val_loss: 415.3236 - val_mae: 415.3236\n",
      "Epoch 123/145\n",
      " - 3s - loss: 356.6923 - mae: 356.6924 - val_loss: 415.6868 - val_mae: 415.6868\n",
      "Epoch 124/145\n",
      " - 3s - loss: 356.8824 - mae: 356.8824 - val_loss: 415.6254 - val_mae: 415.6254\n",
      "Epoch 125/145\n",
      " - 3s - loss: 356.6321 - mae: 356.6321 - val_loss: 416.2047 - val_mae: 416.2047\n",
      "Epoch 126/145\n",
      " - 4s - loss: 356.2107 - mae: 356.2107 - val_loss: 415.1961 - val_mae: 415.1961\n",
      "Epoch 127/145\n",
      " - 3s - loss: 356.2531 - mae: 356.2531 - val_loss: 415.1050 - val_mae: 415.1049\n",
      "Epoch 128/145\n",
      " - 3s - loss: 356.0515 - mae: 356.0515 - val_loss: 415.6006 - val_mae: 415.6006\n",
      "Epoch 129/145\n",
      " - 3s - loss: 355.9179 - mae: 355.9179 - val_loss: 415.7397 - val_mae: 415.7397\n",
      "Epoch 130/145\n",
      " - 4s - loss: 356.0016 - mae: 356.0016 - val_loss: 415.3230 - val_mae: 415.3230\n",
      "Epoch 131/145\n",
      " - 3s - loss: 356.0833 - mae: 356.0834 - val_loss: 415.3092 - val_mae: 415.3092\n",
      "Epoch 132/145\n",
      " - 4s - loss: 355.5584 - mae: 355.5584 - val_loss: 415.4856 - val_mae: 415.4856\n",
      "Epoch 133/145\n",
      " - 4s - loss: 355.5545 - mae: 355.5544 - val_loss: 416.1876 - val_mae: 416.1877\n",
      "Epoch 134/145\n",
      " - 4s - loss: 355.3740 - mae: 355.3741 - val_loss: 416.1507 - val_mae: 416.1507\n",
      "Epoch 135/145\n",
      " - 3s - loss: 355.1613 - mae: 355.1613 - val_loss: 415.5311 - val_mae: 415.5310\n",
      "Epoch 136/145\n",
      " - 3s - loss: 355.1137 - mae: 355.1138 - val_loss: 416.4265 - val_mae: 416.4265\n",
      "Epoch 137/145\n",
      " - 3s - loss: 354.8850 - mae: 354.8849 - val_loss: 415.9289 - val_mae: 415.9289\n",
      "Epoch 138/145\n",
      " - 3s - loss: 354.7052 - mae: 354.7051 - val_loss: 415.5556 - val_mae: 415.5556\n",
      "Epoch 139/145\n",
      " - 3s - loss: 354.6130 - mae: 354.6131 - val_loss: 415.9827 - val_mae: 415.9827\n",
      "Epoch 140/145\n",
      " - 3s - loss: 354.6773 - mae: 354.6773 - val_loss: 415.9017 - val_mae: 415.9017\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      " - 3s - loss: 353.6502 - mae: 353.6502 - val_loss: 415.6775 - val_mae: 415.6776\n",
      "Epoch 142/145\n",
      " - 3s - loss: 353.6029 - mae: 353.6029 - val_loss: 416.0216 - val_mae: 416.0216\n",
      "Epoch 143/145\n",
      " - 3s - loss: 353.4421 - mae: 353.4421 - val_loss: 415.9812 - val_mae: 415.9812\n",
      "Epoch 144/145\n",
      " - 3s - loss: 353.3625 - mae: 353.3626 - val_loss: 416.5374 - val_mae: 416.5374\n",
      "Epoch 145/145\n",
      " - 3s - loss: 353.4923 - mae: 353.4923 - val_loss: 415.7132 - val_mae: 415.7132\n",
      "\n",
      "val_mae is:415.7131580339813\n",
      "\n",
      "fold: 5\n",
      "Train on 125000 samples, validate on 25000 samples\n",
      "Epoch 1/145\n",
      " - 4s - loss: 2717.8152 - mae: 2717.8149 - val_loss: 1008.0387 - val_mae: 1008.0387\n",
      "Epoch 2/145\n",
      " - 3s - loss: 922.2194 - mae: 922.2195 - val_loss: 836.2477 - val_mae: 836.2477\n",
      "Epoch 3/145\n",
      " - 3s - loss: 732.7973 - mae: 732.7972 - val_loss: 717.4441 - val_mae: 717.4441\n",
      "Epoch 4/145\n",
      " - 4s - loss: 656.5190 - mae: 656.5190 - val_loss: 589.1451 - val_mae: 589.1450\n",
      "Epoch 5/145\n",
      " - 3s - loss: 585.3627 - mae: 585.3628 - val_loss: 600.4749 - val_mae: 600.4750\n",
      "Epoch 6/145\n",
      " - 3s - loss: 592.2965 - mae: 592.2964 - val_loss: 566.6947 - val_mae: 566.6947\n",
      "Epoch 7/145\n",
      " - 3s - loss: 542.2625 - mae: 542.2626 - val_loss: 585.8654 - val_mae: 585.8654\n",
      "Epoch 8/145\n",
      " - 3s - loss: 540.7958 - mae: 540.7958 - val_loss: 586.8642 - val_mae: 586.8641\n",
      "Epoch 9/145\n",
      " - 3s - loss: 552.6116 - mae: 552.6116 - val_loss: 664.2048 - val_mae: 664.2048\n",
      "Epoch 10/145\n",
      " - 4s - loss: 572.4491 - mae: 572.4491 - val_loss: 516.6888 - val_mae: 516.6888\n",
      "Epoch 11/145\n",
      " - 3s - loss: 531.4461 - mae: 531.4461 - val_loss: 517.4388 - val_mae: 517.4388\n",
      "Epoch 12/145\n",
      " - 3s - loss: 524.5564 - mae: 524.5564 - val_loss: 518.7863 - val_mae: 518.7863\n",
      "Epoch 13/145\n",
      " - 4s - loss: 499.1551 - mae: 499.1551 - val_loss: 528.8768 - val_mae: 528.8768\n",
      "Epoch 14/145\n",
      " - 4s - loss: 496.5715 - mae: 496.5715 - val_loss: 536.4235 - val_mae: 536.4235\n",
      "Epoch 15/145\n",
      " - 3s - loss: 501.5530 - mae: 501.5531 - val_loss: 488.2770 - val_mae: 488.2770\n",
      "Epoch 16/145\n",
      " - 3s - loss: 491.6405 - mae: 491.6404 - val_loss: 489.4272 - val_mae: 489.4271\n",
      "Epoch 17/145\n",
      " - 4s - loss: 490.9370 - mae: 490.9369 - val_loss: 499.9096 - val_mae: 499.9095\n",
      "Epoch 18/145\n",
      " - 4s - loss: 502.8530 - mae: 502.8530 - val_loss: 518.8987 - val_mae: 518.8987\n",
      "Epoch 19/145\n",
      " - 4s - loss: 479.7016 - mae: 479.7016 - val_loss: 512.3298 - val_mae: 512.3298\n",
      "Epoch 20/145\n",
      " - 4s - loss: 540.9814 - mae: 540.9814 - val_loss: 508.9646 - val_mae: 508.9646\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      " - 4s - loss: 468.1052 - mae: 468.1053 - val_loss: 469.2283 - val_mae: 469.2283\n",
      "Epoch 22/145\n",
      " - 3s - loss: 448.8349 - mae: 448.8349 - val_loss: 468.0046 - val_mae: 468.0045\n",
      "Epoch 23/145\n",
      " - 4s - loss: 465.9170 - mae: 465.9169 - val_loss: 502.2030 - val_mae: 502.2030\n",
      "Epoch 24/145\n",
      " - 4s - loss: 464.5684 - mae: 464.5684 - val_loss: 470.2407 - val_mae: 470.2406\n",
      "Epoch 25/145\n",
      " - 4s - loss: 444.5611 - mae: 444.5611 - val_loss: 464.6756 - val_mae: 464.6756\n",
      "Epoch 26/145\n",
      " - 4s - loss: 447.4331 - mae: 447.4333 - val_loss: 461.2492 - val_mae: 461.2491\n",
      "Epoch 27/145\n",
      " - 4s - loss: 443.1600 - mae: 443.1600 - val_loss: 456.6453 - val_mae: 456.6452\n",
      "Epoch 28/145\n",
      " - 4s - loss: 442.7526 - mae: 442.7527 - val_loss: 472.9090 - val_mae: 472.9090\n",
      "Epoch 29/145\n",
      " - 4s - loss: 440.8913 - mae: 440.8913 - val_loss: 455.0014 - val_mae: 455.0014\n",
      "Epoch 30/145\n",
      " - 4s - loss: 445.2644 - mae: 445.2644 - val_loss: 461.9641 - val_mae: 461.9641\n",
      "Epoch 31/145\n",
      " - 4s - loss: 438.8270 - mae: 438.8270 - val_loss: 459.8377 - val_mae: 459.8377\n",
      "Epoch 32/145\n",
      " - 4s - loss: 448.6380 - mae: 448.6381 - val_loss: 474.4802 - val_mae: 474.4802\n",
      "Epoch 33/145\n",
      " - 4s - loss: 436.8776 - mae: 436.8776 - val_loss: 453.7228 - val_mae: 453.7229\n",
      "Epoch 34/145\n",
      " - 4s - loss: 444.3292 - mae: 444.3293 - val_loss: 451.6504 - val_mae: 451.6505\n",
      "Epoch 35/145\n",
      " - 4s - loss: 429.6042 - mae: 429.6042 - val_loss: 462.9992 - val_mae: 462.9992\n",
      "Epoch 36/145\n",
      " - 4s - loss: 425.4778 - mae: 425.4778 - val_loss: 452.2338 - val_mae: 452.2338\n",
      "Epoch 37/145\n",
      " - 4s - loss: 427.5135 - mae: 427.5136 - val_loss: 449.2235 - val_mae: 449.2236\n",
      "Epoch 38/145\n",
      " - 4s - loss: 451.1741 - mae: 451.1740 - val_loss: 522.4385 - val_mae: 522.4385\n",
      "Epoch 39/145\n",
      " - 3s - loss: 435.2439 - mae: 435.2439 - val_loss: 454.9245 - val_mae: 454.9245\n",
      "Epoch 40/145\n",
      " - 4s - loss: 434.2856 - mae: 434.2856 - val_loss: 463.6579 - val_mae: 463.6578\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      " - 4s - loss: 416.3080 - mae: 416.3079 - val_loss: 443.8405 - val_mae: 443.8405\n",
      "Epoch 42/145\n",
      " - 4s - loss: 411.8598 - mae: 411.8598 - val_loss: 443.7415 - val_mae: 443.7415\n",
      "Epoch 43/145\n",
      " - 4s - loss: 411.0081 - mae: 411.0080 - val_loss: 441.5251 - val_mae: 441.5251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/145\n",
      " - 4s - loss: 408.7008 - mae: 408.7007 - val_loss: 441.4754 - val_mae: 441.4754\n",
      "Epoch 45/145\n",
      " - 4s - loss: 409.4533 - mae: 409.4533 - val_loss: 445.1753 - val_mae: 445.1753\n",
      "Epoch 46/145\n",
      " - 4s - loss: 407.3516 - mae: 407.3516 - val_loss: 444.1835 - val_mae: 444.1836\n",
      "Epoch 47/145\n",
      " - 4s - loss: 417.4611 - mae: 417.4612 - val_loss: 454.0324 - val_mae: 454.0324\n",
      "Epoch 48/145\n",
      " - 4s - loss: 406.1885 - mae: 406.1885 - val_loss: 442.4927 - val_mae: 442.4928\n",
      "Epoch 49/145\n",
      " - 4s - loss: 406.4706 - mae: 406.4706 - val_loss: 452.2233 - val_mae: 452.2232\n",
      "Epoch 50/145\n",
      " - 4s - loss: 418.4082 - mae: 418.4082 - val_loss: 445.7116 - val_mae: 445.7115\n",
      "Epoch 51/145\n",
      " - 4s - loss: 404.8864 - mae: 404.8863 - val_loss: 443.0200 - val_mae: 443.0201\n",
      "Epoch 52/145\n",
      " - 4s - loss: 407.2226 - mae: 407.2226 - val_loss: 441.3334 - val_mae: 441.3333\n",
      "Epoch 53/145\n",
      " - 4s - loss: 407.4568 - mae: 407.4566 - val_loss: 442.2958 - val_mae: 442.2957\n",
      "Epoch 54/145\n",
      " - 4s - loss: 404.6435 - mae: 404.6435 - val_loss: 441.5178 - val_mae: 441.5178\n",
      "Epoch 55/145\n",
      " - 4s - loss: 402.1626 - mae: 402.1625 - val_loss: 446.4849 - val_mae: 446.4848\n",
      "Epoch 56/145\n",
      " - 4s - loss: 402.1442 - mae: 402.1442 - val_loss: 445.3077 - val_mae: 445.3077\n",
      "Epoch 57/145\n",
      " - 4s - loss: 405.4647 - mae: 405.4647 - val_loss: 445.8877 - val_mae: 445.8877\n",
      "Epoch 58/145\n",
      " - 4s - loss: 400.4323 - mae: 400.4324 - val_loss: 439.7450 - val_mae: 439.7450\n",
      "Epoch 59/145\n",
      " - 4s - loss: 401.3964 - mae: 401.3964 - val_loss: 440.5389 - val_mae: 440.5389\n",
      "Epoch 60/145\n",
      " - 4s - loss: 400.3297 - mae: 400.3298 - val_loss: 442.1299 - val_mae: 442.1299\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      " - 4s - loss: 392.4406 - mae: 392.4406 - val_loss: 434.3241 - val_mae: 434.3241\n",
      "Epoch 62/145\n",
      " - 4s - loss: 392.1024 - mae: 392.1024 - val_loss: 434.6051 - val_mae: 434.6051\n",
      "Epoch 63/145\n",
      " - 4s - loss: 390.1771 - mae: 390.1772 - val_loss: 435.6096 - val_mae: 435.6096\n",
      "Epoch 64/145\n",
      " - 4s - loss: 391.4257 - mae: 391.4257 - val_loss: 434.9739 - val_mae: 434.9740\n",
      "Epoch 65/145\n",
      " - 4s - loss: 391.2841 - mae: 391.2842 - val_loss: 434.5383 - val_mae: 434.5382\n",
      "Epoch 66/145\n",
      " - 4s - loss: 389.2562 - mae: 389.2561 - val_loss: 436.7218 - val_mae: 436.7218\n",
      "Epoch 67/145\n",
      " - 4s - loss: 389.2120 - mae: 389.2119 - val_loss: 435.5564 - val_mae: 435.5564\n",
      "Epoch 68/145\n",
      " - 4s - loss: 390.3517 - mae: 390.3516 - val_loss: 436.0663 - val_mae: 436.0663\n",
      "Epoch 69/145\n",
      " - 3s - loss: 387.7833 - mae: 387.7833 - val_loss: 433.7489 - val_mae: 433.7489\n",
      "Epoch 70/145\n",
      " - 4s - loss: 389.8384 - mae: 389.8384 - val_loss: 437.2867 - val_mae: 437.2867\n",
      "Epoch 71/145\n",
      " - 4s - loss: 389.3827 - mae: 389.3827 - val_loss: 435.1808 - val_mae: 435.1808\n",
      "Epoch 72/145\n",
      " - 4s - loss: 388.0755 - mae: 388.0754 - val_loss: 435.8854 - val_mae: 435.8853\n",
      "Epoch 73/145\n",
      " - 4s - loss: 387.2356 - mae: 387.2356 - val_loss: 436.4340 - val_mae: 436.4341\n",
      "Epoch 74/145\n",
      " - 4s - loss: 389.6418 - mae: 389.6418 - val_loss: 437.9268 - val_mae: 437.9268\n",
      "Epoch 75/145\n",
      " - 4s - loss: 387.9229 - mae: 387.9229 - val_loss: 435.4297 - val_mae: 435.4297\n",
      "Epoch 76/145\n",
      " - 4s - loss: 385.7180 - mae: 385.7180 - val_loss: 439.8698 - val_mae: 439.8698\n",
      "Epoch 77/145\n",
      " - 4s - loss: 394.9813 - mae: 394.9812 - val_loss: 442.1694 - val_mae: 442.1694\n",
      "Epoch 78/145\n",
      " - 4s - loss: 385.3767 - mae: 385.3767 - val_loss: 434.7931 - val_mae: 434.7931\n",
      "Epoch 79/145\n",
      " - 4s - loss: 384.3985 - mae: 384.3985 - val_loss: 432.0980 - val_mae: 432.0981\n",
      "Epoch 80/145\n",
      " - 4s - loss: 383.4459 - mae: 383.4459 - val_loss: 435.5166 - val_mae: 435.5166\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      " - 4s - loss: 380.7076 - mae: 380.7075 - val_loss: 431.8481 - val_mae: 431.8481\n",
      "Epoch 82/145\n",
      " - 4s - loss: 380.5144 - mae: 380.5144 - val_loss: 432.5198 - val_mae: 432.5198\n",
      "Epoch 83/145\n",
      " - 4s - loss: 379.6478 - mae: 379.6477 - val_loss: 432.8288 - val_mae: 432.8288\n",
      "Epoch 84/145\n",
      " - 4s - loss: 378.6030 - mae: 378.6029 - val_loss: 433.1426 - val_mae: 433.1426\n",
      "Epoch 85/145\n",
      " - 4s - loss: 379.0768 - mae: 379.0768 - val_loss: 432.3422 - val_mae: 432.3422\n",
      "Epoch 86/145\n",
      " - 4s - loss: 378.4363 - mae: 378.4364 - val_loss: 432.6722 - val_mae: 432.6722\n",
      "Epoch 87/145\n",
      " - 3s - loss: 378.0440 - mae: 378.0441 - val_loss: 433.4835 - val_mae: 433.4835\n",
      "Epoch 88/145\n",
      " - 4s - loss: 377.9644 - mae: 377.9644 - val_loss: 433.9132 - val_mae: 433.9132\n",
      "Epoch 89/145\n",
      " - 4s - loss: 378.5671 - mae: 378.5671 - val_loss: 434.0568 - val_mae: 434.0568\n",
      "Epoch 90/145\n",
      " - 4s - loss: 378.6478 - mae: 378.6478 - val_loss: 434.4624 - val_mae: 434.4624\n",
      "Epoch 91/145\n",
      " - 4s - loss: 376.5748 - mae: 376.5748 - val_loss: 431.8862 - val_mae: 431.8862\n",
      "Epoch 92/145\n",
      " - 4s - loss: 376.2565 - mae: 376.2565 - val_loss: 432.0526 - val_mae: 432.0526\n",
      "Epoch 93/145\n",
      " - 4s - loss: 376.0138 - mae: 376.0137 - val_loss: 431.8773 - val_mae: 431.8773\n",
      "Epoch 94/145\n",
      " - 4s - loss: 376.4536 - mae: 376.4536 - val_loss: 434.4332 - val_mae: 434.4332\n",
      "Epoch 95/145\n",
      " - 4s - loss: 375.8539 - mae: 375.8539 - val_loss: 438.7987 - val_mae: 438.7988\n",
      "Epoch 96/145\n",
      " - 4s - loss: 377.3560 - mae: 377.3560 - val_loss: 431.9677 - val_mae: 431.9678\n",
      "Epoch 97/145\n",
      " - 4s - loss: 378.1016 - mae: 378.1016 - val_loss: 436.2429 - val_mae: 436.2430\n",
      "Epoch 98/145\n",
      " - 4s - loss: 375.3447 - mae: 375.3448 - val_loss: 434.4953 - val_mae: 434.4953\n",
      "Epoch 99/145\n",
      " - 4s - loss: 375.3127 - mae: 375.3127 - val_loss: 433.7424 - val_mae: 433.7424\n",
      "Epoch 100/145\n",
      " - 4s - loss: 375.2998 - mae: 375.2999 - val_loss: 436.3827 - val_mae: 436.3826\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      " - 4s - loss: 373.6059 - mae: 373.6059 - val_loss: 431.3108 - val_mae: 431.3108\n",
      "Epoch 102/145\n",
      " - 4s - loss: 371.8467 - mae: 371.8466 - val_loss: 430.9555 - val_mae: 430.9554\n",
      "Epoch 103/145\n",
      " - 4s - loss: 371.2600 - mae: 371.2600 - val_loss: 430.4287 - val_mae: 430.4287\n",
      "Epoch 104/145\n",
      " - 4s - loss: 371.3240 - mae: 371.3240 - val_loss: 431.5151 - val_mae: 431.5151\n",
      "Epoch 105/145\n",
      " - 4s - loss: 371.1046 - mae: 371.1047 - val_loss: 432.8464 - val_mae: 432.8464\n",
      "Epoch 106/145\n",
      " - 4s - loss: 371.4645 - mae: 371.4645 - val_loss: 432.8827 - val_mae: 432.8827\n",
      "Epoch 107/145\n",
      " - 4s - loss: 370.7962 - mae: 370.7962 - val_loss: 431.4162 - val_mae: 431.4162\n",
      "Epoch 108/145\n",
      " - 3s - loss: 370.3491 - mae: 370.3492 - val_loss: 431.3398 - val_mae: 431.3397\n",
      "Epoch 109/145\n",
      " - 4s - loss: 370.3286 - mae: 370.3286 - val_loss: 430.5262 - val_mae: 430.5262\n",
      "Epoch 110/145\n",
      " - 4s - loss: 370.9270 - mae: 370.9270 - val_loss: 431.8502 - val_mae: 431.8503\n",
      "Epoch 111/145\n",
      " - 4s - loss: 370.3020 - mae: 370.3021 - val_loss: 432.2802 - val_mae: 432.2802\n",
      "Epoch 112/145\n",
      " - 4s - loss: 370.4048 - mae: 370.4047 - val_loss: 432.9337 - val_mae: 432.9337\n",
      "Epoch 113/145\n",
      " - 4s - loss: 370.0870 - mae: 370.0871 - val_loss: 432.3828 - val_mae: 432.3828\n",
      "Epoch 114/145\n",
      " - 4s - loss: 368.8814 - mae: 368.8814 - val_loss: 431.0302 - val_mae: 431.0302\n",
      "Epoch 115/145\n",
      " - 4s - loss: 369.3857 - mae: 369.3858 - val_loss: 430.5270 - val_mae: 430.5270\n",
      "Epoch 116/145\n",
      " - 4s - loss: 368.9753 - mae: 368.9754 - val_loss: 430.9103 - val_mae: 430.9104\n",
      "Epoch 117/145\n",
      " - 4s - loss: 369.0232 - mae: 369.0232 - val_loss: 430.4472 - val_mae: 430.4472\n",
      "Epoch 118/145\n",
      " - 4s - loss: 368.7128 - mae: 368.7130 - val_loss: 431.3338 - val_mae: 431.3338\n",
      "Epoch 119/145\n",
      " - 4s - loss: 368.7482 - mae: 368.7482 - val_loss: 432.7402 - val_mae: 432.7402\n",
      "Epoch 120/145\n",
      " - 4s - loss: 368.2378 - mae: 368.2377 - val_loss: 432.5827 - val_mae: 432.5827\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      " - 4s - loss: 366.5980 - mae: 366.5980 - val_loss: 432.2822 - val_mae: 432.2822\n",
      "Epoch 122/145\n",
      " - 4s - loss: 366.5441 - mae: 366.5442 - val_loss: 430.7140 - val_mae: 430.7140\n",
      "Epoch 123/145\n",
      " - 4s - loss: 366.1469 - mae: 366.1469 - val_loss: 430.6978 - val_mae: 430.6978\n",
      "Epoch 124/145\n",
      " - 4s - loss: 365.9463 - mae: 365.9462 - val_loss: 430.7202 - val_mae: 430.7202\n",
      "Epoch 125/145\n",
      " - 4s - loss: 366.1017 - mae: 366.1017 - val_loss: 430.6492 - val_mae: 430.6492\n",
      "Epoch 126/145\n",
      " - 4s - loss: 365.8915 - mae: 365.8915 - val_loss: 430.2477 - val_mae: 430.2477\n",
      "Epoch 127/145\n",
      " - 4s - loss: 365.4940 - mae: 365.4940 - val_loss: 430.6321 - val_mae: 430.6322\n",
      "Epoch 128/145\n",
      " - 4s - loss: 365.4932 - mae: 365.4932 - val_loss: 430.6205 - val_mae: 430.6205\n",
      "Epoch 129/145\n",
      " - 4s - loss: 365.2997 - mae: 365.2996 - val_loss: 430.5672 - val_mae: 430.5672\n",
      "Epoch 130/145\n",
      " - 4s - loss: 365.2049 - mae: 365.2048 - val_loss: 430.6712 - val_mae: 430.6712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/145\n",
      " - 5s - loss: 365.1033 - mae: 365.1032 - val_loss: 431.6276 - val_mae: 431.6276\n",
      "Epoch 132/145\n",
      " - 4s - loss: 365.0771 - mae: 365.0771 - val_loss: 430.6619 - val_mae: 430.6619\n",
      "Epoch 133/145\n",
      " - 4s - loss: 364.8043 - mae: 364.8043 - val_loss: 430.2758 - val_mae: 430.2758\n",
      "Epoch 134/145\n",
      " - 4s - loss: 364.6652 - mae: 364.6652 - val_loss: 433.4426 - val_mae: 433.4426\n",
      "Epoch 135/145\n",
      " - 4s - loss: 364.9140 - mae: 364.9140 - val_loss: 430.6142 - val_mae: 430.6141\n",
      "Epoch 136/145\n",
      " - 4s - loss: 364.9540 - mae: 364.9540 - val_loss: 431.2524 - val_mae: 431.2524\n",
      "Epoch 137/145\n",
      " - 4s - loss: 364.4729 - mae: 364.4729 - val_loss: 430.9207 - val_mae: 430.9207\n",
      "Epoch 138/145\n",
      " - 4s - loss: 364.5974 - mae: 364.5974 - val_loss: 433.2503 - val_mae: 433.2503\n",
      "Epoch 139/145\n",
      " - 3s - loss: 364.6623 - mae: 364.6622 - val_loss: 430.3875 - val_mae: 430.3875\n",
      "Epoch 140/145\n",
      " - 4s - loss: 363.8677 - mae: 363.8676 - val_loss: 431.1998 - val_mae: 431.1998\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      " - 4s - loss: 363.0503 - mae: 363.0502 - val_loss: 430.2480 - val_mae: 430.2480\n",
      "Epoch 142/145\n",
      " - 4s - loss: 362.9663 - mae: 362.9663 - val_loss: 429.9865 - val_mae: 429.9865\n",
      "Epoch 143/145\n",
      " - 4s - loss: 362.9350 - mae: 362.9350 - val_loss: 430.6858 - val_mae: 430.6859\n",
      "Epoch 144/145\n",
      " - 4s - loss: 363.1383 - mae: 363.1383 - val_loss: 430.3647 - val_mae: 430.3647\n",
      "Epoch 145/145\n",
      " - 4s - loss: 362.5559 - mae: 362.5559 - val_loss: 431.0850 - val_mae: 431.0850\n",
      "\n",
      "val_mae is:431.0850090152359\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "419.63831483318967"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_splits = 6\n",
    "kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "import keras \n",
    "\n",
    "b_size = 2000\n",
    "max_epochs = 145\n",
    "oof_pred = np.zeros((len(X_pca), ))\n",
    "\n",
    "sub = pd.read_csv('input/used_car_testB_20200421.csv', sep=' ')[['SaleID']].copy()\n",
    "sub['price'] = 0\n",
    "\n",
    "avg_mae = 0\n",
    "for fold, (trn_idx, val_idx) in enumerate(kf.split(X_pca, y)):\n",
    "    print('fold:', fold)\n",
    "    X_train, y_train = X_pca[trn_idx], y[trn_idx]\n",
    "    X_val, y_val = X_pca[val_idx], y[val_idx]\n",
    "    \n",
    "    model = NN_model(X_train.shape[1])\n",
    "    simple_adam = keras.optimizers.Adam(lr = 0.015)\n",
    "    \n",
    "    model.compile(loss='mae', optimizer=simple_adam,metrics=['mae'])\n",
    "    es = EarlyStopping(monitor='val_score', patience=10, verbose=2, mode='min', restore_best_weights=True,)\n",
    "    es.set_model(model)\n",
    "    metric = Metric(model, [es], [(X_train, y_train), (X_val, y_val)])\n",
    "    model.fit(X_train, y_train, batch_size=b_size, epochs=max_epochs, \n",
    "              validation_data = [X_val, y_val],\n",
    "              callbacks=[reduce_lr], shuffle=True, verbose=2)\n",
    "    y_pred3 = model.predict(X_val)\n",
    "    y_pred = np.zeros((len(y_pred3), ))\n",
    "    sub['price'] += model.predict(test).reshape(-1,) / n_splits\n",
    "    for i in range(len(y_pred3)):\n",
    "        y_pred[i] = y_pred3[i]\n",
    "        \n",
    "    oof_pred[val_idx] = y_pred\n",
    "    val_mae = mean_absolute_error(y[val_idx], y_pred)\n",
    "    avg_mae += val_mae / n_splits\n",
    "    print()\n",
    "    print('val_mae is:{}'.format(val_mae))\n",
    "    print()\n",
    "mean_absolute_error(y, oof_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T14:45:07.698212Z",
     "start_time": "2020-04-26T14:45:07.676225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SaleID</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>1269.406982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>1994.119873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>8966.318359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>1237.695557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>1987.327881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200005</td>\n",
       "      <td>1115.205811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200006</td>\n",
       "      <td>444.999847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200007</td>\n",
       "      <td>3564.075439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200008</td>\n",
       "      <td>14262.400391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200009</td>\n",
       "      <td>599.867920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>200010</td>\n",
       "      <td>645.497498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>200011</td>\n",
       "      <td>2753.483398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200012</td>\n",
       "      <td>5592.105469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200013</td>\n",
       "      <td>7206.826660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200014</td>\n",
       "      <td>1073.914551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>200015</td>\n",
       "      <td>249.085510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200016</td>\n",
       "      <td>1753.982666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>200017</td>\n",
       "      <td>8294.636719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>200018</td>\n",
       "      <td>6684.482422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>200019</td>\n",
       "      <td>1092.759155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SaleID         price\n",
       "0   200000   1269.406982\n",
       "1   200001   1994.119873\n",
       "2   200002   8966.318359\n",
       "3   200003   1237.695557\n",
       "4   200004   1987.327881\n",
       "5   200005   1115.205811\n",
       "6   200006    444.999847\n",
       "7   200007   3564.075439\n",
       "8   200008  14262.400391\n",
       "9   200009    599.867920\n",
       "10  200010    645.497498\n",
       "11  200011   2753.483398\n",
       "12  200012   5592.105469\n",
       "13  200013   7206.826660\n",
       "14  200014   1073.914551\n",
       "15  200015    249.085510\n",
       "16  200016   1753.982666\n",
       "17  200017   8294.636719\n",
       "18  200018   6684.482422\n",
       "19  200019   1092.759155"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T14:45:07.946645Z",
     "start_time": "2020-04-26T14:45:07.703212Z"
    }
   },
   "outputs": [],
   "source": [
    "sub.to_csv('nn_sub_{}_{}.csv'.format('mae', sub['price'].mean()), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
