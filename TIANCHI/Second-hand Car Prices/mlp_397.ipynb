{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tianchi.aliyun.com/notebook-ai/detail?postId=103212"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:10:40.351185Z",
     "start_time": "2020-06-22T10:10:38.521253Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import jn\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', None)\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "from sklearn.decomposition import PCA, FastICA, FactorAnalysis, SparsePCA\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import scipy.signal as signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:10:40.366175Z",
     "start_time": "2020-06-22T10:10:40.352144Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" \n",
    "    iterate through all the columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:10:40.391779Z",
     "start_time": "2020-06-22T10:10:40.367367Z"
    }
   },
   "outputs": [],
   "source": [
    "# 处理异常值\n",
    "def smooth_cols(group,out_value,kind):\n",
    "    cols = ['power']\n",
    "    if kind == 'g':\n",
    "        for col in cols:\n",
    "            yes_no = (group[col]<out_value).astype('int')\n",
    "            new = yes_no * group[col]\n",
    "            group[col] = new.replace(0,group[col].quantile(q=0.995))\n",
    "        return group\n",
    "    if kind == 'l':\n",
    "        for col in cols:\n",
    "            yes_no = (group[col]>out_value).astype('int')\n",
    "            new = yes_no * group[col]\n",
    "            group[col] = new.replace(0,group[col].quantile(q=0.07))\n",
    "        return group        \n",
    "\n",
    "def date_proc(x):\n",
    "    # '20200426' '20200026'\n",
    "    m = int(x[4:6])\n",
    "    if m == 0:\n",
    "        m = 1\n",
    "    return x[:4] + '-' + str(m) + '-' + x[6:]\n",
    "\n",
    "# 定义日期提取函数\n",
    "def date_tran(df,fea_col):\n",
    "    for f in tqdm(fea_col):\n",
    "        df[f] = pd.to_datetime(df[f].astype('str').apply(date_proc))\n",
    "        df[f + '_year'] = df[f].dt.year # 年份\n",
    "        df[f + '_month'] = df[f].dt.month # 月份\n",
    "        df[f + '_day'] = df[f].dt.day # 多少号\n",
    "        df[f + '_dayofweek'] = df[f].dt.dayofweek # 周几\n",
    "    return df\n",
    "\n",
    "# 分桶操作\n",
    "def cut_group(df, cols, num_bins=50):\n",
    "    for col in cols:\n",
    "        all_range = int((df[col].max() + 1) - (df[col].min() - 1))\n",
    "        bin = [i * all_range / num_bins for i in range(all_range)]\n",
    "        df[col + '_bin'] = pd.cut(df[col], bin, labels=False)\n",
    "    return df\n",
    "\n",
    "# count编码\n",
    "def count_coding(df, fea_col):\n",
    "    for f in fea_col:\n",
    "        df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "    return df\n",
    "\n",
    "# 定义交叉特征统计\n",
    "def cross_cat_num(df, num_col, cat_col):\n",
    "    for f1 in tqdm(cat_col):\n",
    "        g = df.groupby(f1, as_index=False)\n",
    "        for f2 in tqdm(num_col):\n",
    "            feat = g[f2].agg({\n",
    "                '{}_{}_max'.format(f1, f2): 'max', # 最大值\n",
    "                '{}_{}_min'.format(f1, f2): 'min', # 最小值\n",
    "                '{}_{}_median'.format(f1, f2): 'median', # 中位数\n",
    "            })\n",
    "            df = df.merge(feat, on=f1, how='left')\n",
    "    return df\n",
    "\n",
    "# 类别特征的二阶交叉\n",
    "from scipy.stats import entropy\n",
    "def cross_qua_cat_num(df):\n",
    "    for f_pair in tqdm([\n",
    "        ['model', 'brand'], ['model', 'regionCode'], ['brand', 'regionCode']\n",
    "    ]):\n",
    "        # 共现次数\n",
    "        df['_'.join(f_pair) + '_count'] = df.groupby(f_pair)['SaleID'].transform('count')\n",
    "        # n unique、熵\n",
    "        df = df.merge(df.groupby(f_pair[0], as_index=False)[f_pair[1]].agg({\n",
    "            '{}_{}_nunique'.format(f_pair[0], f_pair[1]): 'nunique',\n",
    "            '{}_{}_ent'.format(f_pair[0], f_pair[1]): lambda x: entropy(x.value_counts() / x.shape[0])\n",
    "        }), on=f_pair[0], how='left')\n",
    "        df = df.merge(df.groupby(f_pair[1], as_index=False)[f_pair[0]].agg({\n",
    "            '{}_{}_nunique'.format(f_pair[1], f_pair[0]): 'nunique',\n",
    "            '{}_{}_ent'.format(f_pair[1], f_pair[0]): lambda x: entropy(x.value_counts() / x.shape[0])\n",
    "        }), on=f_pair[1], how='left')\n",
    "        # 比例偏好\n",
    "        df['{}_in_{}_prop'.format(f_pair[0], f_pair[1])] = df['_'.join(f_pair) + '_count'] / df[f_pair[1] + '_count']\n",
    "        df['{}_in_{}_prop'.format(f_pair[1], f_pair[0])] = df['_'.join(f_pair) + '_count'] / df[f_pair[0] + '_count']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:10:41.524864Z",
     "start_time": "2020-06-22T10:10:40.392730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 37200128.00 MB\n",
      "Memory usage after optimization is: 10200232.00 MB\n",
      "Decreased by 72.6%\n",
      "Memory usage of dataframe is 12000128.00 MB\n",
      "Memory usage after optimization is: 3200232.00 MB\n",
      "Decreased by 73.3%\n",
      "Train data shape: (150000, 31)\n",
      "TestA_data shape: (50000, 30)\n"
     ]
    }
   ],
   "source": [
    "Train_data = reduce_mem_usage(pd.read_csv('input/used_car_train_20200313.csv',\n",
    "                                          sep=' '))\n",
    "TestA_data = reduce_mem_usage(pd.read_csv('input/used_car_testB_20200421.csv',\n",
    "                                          sep=' '))\n",
    "\n",
    "print('Train data shape: {}'.format(Train_data.shape))\n",
    "print('TestA_data shape: {}'.format(TestA_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:10:41.560784Z",
     "start_time": "2020-06-22T10:10:41.525827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SaleID</th>\n",
       "      <th>name</th>\n",
       "      <th>regDate</th>\n",
       "      <th>model</th>\n",
       "      <th>brand</th>\n",
       "      <th>bodyType</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>power</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>notRepairedDamage</th>\n",
       "      <th>regionCode</th>\n",
       "      <th>seller</th>\n",
       "      <th>offerType</th>\n",
       "      <th>creatDate</th>\n",
       "      <th>price</th>\n",
       "      <th>v_0</th>\n",
       "      <th>v_1</th>\n",
       "      <th>v_2</th>\n",
       "      <th>v_3</th>\n",
       "      <th>v_4</th>\n",
       "      <th>v_5</th>\n",
       "      <th>v_6</th>\n",
       "      <th>v_7</th>\n",
       "      <th>v_8</th>\n",
       "      <th>v_9</th>\n",
       "      <th>v_10</th>\n",
       "      <th>v_11</th>\n",
       "      <th>v_12</th>\n",
       "      <th>v_13</th>\n",
       "      <th>v_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>736</td>\n",
       "      <td>20040402</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1046</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160404</td>\n",
       "      <td>1850</td>\n",
       "      <td>43.34375</td>\n",
       "      <td>3.966797</td>\n",
       "      <td>0.050262</td>\n",
       "      <td>2.160156</td>\n",
       "      <td>1.143555</td>\n",
       "      <td>0.235718</td>\n",
       "      <td>0.101990</td>\n",
       "      <td>0.129517</td>\n",
       "      <td>0.022812</td>\n",
       "      <td>0.097473</td>\n",
       "      <td>-2.880859</td>\n",
       "      <td>2.804688</td>\n",
       "      <td>-2.419922</td>\n",
       "      <td>0.795410</td>\n",
       "      <td>0.914551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2262</td>\n",
       "      <td>20030301</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-</td>\n",
       "      <td>4366</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160309</td>\n",
       "      <td>3600</td>\n",
       "      <td>45.31250</td>\n",
       "      <td>5.234375</td>\n",
       "      <td>0.137939</td>\n",
       "      <td>1.380859</td>\n",
       "      <td>-1.421875</td>\n",
       "      <td>0.264893</td>\n",
       "      <td>0.121033</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0.026596</td>\n",
       "      <td>0.020584</td>\n",
       "      <td>-4.902344</td>\n",
       "      <td>2.095703</td>\n",
       "      <td>-1.030273</td>\n",
       "      <td>-1.722656</td>\n",
       "      <td>0.245483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>14874</td>\n",
       "      <td>20040403</td>\n",
       "      <td>115.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2806</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160402</td>\n",
       "      <td>6222</td>\n",
       "      <td>45.96875</td>\n",
       "      <td>4.824219</td>\n",
       "      <td>1.319336</td>\n",
       "      <td>-0.998535</td>\n",
       "      <td>-0.997070</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.114929</td>\n",
       "      <td>0.165161</td>\n",
       "      <td>0.062164</td>\n",
       "      <td>0.027069</td>\n",
       "      <td>-4.847656</td>\n",
       "      <td>1.803711</td>\n",
       "      <td>1.565430</td>\n",
       "      <td>-0.832520</td>\n",
       "      <td>-0.229980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>71865</td>\n",
       "      <td>19960908</td>\n",
       "      <td>109.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>193</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>434</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160312</td>\n",
       "      <td>2400</td>\n",
       "      <td>45.68750</td>\n",
       "      <td>4.492188</td>\n",
       "      <td>-0.050629</td>\n",
       "      <td>0.883789</td>\n",
       "      <td>-2.228516</td>\n",
       "      <td>0.274414</td>\n",
       "      <td>0.110291</td>\n",
       "      <td>0.121948</td>\n",
       "      <td>0.033386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.507812</td>\n",
       "      <td>1.286133</td>\n",
       "      <td>-0.501953</td>\n",
       "      <td>-2.437500</td>\n",
       "      <td>-0.478760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>111080</td>\n",
       "      <td>20120103</td>\n",
       "      <td>110.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6977</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160313</td>\n",
       "      <td>5200</td>\n",
       "      <td>44.37500</td>\n",
       "      <td>2.031250</td>\n",
       "      <td>0.572266</td>\n",
       "      <td>-1.571289</td>\n",
       "      <td>2.246094</td>\n",
       "      <td>0.228027</td>\n",
       "      <td>0.073181</td>\n",
       "      <td>0.091858</td>\n",
       "      <td>0.078796</td>\n",
       "      <td>0.121521</td>\n",
       "      <td>-1.896484</td>\n",
       "      <td>0.910645</td>\n",
       "      <td>0.931152</td>\n",
       "      <td>2.833984</td>\n",
       "      <td>1.923828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>137642</td>\n",
       "      <td>20090602</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3690</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160319</td>\n",
       "      <td>8000</td>\n",
       "      <td>46.31250</td>\n",
       "      <td>-3.228516</td>\n",
       "      <td>0.156616</td>\n",
       "      <td>-1.727539</td>\n",
       "      <td>-0.345703</td>\n",
       "      <td>0.260254</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.119812</td>\n",
       "      <td>0.090942</td>\n",
       "      <td>0.048767</td>\n",
       "      <td>1.885742</td>\n",
       "      <td>-2.722656</td>\n",
       "      <td>2.457031</td>\n",
       "      <td>-0.286865</td>\n",
       "      <td>0.206543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2402</td>\n",
       "      <td>19990411</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3073</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160317</td>\n",
       "      <td>3500</td>\n",
       "      <td>46.09375</td>\n",
       "      <td>4.925781</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>1.644531</td>\n",
       "      <td>-1.270508</td>\n",
       "      <td>0.268066</td>\n",
       "      <td>0.117676</td>\n",
       "      <td>0.142334</td>\n",
       "      <td>0.025452</td>\n",
       "      <td>0.028168</td>\n",
       "      <td>-4.902344</td>\n",
       "      <td>1.610352</td>\n",
       "      <td>-0.834473</td>\n",
       "      <td>-1.996094</td>\n",
       "      <td>-0.103210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>165346</td>\n",
       "      <td>19990706</td>\n",
       "      <td>26.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160326</td>\n",
       "      <td>1000</td>\n",
       "      <td>42.25000</td>\n",
       "      <td>-3.167969</td>\n",
       "      <td>-0.676758</td>\n",
       "      <td>1.942383</td>\n",
       "      <td>0.524414</td>\n",
       "      <td>0.239502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122925</td>\n",
       "      <td>0.039825</td>\n",
       "      <td>0.082397</td>\n",
       "      <td>3.693359</td>\n",
       "      <td>-0.244995</td>\n",
       "      <td>-2.193359</td>\n",
       "      <td>0.236694</td>\n",
       "      <td>0.195557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2974</td>\n",
       "      <td>20030205</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>179</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4679</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160326</td>\n",
       "      <td>2850</td>\n",
       "      <td>46.09375</td>\n",
       "      <td>4.894531</td>\n",
       "      <td>0.475342</td>\n",
       "      <td>0.556641</td>\n",
       "      <td>-1.262695</td>\n",
       "      <td>0.263916</td>\n",
       "      <td>0.116577</td>\n",
       "      <td>0.144287</td>\n",
       "      <td>0.039856</td>\n",
       "      <td>0.024384</td>\n",
       "      <td>-4.925781</td>\n",
       "      <td>1.587891</td>\n",
       "      <td>0.075317</td>\n",
       "      <td>-1.550781</td>\n",
       "      <td>0.069458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>82021</td>\n",
       "      <td>19980101</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160402</td>\n",
       "      <td>650</td>\n",
       "      <td>43.06250</td>\n",
       "      <td>1.666016</td>\n",
       "      <td>-2.201172</td>\n",
       "      <td>3.097656</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.262451</td>\n",
       "      <td>0.068237</td>\n",
       "      <td>0.012177</td>\n",
       "      <td>0.010292</td>\n",
       "      <td>0.098755</td>\n",
       "      <td>-1.089844</td>\n",
       "      <td>0.600586</td>\n",
       "      <td>-4.187500</td>\n",
       "      <td>0.198242</td>\n",
       "      <td>-1.025391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SaleID    name   regDate  model  brand  bodyType  fuelType  gearbox  power  \\\n",
       "0       0     736  20040402   30.0      6       1.0       0.0      0.0     60   \n",
       "1       1    2262  20030301   40.0      1       2.0       0.0      0.0      0   \n",
       "2       2   14874  20040403  115.0     15       1.0       0.0      0.0    163   \n",
       "3       3   71865  19960908  109.0     10       0.0       0.0      1.0    193   \n",
       "4       4  111080  20120103  110.0      5       1.0       0.0      0.0     68   \n",
       "5       5  137642  20090602   24.0     10       0.0       1.0      0.0    109   \n",
       "6       6    2402  19990411   13.0      4       0.0       0.0      1.0    150   \n",
       "7       7  165346  19990706   26.0     14       1.0       0.0      0.0    101   \n",
       "8       8    2974  20030205   19.0      1       2.0       1.0      1.0    179   \n",
       "9       9   82021  19980101    7.0      7       5.0       0.0      0.0     88   \n",
       "\n",
       "   kilometer notRepairedDamage  regionCode  seller  offerType  creatDate  \\\n",
       "0       12.5               0.0        1046       0          0   20160404   \n",
       "1       15.0                 -        4366       0          0   20160309   \n",
       "2       12.5               0.0        2806       0          0   20160402   \n",
       "3       15.0               0.0         434       0          0   20160312   \n",
       "4        5.0               0.0        6977       0          0   20160313   \n",
       "5       10.0               0.0        3690       0          0   20160319   \n",
       "6       15.0               0.0        3073       0          0   20160317   \n",
       "7       15.0               0.0        4000       0          0   20160326   \n",
       "8       15.0               0.0        4679       0          0   20160326   \n",
       "9       15.0               0.0         302       0          0   20160402   \n",
       "\n",
       "   price       v_0       v_1       v_2       v_3       v_4       v_5  \\\n",
       "0   1850  43.34375  3.966797  0.050262  2.160156  1.143555  0.235718   \n",
       "1   3600  45.31250  5.234375  0.137939  1.380859 -1.421875  0.264893   \n",
       "2   6222  45.96875  4.824219  1.319336 -0.998535 -0.997070  0.251465   \n",
       "3   2400  45.68750  4.492188 -0.050629  0.883789 -2.228516  0.274414   \n",
       "4   5200  44.37500  2.031250  0.572266 -1.571289  2.246094  0.228027   \n",
       "5   8000  46.31250 -3.228516  0.156616 -1.727539 -0.345703  0.260254   \n",
       "6   3500  46.09375  4.925781  0.113281  1.644531 -1.270508  0.268066   \n",
       "7   1000  42.25000 -3.167969 -0.676758  1.942383  0.524414  0.239502   \n",
       "8   2850  46.09375  4.894531  0.475342  0.556641 -1.262695  0.263916   \n",
       "9    650  43.06250  1.666016 -2.201172  3.097656  0.843750  0.262451   \n",
       "\n",
       "        v_6       v_7       v_8       v_9      v_10      v_11      v_12  \\\n",
       "0  0.101990  0.129517  0.022812  0.097473 -2.880859  2.804688 -2.419922   \n",
       "1  0.121033  0.135742  0.026596  0.020584 -4.902344  2.095703 -1.030273   \n",
       "2  0.114929  0.165161  0.062164  0.027069 -4.847656  1.803711  1.565430   \n",
       "3  0.110291  0.121948  0.033386  0.000000 -4.507812  1.286133 -0.501953   \n",
       "4  0.073181  0.091858  0.078796  0.121521 -1.896484  0.910645  0.931152   \n",
       "5  0.000518  0.119812  0.090942  0.048767  1.885742 -2.722656  2.457031   \n",
       "6  0.117676  0.142334  0.025452  0.028168 -4.902344  1.610352 -0.834473   \n",
       "7  0.000000  0.122925  0.039825  0.082397  3.693359 -0.244995 -2.193359   \n",
       "8  0.116577  0.144287  0.039856  0.024384 -4.925781  1.587891  0.075317   \n",
       "9  0.068237  0.012177  0.010292  0.098755 -1.089844  0.600586 -4.187500   \n",
       "\n",
       "       v_13      v_14  \n",
       "0  0.795410  0.914551  \n",
       "1 -1.722656  0.245483  \n",
       "2 -0.832520 -0.229980  \n",
       "3 -2.437500 -0.478760  \n",
       "4  2.833984  1.923828  \n",
       "5 -0.286865  0.206543  \n",
       "6 -1.996094 -0.103210  \n",
       "7  0.236694  0.195557  \n",
       "8 -1.550781  0.069458  \n",
       "9  0.198242 -1.025391  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:10:41.590709Z",
     "start_time": "2020-06-22T10:10:41.561748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SaleID</th>\n",
       "      <th>name</th>\n",
       "      <th>regDate</th>\n",
       "      <th>model</th>\n",
       "      <th>brand</th>\n",
       "      <th>bodyType</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>power</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>notRepairedDamage</th>\n",
       "      <th>regionCode</th>\n",
       "      <th>seller</th>\n",
       "      <th>offerType</th>\n",
       "      <th>creatDate</th>\n",
       "      <th>v_0</th>\n",
       "      <th>v_1</th>\n",
       "      <th>v_2</th>\n",
       "      <th>v_3</th>\n",
       "      <th>v_4</th>\n",
       "      <th>v_5</th>\n",
       "      <th>v_6</th>\n",
       "      <th>v_7</th>\n",
       "      <th>v_8</th>\n",
       "      <th>v_9</th>\n",
       "      <th>v_10</th>\n",
       "      <th>v_11</th>\n",
       "      <th>v_12</th>\n",
       "      <th>v_13</th>\n",
       "      <th>v_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>133777</td>\n",
       "      <td>20000501</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160308</td>\n",
       "      <td>42.15625</td>\n",
       "      <td>-3.095703</td>\n",
       "      <td>-0.721191</td>\n",
       "      <td>1.466797</td>\n",
       "      <td>1.009766</td>\n",
       "      <td>0.236572</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.105347</td>\n",
       "      <td>0.046234</td>\n",
       "      <td>0.094543</td>\n",
       "      <td>3.619141</td>\n",
       "      <td>-0.280518</td>\n",
       "      <td>-2.019531</td>\n",
       "      <td>0.979004</td>\n",
       "      <td>0.803223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>61206</td>\n",
       "      <td>19950211</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1505</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160310</td>\n",
       "      <td>43.90625</td>\n",
       "      <td>-3.244141</td>\n",
       "      <td>-0.766602</td>\n",
       "      <td>1.276367</td>\n",
       "      <td>-1.065430</td>\n",
       "      <td>0.261475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>0.046783</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>2.998047</td>\n",
       "      <td>-1.406250</td>\n",
       "      <td>-1.020508</td>\n",
       "      <td>-1.349609</td>\n",
       "      <td>-0.200562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>67829</td>\n",
       "      <td>20090606</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-</td>\n",
       "      <td>1776</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160309</td>\n",
       "      <td>45.37500</td>\n",
       "      <td>3.373047</td>\n",
       "      <td>-0.965332</td>\n",
       "      <td>-2.447266</td>\n",
       "      <td>0.624512</td>\n",
       "      <td>0.261719</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079651</td>\n",
       "      <td>0.073608</td>\n",
       "      <td>-3.951172</td>\n",
       "      <td>-0.433350</td>\n",
       "      <td>0.918945</td>\n",
       "      <td>1.634766</td>\n",
       "      <td>1.027344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>8892</td>\n",
       "      <td>20020601</td>\n",
       "      <td>22.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160314</td>\n",
       "      <td>42.78125</td>\n",
       "      <td>4.035156</td>\n",
       "      <td>-0.217407</td>\n",
       "      <td>1.708984</td>\n",
       "      <td>1.119141</td>\n",
       "      <td>0.236084</td>\n",
       "      <td>0.101807</td>\n",
       "      <td>0.098938</td>\n",
       "      <td>0.026825</td>\n",
       "      <td>0.096619</td>\n",
       "      <td>-2.847656</td>\n",
       "      <td>2.800781</td>\n",
       "      <td>-2.525391</td>\n",
       "      <td>1.077148</td>\n",
       "      <td>0.461670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>76998</td>\n",
       "      <td>20030301</td>\n",
       "      <td>46.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>738</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160306</td>\n",
       "      <td>43.65625</td>\n",
       "      <td>-3.134766</td>\n",
       "      <td>-1.133789</td>\n",
       "      <td>0.470215</td>\n",
       "      <td>0.134033</td>\n",
       "      <td>0.257080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066711</td>\n",
       "      <td>0.057770</td>\n",
       "      <td>0.068848</td>\n",
       "      <td>2.839844</td>\n",
       "      <td>-1.660156</td>\n",
       "      <td>-0.924316</td>\n",
       "      <td>0.199463</td>\n",
       "      <td>0.450928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200005</td>\n",
       "      <td>142813</td>\n",
       "      <td>19990006</td>\n",
       "      <td>37.0</td>\n",
       "      <td>18</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3393</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160404</td>\n",
       "      <td>43.65625</td>\n",
       "      <td>-3.130859</td>\n",
       "      <td>-2.519531</td>\n",
       "      <td>1.180664</td>\n",
       "      <td>1.295898</td>\n",
       "      <td>0.261475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.110718</td>\n",
       "      <td>2.646484</td>\n",
       "      <td>-2.441406</td>\n",
       "      <td>-2.255859</td>\n",
       "      <td>0.712402</td>\n",
       "      <td>-3.314453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200006</td>\n",
       "      <td>135370</td>\n",
       "      <td>19980503</td>\n",
       "      <td>36.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2244</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160305</td>\n",
       "      <td>41.90625</td>\n",
       "      <td>-3.117188</td>\n",
       "      <td>-2.884766</td>\n",
       "      <td>3.189453</td>\n",
       "      <td>0.860352</td>\n",
       "      <td>0.261963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018356</td>\n",
       "      <td>0.098389</td>\n",
       "      <td>3.501953</td>\n",
       "      <td>-1.248047</td>\n",
       "      <td>-4.574219</td>\n",
       "      <td>0.569824</td>\n",
       "      <td>1.083984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200007</td>\n",
       "      <td>7138</td>\n",
       "      <td>20040201</td>\n",
       "      <td>88.0</td>\n",
       "      <td>14</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160325</td>\n",
       "      <td>44.87500</td>\n",
       "      <td>4.542969</td>\n",
       "      <td>-0.659668</td>\n",
       "      <td>-0.196411</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.109070</td>\n",
       "      <td>0.027527</td>\n",
       "      <td>0.051117</td>\n",
       "      <td>0.108276</td>\n",
       "      <td>-4.433594</td>\n",
       "      <td>1.144531</td>\n",
       "      <td>-0.750977</td>\n",
       "      <td>1.530273</td>\n",
       "      <td>-0.903320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200008</td>\n",
       "      <td>7977</td>\n",
       "      <td>20110209</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1184</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160328</td>\n",
       "      <td>47.09375</td>\n",
       "      <td>4.433594</td>\n",
       "      <td>0.462402</td>\n",
       "      <td>-1.986328</td>\n",
       "      <td>0.339111</td>\n",
       "      <td>0.259277</td>\n",
       "      <td>0.109070</td>\n",
       "      <td>0.081909</td>\n",
       "      <td>0.076965</td>\n",
       "      <td>0.066772</td>\n",
       "      <td>-5.183594</td>\n",
       "      <td>0.286865</td>\n",
       "      <td>2.080078</td>\n",
       "      <td>0.625977</td>\n",
       "      <td>0.684570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200009</td>\n",
       "      <td>104001</td>\n",
       "      <td>19991012</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4874</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160317</td>\n",
       "      <td>41.50000</td>\n",
       "      <td>-3.134766</td>\n",
       "      <td>-1.010742</td>\n",
       "      <td>2.517578</td>\n",
       "      <td>0.414551</td>\n",
       "      <td>0.240112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110840</td>\n",
       "      <td>0.030624</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>3.958984</td>\n",
       "      <td>0.066406</td>\n",
       "      <td>-3.095703</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>0.367188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SaleID    name   regDate  model  brand  bodyType  fuelType  gearbox  power  \\\n",
       "0  200000  133777  20000501   67.0      0       1.0       0.0      0.0    101   \n",
       "1  200001   61206  19950211   19.0      6       2.0       0.0      0.0     73   \n",
       "2  200002   67829  20090606    5.0      5       4.0       0.0      0.0    120   \n",
       "3  200003    8892  20020601   22.0      9       1.0       0.0      0.0     58   \n",
       "4  200004   76998  20030301   46.0      6       0.0       NaN      0.0    116   \n",
       "5  200005  142813  19990006   37.0     18       6.0       0.0      0.0    125   \n",
       "6  200006  135370  19980503   36.0      6       4.0       0.0      0.0     75   \n",
       "7  200007    7138  20040201   88.0     14       3.0       0.0      1.0    125   \n",
       "8  200008    7977  20110209   77.0      0       3.0       1.0      1.0    140   \n",
       "9  200009  104001  19991012   30.0      6       1.0       0.0      0.0     74   \n",
       "\n",
       "   kilometer notRepairedDamage  regionCode  seller  offerType  creatDate  \\\n",
       "0       15.0               0.0        5019       0          0   20160308   \n",
       "1        6.0               0.0        1505       0          0   20160310   \n",
       "2        5.0                 -        1776       0          0   20160309   \n",
       "3       15.0               0.0          26       0          0   20160314   \n",
       "4       15.0               0.0         738       0          0   20160306   \n",
       "5       15.0               0.0        3393       0          0   20160404   \n",
       "6       15.0               0.0        2244       0          0   20160305   \n",
       "7       15.0               0.0         155       0          0   20160325   \n",
       "8        7.0               0.0        1184       0          0   20160328   \n",
       "9       15.0               0.0        4874       0          0   20160317   \n",
       "\n",
       "        v_0       v_1       v_2       v_3       v_4       v_5       v_6  \\\n",
       "0  42.15625 -3.095703 -0.721191  1.466797  1.009766  0.236572  0.000241   \n",
       "1  43.90625 -3.244141 -0.766602  1.276367 -1.065430  0.261475  0.000000   \n",
       "2  45.37500  3.373047 -0.965332 -2.447266  0.624512  0.261719  0.090820   \n",
       "3  42.78125  4.035156 -0.217407  1.708984  1.119141  0.236084  0.101807   \n",
       "4  43.65625 -3.134766 -1.133789  0.470215  0.134033  0.257080  0.000000   \n",
       "5  43.65625 -3.130859 -2.519531  1.180664  1.295898  0.261475  0.000000   \n",
       "6  41.90625 -3.117188 -2.884766  3.189453  0.860352  0.261963  0.000000   \n",
       "7  44.87500  4.542969 -0.659668 -0.196411  1.531250  0.251465  0.109070   \n",
       "8  47.09375  4.433594  0.462402 -1.986328  0.339111  0.259277  0.109070   \n",
       "9  41.50000 -3.134766 -1.010742  2.517578  0.414551  0.240112  0.000000   \n",
       "\n",
       "        v_7       v_8       v_9      v_10      v_11      v_12      v_13  \\\n",
       "0  0.105347  0.046234  0.094543  3.619141 -0.280518 -2.019531  0.979004   \n",
       "1  0.120300  0.046783  0.035400  2.998047 -1.406250 -1.020508 -1.349609   \n",
       "2  0.000000  0.079651  0.073608 -3.951172 -0.433350  0.918945  1.634766   \n",
       "3  0.098938  0.026825  0.096619 -2.847656  2.800781 -2.525391  1.077148   \n",
       "4  0.066711  0.057770  0.068848  2.839844 -1.660156 -0.924316  0.199463   \n",
       "5  0.000000  0.046875  0.110718  2.646484 -2.441406 -2.255859  0.712402   \n",
       "6  0.000000  0.018356  0.098389  3.501953 -1.248047 -4.574219  0.569824   \n",
       "7  0.027527  0.051117  0.108276 -4.433594  1.144531 -0.750977  1.530273   \n",
       "8  0.081909  0.076965  0.066772 -5.183594  0.286865  2.080078  0.625977   \n",
       "9  0.110840  0.030624  0.080078  3.958984  0.066406 -3.095703  0.205444   \n",
       "\n",
       "       v_14  \n",
       "0  0.803223  \n",
       "1 -0.200562  \n",
       "2  1.027344  \n",
       "3  0.461670  \n",
       "4  0.450928  \n",
       "5 -3.314453  \n",
       "6  1.083984  \n",
       "7 -0.903320  \n",
       "8  0.684570  \n",
       "9  0.367188  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TestA_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:10:41.680477Z",
     "start_time": "2020-06-22T10:10:41.592666Z"
    }
   },
   "outputs": [],
   "source": [
    "# 合并数据集\n",
    "concat_data = pd.concat([Train_data, TestA_data],\n",
    "                        ignore_index=True) # 重新生成索引\n",
    "\n",
    "# 'notRepairedDamage'中的'-'用0替换\n",
    "concat_data['notRepairedDamage'] = concat_data['notRepairedDamage'].replace('-', 0).astype('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:10:42.226573Z",
     "start_time": "2020-06-22T10:10:41.682424Z"
    }
   },
   "outputs": [],
   "source": [
    "# 每列中的缺失值用每列的众数填充\n",
    "concat_data = concat_data.fillna(concat_data.mode().iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:10:42.232555Z",
     "start_time": "2020-06-22T10:10:42.227566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 31)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:10:42.263511Z",
     "start_time": "2020-06-22T10:10:42.234550Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SaleID</th>\n",
       "      <th>name</th>\n",
       "      <th>regDate</th>\n",
       "      <th>model</th>\n",
       "      <th>brand</th>\n",
       "      <th>bodyType</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>power</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>notRepairedDamage</th>\n",
       "      <th>regionCode</th>\n",
       "      <th>seller</th>\n",
       "      <th>offerType</th>\n",
       "      <th>creatDate</th>\n",
       "      <th>price</th>\n",
       "      <th>v_0</th>\n",
       "      <th>v_1</th>\n",
       "      <th>v_2</th>\n",
       "      <th>v_3</th>\n",
       "      <th>v_4</th>\n",
       "      <th>v_5</th>\n",
       "      <th>v_6</th>\n",
       "      <th>v_7</th>\n",
       "      <th>v_8</th>\n",
       "      <th>v_9</th>\n",
       "      <th>v_10</th>\n",
       "      <th>v_11</th>\n",
       "      <th>v_12</th>\n",
       "      <th>v_13</th>\n",
       "      <th>v_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>736</td>\n",
       "      <td>20040402</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1046</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160404</td>\n",
       "      <td>1850.0</td>\n",
       "      <td>43.34375</td>\n",
       "      <td>3.966797</td>\n",
       "      <td>0.050262</td>\n",
       "      <td>2.160156</td>\n",
       "      <td>1.143555</td>\n",
       "      <td>0.235718</td>\n",
       "      <td>0.101990</td>\n",
       "      <td>0.129517</td>\n",
       "      <td>0.022812</td>\n",
       "      <td>0.097473</td>\n",
       "      <td>-2.880859</td>\n",
       "      <td>2.804688</td>\n",
       "      <td>-2.419922</td>\n",
       "      <td>0.795410</td>\n",
       "      <td>0.914551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2262</td>\n",
       "      <td>20030301</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4366</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160309</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>45.31250</td>\n",
       "      <td>5.234375</td>\n",
       "      <td>0.137939</td>\n",
       "      <td>1.380859</td>\n",
       "      <td>-1.421875</td>\n",
       "      <td>0.264893</td>\n",
       "      <td>0.121033</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0.026596</td>\n",
       "      <td>0.020584</td>\n",
       "      <td>-4.902344</td>\n",
       "      <td>2.095703</td>\n",
       "      <td>-1.030273</td>\n",
       "      <td>-1.722656</td>\n",
       "      <td>0.245483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>14874</td>\n",
       "      <td>20040403</td>\n",
       "      <td>115.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2806</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160402</td>\n",
       "      <td>6222.0</td>\n",
       "      <td>45.96875</td>\n",
       "      <td>4.824219</td>\n",
       "      <td>1.319336</td>\n",
       "      <td>-0.998535</td>\n",
       "      <td>-0.997070</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.114929</td>\n",
       "      <td>0.165161</td>\n",
       "      <td>0.062164</td>\n",
       "      <td>0.027069</td>\n",
       "      <td>-4.847656</td>\n",
       "      <td>1.803711</td>\n",
       "      <td>1.565430</td>\n",
       "      <td>-0.832520</td>\n",
       "      <td>-0.229980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>71865</td>\n",
       "      <td>19960908</td>\n",
       "      <td>109.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>193</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>434</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160312</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>45.68750</td>\n",
       "      <td>4.492188</td>\n",
       "      <td>-0.050629</td>\n",
       "      <td>0.883789</td>\n",
       "      <td>-2.228516</td>\n",
       "      <td>0.274414</td>\n",
       "      <td>0.110291</td>\n",
       "      <td>0.121948</td>\n",
       "      <td>0.033386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.507812</td>\n",
       "      <td>1.286133</td>\n",
       "      <td>-0.501953</td>\n",
       "      <td>-2.437500</td>\n",
       "      <td>-0.478760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>111080</td>\n",
       "      <td>20120103</td>\n",
       "      <td>110.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6977</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160313</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>44.37500</td>\n",
       "      <td>2.031250</td>\n",
       "      <td>0.572266</td>\n",
       "      <td>-1.571289</td>\n",
       "      <td>2.246094</td>\n",
       "      <td>0.228027</td>\n",
       "      <td>0.073181</td>\n",
       "      <td>0.091858</td>\n",
       "      <td>0.078796</td>\n",
       "      <td>0.121521</td>\n",
       "      <td>-1.896484</td>\n",
       "      <td>0.910645</td>\n",
       "      <td>0.931152</td>\n",
       "      <td>2.833984</td>\n",
       "      <td>1.923828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>137642</td>\n",
       "      <td>20090602</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3690</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160319</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>46.31250</td>\n",
       "      <td>-3.228516</td>\n",
       "      <td>0.156616</td>\n",
       "      <td>-1.727539</td>\n",
       "      <td>-0.345703</td>\n",
       "      <td>0.260254</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.119812</td>\n",
       "      <td>0.090942</td>\n",
       "      <td>0.048767</td>\n",
       "      <td>1.885742</td>\n",
       "      <td>-2.722656</td>\n",
       "      <td>2.457031</td>\n",
       "      <td>-0.286865</td>\n",
       "      <td>0.206543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2402</td>\n",
       "      <td>19990411</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3073</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160317</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>46.09375</td>\n",
       "      <td>4.925781</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>1.644531</td>\n",
       "      <td>-1.270508</td>\n",
       "      <td>0.268066</td>\n",
       "      <td>0.117676</td>\n",
       "      <td>0.142334</td>\n",
       "      <td>0.025452</td>\n",
       "      <td>0.028168</td>\n",
       "      <td>-4.902344</td>\n",
       "      <td>1.610352</td>\n",
       "      <td>-0.834473</td>\n",
       "      <td>-1.996094</td>\n",
       "      <td>-0.103210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>165346</td>\n",
       "      <td>19990706</td>\n",
       "      <td>26.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160326</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>42.25000</td>\n",
       "      <td>-3.167969</td>\n",
       "      <td>-0.676758</td>\n",
       "      <td>1.942383</td>\n",
       "      <td>0.524414</td>\n",
       "      <td>0.239502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122925</td>\n",
       "      <td>0.039825</td>\n",
       "      <td>0.082397</td>\n",
       "      <td>3.693359</td>\n",
       "      <td>-0.244995</td>\n",
       "      <td>-2.193359</td>\n",
       "      <td>0.236694</td>\n",
       "      <td>0.195557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2974</td>\n",
       "      <td>20030205</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>179</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4679</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160326</td>\n",
       "      <td>2850.0</td>\n",
       "      <td>46.09375</td>\n",
       "      <td>4.894531</td>\n",
       "      <td>0.475342</td>\n",
       "      <td>0.556641</td>\n",
       "      <td>-1.262695</td>\n",
       "      <td>0.263916</td>\n",
       "      <td>0.116577</td>\n",
       "      <td>0.144287</td>\n",
       "      <td>0.039856</td>\n",
       "      <td>0.024384</td>\n",
       "      <td>-4.925781</td>\n",
       "      <td>1.587891</td>\n",
       "      <td>0.075317</td>\n",
       "      <td>-1.550781</td>\n",
       "      <td>0.069458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>82021</td>\n",
       "      <td>19980101</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160402</td>\n",
       "      <td>650.0</td>\n",
       "      <td>43.06250</td>\n",
       "      <td>1.666016</td>\n",
       "      <td>-2.201172</td>\n",
       "      <td>3.097656</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.262451</td>\n",
       "      <td>0.068237</td>\n",
       "      <td>0.012177</td>\n",
       "      <td>0.010292</td>\n",
       "      <td>0.098755</td>\n",
       "      <td>-1.089844</td>\n",
       "      <td>0.600586</td>\n",
       "      <td>-4.187500</td>\n",
       "      <td>0.198242</td>\n",
       "      <td>-1.025391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SaleID    name   regDate  model  brand  bodyType  fuelType  gearbox  power  \\\n",
       "0       0     736  20040402   30.0      6       1.0       0.0      0.0     60   \n",
       "1       1    2262  20030301   40.0      1       2.0       0.0      0.0      0   \n",
       "2       2   14874  20040403  115.0     15       1.0       0.0      0.0    163   \n",
       "3       3   71865  19960908  109.0     10       0.0       0.0      1.0    193   \n",
       "4       4  111080  20120103  110.0      5       1.0       0.0      0.0     68   \n",
       "5       5  137642  20090602   24.0     10       0.0       1.0      0.0    109   \n",
       "6       6    2402  19990411   13.0      4       0.0       0.0      1.0    150   \n",
       "7       7  165346  19990706   26.0     14       1.0       0.0      0.0    101   \n",
       "8       8    2974  20030205   19.0      1       2.0       1.0      1.0    179   \n",
       "9       9   82021  19980101    7.0      7       5.0       0.0      0.0     88   \n",
       "\n",
       "   kilometer  notRepairedDamage  regionCode  seller  offerType  creatDate  \\\n",
       "0       12.5                0.0        1046       0          0   20160404   \n",
       "1       15.0                0.0        4366       0          0   20160309   \n",
       "2       12.5                0.0        2806       0          0   20160402   \n",
       "3       15.0                0.0         434       0          0   20160312   \n",
       "4        5.0                0.0        6977       0          0   20160313   \n",
       "5       10.0                0.0        3690       0          0   20160319   \n",
       "6       15.0                0.0        3073       0          0   20160317   \n",
       "7       15.0                0.0        4000       0          0   20160326   \n",
       "8       15.0                0.0        4679       0          0   20160326   \n",
       "9       15.0                0.0         302       0          0   20160402   \n",
       "\n",
       "    price       v_0       v_1       v_2       v_3       v_4       v_5  \\\n",
       "0  1850.0  43.34375  3.966797  0.050262  2.160156  1.143555  0.235718   \n",
       "1  3600.0  45.31250  5.234375  0.137939  1.380859 -1.421875  0.264893   \n",
       "2  6222.0  45.96875  4.824219  1.319336 -0.998535 -0.997070  0.251465   \n",
       "3  2400.0  45.68750  4.492188 -0.050629  0.883789 -2.228516  0.274414   \n",
       "4  5200.0  44.37500  2.031250  0.572266 -1.571289  2.246094  0.228027   \n",
       "5  8000.0  46.31250 -3.228516  0.156616 -1.727539 -0.345703  0.260254   \n",
       "6  3500.0  46.09375  4.925781  0.113281  1.644531 -1.270508  0.268066   \n",
       "7  1000.0  42.25000 -3.167969 -0.676758  1.942383  0.524414  0.239502   \n",
       "8  2850.0  46.09375  4.894531  0.475342  0.556641 -1.262695  0.263916   \n",
       "9   650.0  43.06250  1.666016 -2.201172  3.097656  0.843750  0.262451   \n",
       "\n",
       "        v_6       v_7       v_8       v_9      v_10      v_11      v_12  \\\n",
       "0  0.101990  0.129517  0.022812  0.097473 -2.880859  2.804688 -2.419922   \n",
       "1  0.121033  0.135742  0.026596  0.020584 -4.902344  2.095703 -1.030273   \n",
       "2  0.114929  0.165161  0.062164  0.027069 -4.847656  1.803711  1.565430   \n",
       "3  0.110291  0.121948  0.033386  0.000000 -4.507812  1.286133 -0.501953   \n",
       "4  0.073181  0.091858  0.078796  0.121521 -1.896484  0.910645  0.931152   \n",
       "5  0.000518  0.119812  0.090942  0.048767  1.885742 -2.722656  2.457031   \n",
       "6  0.117676  0.142334  0.025452  0.028168 -4.902344  1.610352 -0.834473   \n",
       "7  0.000000  0.122925  0.039825  0.082397  3.693359 -0.244995 -2.193359   \n",
       "8  0.116577  0.144287  0.039856  0.024384 -4.925781  1.587891  0.075317   \n",
       "9  0.068237  0.012177  0.010292  0.098755 -1.089844  0.600586 -4.187500   \n",
       "\n",
       "       v_13      v_14  \n",
       "0  0.795410  0.914551  \n",
       "1 -1.722656  0.245483  \n",
       "2 -0.832520 -0.229980  \n",
       "3 -2.437500 -0.478760  \n",
       "4  2.833984  1.923828  \n",
       "5 -0.286865  0.206543  \n",
       "6 -1.996094 -0.103210  \n",
       "7  0.236694  0.195557  \n",
       "8 -1.550781  0.069458  \n",
       "9  0.198242 -1.025391  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:10:42.298378Z",
     "start_time": "2020-06-22T10:10:42.264468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SaleID</th>\n",
       "      <th>name</th>\n",
       "      <th>regDate</th>\n",
       "      <th>model</th>\n",
       "      <th>brand</th>\n",
       "      <th>bodyType</th>\n",
       "      <th>fuelType</th>\n",
       "      <th>gearbox</th>\n",
       "      <th>power</th>\n",
       "      <th>kilometer</th>\n",
       "      <th>notRepairedDamage</th>\n",
       "      <th>regionCode</th>\n",
       "      <th>seller</th>\n",
       "      <th>offerType</th>\n",
       "      <th>creatDate</th>\n",
       "      <th>price</th>\n",
       "      <th>v_0</th>\n",
       "      <th>v_1</th>\n",
       "      <th>v_2</th>\n",
       "      <th>v_3</th>\n",
       "      <th>v_4</th>\n",
       "      <th>v_5</th>\n",
       "      <th>v_6</th>\n",
       "      <th>v_7</th>\n",
       "      <th>v_8</th>\n",
       "      <th>v_9</th>\n",
       "      <th>v_10</th>\n",
       "      <th>v_11</th>\n",
       "      <th>v_12</th>\n",
       "      <th>v_13</th>\n",
       "      <th>v_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199990</th>\n",
       "      <td>249990</td>\n",
       "      <td>61395</td>\n",
       "      <td>19990906</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3160</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160331</td>\n",
       "      <td>500.0</td>\n",
       "      <td>43.09375</td>\n",
       "      <td>-3.195312</td>\n",
       "      <td>-1.672852</td>\n",
       "      <td>2.144531</td>\n",
       "      <td>-0.656738</td>\n",
       "      <td>0.265137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070984</td>\n",
       "      <td>0.033417</td>\n",
       "      <td>0.051117</td>\n",
       "      <td>3.181641</td>\n",
       "      <td>-1.394531</td>\n",
       "      <td>-2.490234</td>\n",
       "      <td>-1.065430</td>\n",
       "      <td>-0.442627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199991</th>\n",
       "      <td>249991</td>\n",
       "      <td>72277</td>\n",
       "      <td>20031106</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>224</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160403</td>\n",
       "      <td>500.0</td>\n",
       "      <td>45.96875</td>\n",
       "      <td>-3.212891</td>\n",
       "      <td>0.245117</td>\n",
       "      <td>-1.666992</td>\n",
       "      <td>-1.884766</td>\n",
       "      <td>0.268066</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.137573</td>\n",
       "      <td>0.086853</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>1.997070</td>\n",
       "      <td>-2.519531</td>\n",
       "      <td>2.419922</td>\n",
       "      <td>-1.568359</td>\n",
       "      <td>0.269043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199992</th>\n",
       "      <td>249992</td>\n",
       "      <td>29738</td>\n",
       "      <td>20071009</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160331</td>\n",
       "      <td>500.0</td>\n",
       "      <td>43.37500</td>\n",
       "      <td>3.027344</td>\n",
       "      <td>0.086121</td>\n",
       "      <td>0.496582</td>\n",
       "      <td>1.914062</td>\n",
       "      <td>0.230103</td>\n",
       "      <td>0.087463</td>\n",
       "      <td>0.096252</td>\n",
       "      <td>0.047455</td>\n",
       "      <td>0.116638</td>\n",
       "      <td>-2.238281</td>\n",
       "      <td>2.027344</td>\n",
       "      <td>-1.245117</td>\n",
       "      <td>2.113281</td>\n",
       "      <td>1.605469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199993</th>\n",
       "      <td>249993</td>\n",
       "      <td>35</td>\n",
       "      <td>19970712</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>193</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160328</td>\n",
       "      <td>500.0</td>\n",
       "      <td>44.71875</td>\n",
       "      <td>3.812500</td>\n",
       "      <td>-0.223511</td>\n",
       "      <td>2.148438</td>\n",
       "      <td>-1.631836</td>\n",
       "      <td>0.266113</td>\n",
       "      <td>0.100708</td>\n",
       "      <td>0.136597</td>\n",
       "      <td>0.018753</td>\n",
       "      <td>0.018051</td>\n",
       "      <td>-3.392578</td>\n",
       "      <td>1.723633</td>\n",
       "      <td>-1.783203</td>\n",
       "      <td>-2.158203</td>\n",
       "      <td>-0.417480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199994</th>\n",
       "      <td>249994</td>\n",
       "      <td>41919</td>\n",
       "      <td>20050807</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5640</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160330</td>\n",
       "      <td>500.0</td>\n",
       "      <td>46.40625</td>\n",
       "      <td>2.798828</td>\n",
       "      <td>0.323975</td>\n",
       "      <td>-1.174805</td>\n",
       "      <td>-1.170898</td>\n",
       "      <td>0.267090</td>\n",
       "      <td>0.085876</td>\n",
       "      <td>0.109436</td>\n",
       "      <td>0.067200</td>\n",
       "      <td>0.024033</td>\n",
       "      <td>-3.435547</td>\n",
       "      <td>-0.013077</td>\n",
       "      <td>1.506836</td>\n",
       "      <td>-0.947266</td>\n",
       "      <td>0.359863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>249995</td>\n",
       "      <td>111443</td>\n",
       "      <td>20041005</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5564</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160309</td>\n",
       "      <td>500.0</td>\n",
       "      <td>46.31250</td>\n",
       "      <td>-3.304688</td>\n",
       "      <td>0.073364</td>\n",
       "      <td>-0.622559</td>\n",
       "      <td>-0.778320</td>\n",
       "      <td>0.263672</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.141846</td>\n",
       "      <td>0.076416</td>\n",
       "      <td>0.039276</td>\n",
       "      <td>2.072266</td>\n",
       "      <td>-2.531250</td>\n",
       "      <td>1.716797</td>\n",
       "      <td>-1.063477</td>\n",
       "      <td>0.326660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>249996</td>\n",
       "      <td>152834</td>\n",
       "      <td>20130409</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160323</td>\n",
       "      <td>500.0</td>\n",
       "      <td>48.09375</td>\n",
       "      <td>-3.318359</td>\n",
       "      <td>0.965820</td>\n",
       "      <td>-2.671875</td>\n",
       "      <td>0.357422</td>\n",
       "      <td>0.255371</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.155884</td>\n",
       "      <td>0.108398</td>\n",
       "      <td>0.067871</td>\n",
       "      <td>1.358398</td>\n",
       "      <td>-3.291016</td>\n",
       "      <td>4.269531</td>\n",
       "      <td>0.140503</td>\n",
       "      <td>0.556152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>249997</td>\n",
       "      <td>132531</td>\n",
       "      <td>20041211</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>147</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3795</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160316</td>\n",
       "      <td>500.0</td>\n",
       "      <td>46.15625</td>\n",
       "      <td>-3.304688</td>\n",
       "      <td>-0.015282</td>\n",
       "      <td>-0.288330</td>\n",
       "      <td>-0.687012</td>\n",
       "      <td>0.262939</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.141846</td>\n",
       "      <td>0.071960</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>2.166016</td>\n",
       "      <td>-2.417969</td>\n",
       "      <td>1.371094</td>\n",
       "      <td>-1.073242</td>\n",
       "      <td>0.270508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>249998</td>\n",
       "      <td>143405</td>\n",
       "      <td>20020702</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>176</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160327</td>\n",
       "      <td>500.0</td>\n",
       "      <td>45.50000</td>\n",
       "      <td>-3.197266</td>\n",
       "      <td>-1.141602</td>\n",
       "      <td>-0.434814</td>\n",
       "      <td>-1.844727</td>\n",
       "      <td>0.282227</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.067505</td>\n",
       "      <td>0.067505</td>\n",
       "      <td>0.009003</td>\n",
       "      <td>2.029297</td>\n",
       "      <td>-2.939453</td>\n",
       "      <td>0.568848</td>\n",
       "      <td>-1.717773</td>\n",
       "      <td>0.316406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>249999</td>\n",
       "      <td>78202</td>\n",
       "      <td>20090708</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4158</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>500.0</td>\n",
       "      <td>44.28125</td>\n",
       "      <td>4.179688</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>-0.775879</td>\n",
       "      <td>1.790039</td>\n",
       "      <td>0.231445</td>\n",
       "      <td>0.103943</td>\n",
       "      <td>0.096008</td>\n",
       "      <td>0.062317</td>\n",
       "      <td>0.110168</td>\n",
       "      <td>-3.689453</td>\n",
       "      <td>2.033203</td>\n",
       "      <td>0.109131</td>\n",
       "      <td>2.203125</td>\n",
       "      <td>0.847656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SaleID    name   regDate  model  brand  bodyType  fuelType  gearbox  \\\n",
       "199990  249990   61395  19990906   19.0      5       3.0       0.0      0.0   \n",
       "199991  249991   72277  20031106   17.0     10       2.0       0.0      1.0   \n",
       "199992  249992   29738  20071009   41.0      6       1.0       0.0      0.0   \n",
       "199993  249993      35  19970712   13.0      4       2.0       0.0      1.0   \n",
       "199994  249994   41919  20050807    4.0      4       0.0       0.0      0.0   \n",
       "199995  249995  111443  20041005    4.0      4       0.0       0.0      1.0   \n",
       "199996  249996  152834  20130409   65.0      1       0.0       0.0      0.0   \n",
       "199997  249997  132531  20041211    4.0      4       0.0       0.0      1.0   \n",
       "199998  249998  143405  20020702   40.0      1       4.0       0.0      1.0   \n",
       "199999  249999   78202  20090708   32.0      8       1.0       0.0      0.0   \n",
       "\n",
       "        power  kilometer  notRepairedDamage  regionCode  seller  offerType  \\\n",
       "199990    132       15.0                0.0        3160       0          0   \n",
       "199991    224       15.0                0.0         163       0          0   \n",
       "199992     60        8.0                0.0        3929       0          0   \n",
       "199993    193       15.0                0.0        3258       0          0   \n",
       "199994    150       15.0                0.0        5640       0          0   \n",
       "199995    150       15.0                0.0        5564       0          0   \n",
       "199996    179        4.0                0.0        5220       0          0   \n",
       "199997    147       12.5                0.0        3795       0          0   \n",
       "199998    176       15.0                0.0          61       0          0   \n",
       "199999      0        3.0                0.0        4158       0          0   \n",
       "\n",
       "        creatDate  price       v_0       v_1       v_2       v_3       v_4  \\\n",
       "199990   20160331  500.0  43.09375 -3.195312 -1.672852  2.144531 -0.656738   \n",
       "199991   20160403  500.0  45.96875 -3.212891  0.245117 -1.666992 -1.884766   \n",
       "199992   20160331  500.0  43.37500  3.027344  0.086121  0.496582  1.914062   \n",
       "199993   20160328  500.0  44.71875  3.812500 -0.223511  2.148438 -1.631836   \n",
       "199994   20160330  500.0  46.40625  2.798828  0.323975 -1.174805 -1.170898   \n",
       "199995   20160309  500.0  46.31250 -3.304688  0.073364 -0.622559 -0.778320   \n",
       "199996   20160323  500.0  48.09375 -3.318359  0.965820 -2.671875  0.357422   \n",
       "199997   20160316  500.0  46.15625 -3.304688 -0.015282 -0.288330 -0.687012   \n",
       "199998   20160327  500.0  45.50000 -3.197266 -1.141602 -0.434814 -1.844727   \n",
       "199999   20160401  500.0  44.28125  4.179688  0.546875 -0.775879  1.790039   \n",
       "\n",
       "             v_5       v_6       v_7       v_8       v_9      v_10      v_11  \\\n",
       "199990  0.265137  0.000000  0.070984  0.033417  0.051117  3.181641 -1.394531   \n",
       "199991  0.268066  0.000387  0.137573  0.086853  0.002478  1.997070 -2.519531   \n",
       "199992  0.230103  0.087463  0.096252  0.047455  0.116638 -2.238281  2.027344   \n",
       "199993  0.266113  0.100708  0.136597  0.018753  0.018051 -3.392578  1.723633   \n",
       "199994  0.267090  0.085876  0.109436  0.067200  0.024033 -3.435547 -0.013077   \n",
       "199995  0.263672  0.000292  0.141846  0.076416  0.039276  2.072266 -2.531250   \n",
       "199996  0.255371  0.000991  0.155884  0.108398  0.067871  1.358398 -3.291016   \n",
       "199997  0.262939  0.000318  0.141846  0.071960  0.042969  2.166016 -2.417969   \n",
       "199998  0.282227  0.000023  0.067505  0.067505  0.009003  2.029297 -2.939453   \n",
       "199999  0.231445  0.103943  0.096008  0.062317  0.110168 -3.689453  2.033203   \n",
       "\n",
       "            v_12      v_13      v_14  \n",
       "199990 -2.490234 -1.065430 -0.442627  \n",
       "199991  2.419922 -1.568359  0.269043  \n",
       "199992 -1.245117  2.113281  1.605469  \n",
       "199993 -1.783203 -2.158203 -0.417480  \n",
       "199994  1.506836 -0.947266  0.359863  \n",
       "199995  1.716797 -1.063477  0.326660  \n",
       "199996  4.269531  0.140503  0.556152  \n",
       "199997  1.371094 -1.073242  0.270508  \n",
       "199998  0.568848 -1.717773  0.316406  \n",
       "199999  0.109131  2.203125  0.847656  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:10:51.852727Z",
     "start_time": "2020-06-22T10:10:42.299374Z"
    }
   },
   "outputs": [],
   "source": [
    "# 处理异常值\n",
    "concat_data['power'][concat_data['power'] > 600] = 600\n",
    "concat_data['power'][concat_data['power'] < 1] = 1\n",
    "\n",
    "concat_data['v_13'][concat_data['v_13'] > 6] = 6\n",
    "concat_data['v_14'][concat_data['v_14'] > 4] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:10:52.781206Z",
     "start_time": "2020-06-22T10:10:51.853429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 353)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v系列特征之间相加\n",
    "for j in ['v_' + str(i) for i in range(14)]:\n",
    "    for k in ['v_' + str(m) for m in range(14)]:\n",
    "        concat_data[j + '+' + k] = concat_data[j] + concat_data[k]\n",
    "\n",
    "# 原始特征与v系列特征之间相乘\n",
    "for i in ['model', 'brand', 'bodyType', 'fuelType', 'gearbox', 'power', 'kilometer', 'notRepairedDamage', 'regionCode']:\n",
    "    for j in ['v_' + str(k) for k in range(14)]:\n",
    "        concat_data[i + '*' + j] = concat_data[i] * concat_data[j]\n",
    "    \n",
    "concat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:10:53.632932Z",
     "start_time": "2020-06-22T10:10:52.782170Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200000, 361)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提取日期信息\n",
    "date_cols = ['regDate', 'creatDate']\n",
    "concat_data = date_tran(concat_data, date_cols)\n",
    "\n",
    "concat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:10:53.909159Z",
     "start_time": "2020-06-22T10:10:53.633890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SaleID', 'name', 'regDate', 'model', 'brand', 'bodyType', 'fuelType',\n",
       "       'gearbox', 'power', 'kilometer',\n",
       "       ...\n",
       "       'regionCode*v_12', 'regionCode*v_13', 'regDate_year', 'regDate_month',\n",
       "       'regDate_day', 'regDate_dayofweek', 'creatDate_year', 'creatDate_month',\n",
       "       'creatDate_day', 'creatDate_dayofweek'],\n",
       "      dtype='object', length=361)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = concat_data.copy()\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:10:54.110654Z",
     "start_time": "2020-06-22T10:10:53.910151Z"
    }
   },
   "outputs": [],
   "source": [
    "# count编码\n",
    "count_list = ['regDate', 'creatDate', 'model', 'brand', 'regionCode', 'bodyType', 'fuelType', 'name',\n",
    "              'regDate_year', 'regDate_month', 'regDate_day', 'regDate_dayofweek',\n",
    "              'creatDate_month', 'creatDate_day', 'creatDate_dayofweek', 'kilometer']\n",
    "data = count_coding(data, count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:10:54.174477Z",
     "start_time": "2020-06-22T10:10:54.111614Z"
    }
   },
   "outputs": [],
   "source": [
    "# 特征构造\n",
    "# 使用时间：data['creatDate'] - data['regDate']，反应汽车使用时间，一般来说价格与使用时间成反比\n",
    "# 不过要注意，数据里有时间出错的格式，所以我们需要 errors='coerce'\n",
    "data['used_time1'] = (pd.to_datetime(data['creatDate'], format='%Y%m%d', errors='coerce') - \n",
    "                      pd.to_datetime(data['regDate'], format='%Y%m%d', errors='coerce')).dt.days\n",
    "data['used_time2'] = (pd.datetime.now() - pd.to_datetime(data['regDate'], format='%Y%m%d', errors='coerce')).dt.days                        \n",
    "data['used_time3'] = (pd.datetime.now() - pd.to_datetime(data['creatDate'], format='%Y%m%d', errors='coerce') ).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:10:54.236585Z",
     "start_time": "2020-06-22T10:10:54.175442Z"
    }
   },
   "outputs": [],
   "source": [
    "# 分桶，注意：kilometer应该是已经离散化了的\n",
    "cut_cols = ['power'] + ['used_time1', 'used_time2', 'used_time3']\n",
    "data = cut_group(data, cut_cols, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:11:05.974952Z",
     "start_time": "2020-06-22T10:10:54.237586Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\n",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|████████████                                                                        | 1/7 [00:01<00:08,  1.34s/it]\u001b[A\n",
      " 29%|████████████████████████                                                            | 2/7 [00:01<00:05,  1.03s/it]\u001b[A\n",
      " 43%|████████████████████████████████████                                                | 3/7 [00:01<00:03,  1.22it/s]\u001b[A\n",
      " 57%|████████████████████████████████████████████████                                    | 4/7 [00:02<00:02,  1.49it/s]\u001b[A\n",
      " 71%|████████████████████████████████████████████████████████████                        | 5/7 [00:02<00:01,  1.77it/s]\u001b[A\n",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [00:02<00:00,  2.02it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:03<00:00,  2.13it/s]\u001b[A\n",
      " 33%|████████████████████████████                                                        | 1/3 [00:03<00:06,  3.35s/it]\n",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|████████████                                                                        | 1/7 [00:01<00:09,  1.62s/it]\u001b[A\n",
      " 29%|████████████████████████                                                            | 2/7 [00:01<00:06,  1.23s/it]\u001b[A\n",
      " 43%|████████████████████████████████████                                                | 3/7 [00:02<00:03,  1.04it/s]\u001b[A\n",
      " 57%|████████████████████████████████████████████████                                    | 4/7 [00:02<00:02,  1.29it/s]\u001b[A\n",
      " 71%|████████████████████████████████████████████████████████████                        | 5/7 [00:02<00:01,  1.56it/s]\u001b[A\n",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [00:03<00:00,  1.82it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:03<00:00,  1.93it/s]\u001b[A\n",
      " 67%|████████████████████████████████████████████████████████                            | 2/3 [00:07<00:03,  3.50s/it]\n",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|████████████                                                                        | 1/7 [00:01<00:11,  1.98s/it]\u001b[A\n",
      " 29%|████████████████████████                                                            | 2/7 [00:02<00:07,  1.49s/it]\u001b[A\n",
      " 43%|████████████████████████████████████                                                | 3/7 [00:02<00:04,  1.16s/it]\u001b[A\n",
      " 57%|████████████████████████████████████████████████                                    | 4/7 [00:03<00:02,  1.08it/s]\u001b[A\n",
      " 71%|████████████████████████████████████████████████████████████                        | 5/7 [00:03<00:01,  1.32it/s]\u001b[A\n",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [00:03<00:00,  1.56it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:04<00:00,  1.67it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:11<00:00,  3.90s/it]\n"
     ]
    }
   ],
   "source": [
    "# 用数值特征对类别特征做统计刻画，随便挑了几个跟price相关性最高的匿名特征\n",
    "cross_cat = ['model', 'brand','regDate_year']\n",
    "cross_num = ['v_0','v_3', 'v_4', 'v_8', 'v_12','power', 'used_time1']\n",
    "data = cross_cat_num(data, cross_num, cross_cat) # 一阶交叉\n",
    "# data = cross_qua_cat_num(data) # 二阶交叉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:11:05.980976Z",
     "start_time": "2020-06-22T10:11:05.975951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SaleID', 'name', 'regDate', 'model', 'brand', 'bodyType', 'fuelType',\n",
      "       'gearbox', 'power', 'kilometer',\n",
      "       ...\n",
      "       'regDate_year_v_8_median', 'regDate_year_v_12_max',\n",
      "       'regDate_year_v_12_min', 'regDate_year_v_12_median',\n",
      "       'regDate_year_power_max', 'regDate_year_power_min',\n",
      "       'regDate_year_power_median', 'regDate_year_used_time1_max',\n",
      "       'regDate_year_used_time1_min', 'regDate_year_used_time1_median'],\n",
      "      dtype='object', length=447)\n"
     ]
    }
   ],
   "source": [
    "# 选择特征列\n",
    "numerical_cols = data.columns\n",
    "print(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:11:06.513547Z",
     "start_time": "2020-06-22T10:11:05.981934Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_fea = ['SaleID', 'offerType', 'seller']\n",
    "feature_cols = [col for col in numerical_cols if col not in cat_fea]\n",
    "feature_cols = [col for col in feature_cols if col not in ['price']]\n",
    "\n",
    "# 将训练集和测试集分开\n",
    "X_data = data.iloc[:len(Train_data), :][feature_cols]\n",
    "Y_data = Train_data['price']\n",
    "X_test  = data.iloc[len(Train_data):, :][feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:11:06.538808Z",
     "start_time": "2020-06-22T10:11:06.514509Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from itertools import product\n",
    "class MeanEncoder:\n",
    "    def __init__(self, categorical_features, n_splits=10, target_type='classification', prior_weight_func=None):\n",
    "        \"\"\"\n",
    "        :param categorical_features: list of str, the name of the categorical columns to encode\n",
    " \n",
    "        :param n_splits: the number of splits used in mean encoding\n",
    " \n",
    "        :param target_type: str, 'regression' or 'classification'\n",
    " \n",
    "        :param prior_weight_func:\n",
    "        a function that takes in the number of observations, and outputs prior weight\n",
    "        when a dict is passed, the default exponential decay function will be used:\n",
    "        k: the number of observations needed for the posterior to be weighted equally as the prior\n",
    "        f: larger f --> smaller slope\n",
    "        \"\"\"\n",
    " \n",
    "        self.categorical_features = categorical_features\n",
    "        self.n_splits = n_splits\n",
    "        self.learned_stats = {}\n",
    " \n",
    "        if target_type == 'classification':\n",
    "            self.target_type = target_type\n",
    "            self.target_values = []\n",
    "        else:\n",
    "            self.target_type = 'regression'\n",
    "            self.target_values = None\n",
    " \n",
    "        if isinstance(prior_weight_func, dict):\n",
    "            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n",
    "        elif callable(prior_weight_func):\n",
    "            self.prior_weight_func = prior_weight_func\n",
    "        else:\n",
    "            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n",
    " \n",
    "    @staticmethod\n",
    "    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n",
    "        X_train = X_train[[variable]].copy()\n",
    "        X_test = X_test[[variable]].copy()\n",
    " \n",
    "        if target is not None:\n",
    "            nf_name = '{}_pred_{}'.format(variable, target)\n",
    "            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n",
    "        else:\n",
    "            nf_name = '{}_pred'.format(variable)\n",
    "            X_train['pred_temp'] = y_train  # regression\n",
    "        prior = X_train['pred_temp'].mean()\n",
    " \n",
    "        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg(['mean', 'size'])\n",
    "        col_avg_y['size'] = prior_weight_func(col_avg_y['size'])\n",
    "        col_avg_y[nf_name] = col_avg_y['size'] * prior + (1 - col_avg_y['size']) * col_avg_y['mean']\n",
    "        col_avg_y.drop(['size', 'mean'], axis=1, inplace=True)\n",
    " \n",
    "        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n",
    "        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n",
    " \n",
    "        return nf_train, nf_test, prior, col_avg_y\n",
    " \n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :param y: pandas Series or numpy array, n_samples\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "        if self.target_type == 'classification':\n",
    "            skf = StratifiedKFold(self.n_splits)\n",
    "        else:\n",
    "            skf = KFold(self.n_splits)\n",
    " \n",
    "        if self.target_type == 'classification':\n",
    "            self.target_values = sorted(set(y))\n",
    "            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n",
    "                                  product(self.categorical_features, self.target_values)}\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(X, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        else:\n",
    "            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(X, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        return X_new\n",
    " \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    " \n",
    "        if self.target_type == 'classification':\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "        else:\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    " \n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:11:15.327266Z",
     "start_time": "2020-06-22T10:11:06.542784Z"
    }
   },
   "outputs": [],
   "source": [
    "class_list = ['model', 'brand', 'name', 'regionCode'] + date_cols\n",
    "MeanEncodeFeature = class_list\n",
    "ME = MeanEncoder(categorical_features=MeanEncodeFeature, n_splits=5, target_type='regression', prior_weight_func=None)\n",
    "X_data = ME.fit_transform(X_data, Y_data)\n",
    "X_test = ME.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:11:15.333840Z",
     "start_time": "2020-06-22T10:11:15.327266Z"
    }
   },
   "outputs": [],
   "source": [
    "X_data['price'] = Train_data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:11:39.279732Z",
     "start_time": "2020-06-22T10:11:15.335348Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:23<00:00,  3.98s/it]\n"
     ]
    }
   ],
   "source": [
    "# target encoding目标编码，回归场景相对来说做目标编码的选择更多，不仅可以做均值编码，还可以做标准差编码、中位数编码等\n",
    "enc_cols = []\n",
    "stats_default_dict = {\n",
    "    'max': X_data['price'].max(),\n",
    "    'min': X_data['price'].min(),\n",
    "    'median': X_data['price'].median(),\n",
    "    'mean': X_data['price'].mean(),\n",
    "    'sum': X_data['price'].sum(),\n",
    "    'std': X_data['price'].std(),\n",
    "    'skew': X_data['price'].skew(), # 偏度\n",
    "    'kurt': X_data['price'].kurt(), # 峰度\n",
    "    'mad': X_data['price'].mad() # mean absolute deviation 平均绝对偏差\n",
    "}\n",
    "\n",
    "# 暂且选择这三种编码\n",
    "enc_stats = ['max', 'min', 'mean']\n",
    "skf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for f in tqdm(['regionCode', 'brand', 'regDate_year' ,'creatDate_year', 'kilometer', 'model']):\n",
    "    enc_dict = {}\n",
    "    for stat in enc_stats:\n",
    "        enc_dict['{}_target_{}'.format(f, stat)] = stat\n",
    "        X_data['{}_target_{}'.format(f, stat)] = 0\n",
    "        X_test['{}_target_{}'.format(f, stat)] = 0\n",
    "        enc_cols.append('{}_target_{}'.format(f, stat))\n",
    "    for i, (trn_idx, val_idx) in enumerate(skf.split(X_data, Y_data)):\n",
    "        trn_x, val_x = X_data.iloc[trn_idx].reset_index(drop=True), X_data.iloc[val_idx].reset_index(drop=True)\n",
    "        enc_df = trn_x.groupby(f, as_index=False)['price'].agg(enc_dict)\n",
    "        val_x = val_x[[f]].merge(enc_df, on=f, how='left')\n",
    "        test_x = X_test[[f]].merge(enc_df, on=f, how='left')\n",
    "        for stat in enc_stats:\n",
    "            val_x['{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].fillna(stats_default_dict[stat])\n",
    "            test_x['{}_target_{}'.format(f, stat)] = test_x['{}_target_{}'.format(f, stat)].fillna(stats_default_dict[stat])\n",
    "            X_data.loc[val_idx, '{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].values \n",
    "            X_test['{}_target_{}'.format(f, stat)] += test_x['{}_target_{}'.format(f, stat)].values / skf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:11:39.527109Z",
     "start_time": "2020-06-22T10:11:39.280744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 463)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_list = ['regDate', 'creatDate', 'brand_power_min', 'regDate_year_power_min']\n",
    "x_train = X_data.drop(drop_list + ['price'], axis=1)\n",
    "x_test = X_test.drop(drop_list, axis=1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:11:39.817877Z",
     "start_time": "2020-06-22T10:11:39.528073Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:11:41.085621Z",
     "start_time": "2020-06-22T10:11:39.818875Z"
    }
   },
   "outputs": [],
   "source": [
    "# 特征归一化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(pd.concat([x_train, x_test]).values)\n",
    "all_data = min_max_scaler.transform(pd.concat([x_train, x_test]).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:11:49.578332Z",
     "start_time": "2020-06-22T10:11:41.086618Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=200)\n",
    "all_pca = pca.fit_transform(all_data)\n",
    "X_pca = all_pca[:len(x_train)]\n",
    "test = all_pca[len(x_train):]\n",
    "y = Train_data['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:11:51.023755Z",
     "start_time": "2020-06-22T10:11:49.579329Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, Activation, MaxPool1D, Flatten, Dense\n",
    "from keras.layers import Input, Dense, Concatenate, Reshape, Dropout, merge, Add\n",
    "def NN_model(input_dim):\n",
    "    init = keras.initializers.glorot_uniform(seed=1)\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Dense(units=300, input_dim=input_dim, kernel_initializer=init, activation='softplus'))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=300, kernel_initializer=init, activation='softplus'))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=64, kernel_initializer=init, activation='softplus'))\n",
    "    model.add(Dense(units=32, kernel_initializer=init, activation='softplus'))\n",
    "    model.add(Dense(units=8, kernel_initializer=init, activation='softplus'))\n",
    "    model.add(Dense(units=1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:11:51.035745Z",
     "start_time": "2020-06-22T10:11:51.024718Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, EarlyStopping\n",
    "class Metric(Callback):\n",
    "    def __init__(self, model, callbacks, data):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.callbacks = callbacks\n",
    "        self.data = data\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_train_begin(logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_train_end(logs)\n",
    "\n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "        X_train, y_train = self.data[0][0], self.data[0][1]\n",
    "        y_pred3 = self.model.predict(X_train)\n",
    "        y_pred = np.zeros((len(y_pred3), ))\n",
    "        y_true = np.zeros((len(y_pred3), ))\n",
    "        for i in range(len(y_pred3)):\n",
    "            y_pred[i] = y_pred3[i]\n",
    "        for i in range(len(y_pred3)):\n",
    "            y_true[i] = y_train[i]\n",
    "        trn_s = mean_absolute_error(y_true, y_pred)\n",
    "        logs['trn_score'] = trn_s\n",
    "        \n",
    "        X_val, y_val = self.data[1][0], self.data[1][1]\n",
    "        y_pred3 = self.model.predict(X_val)\n",
    "        y_pred = np.zeros((len(y_pred3), ))\n",
    "        y_true = np.zeros((len(y_pred3), ))\n",
    "        for i in range(len(y_pred3)):\n",
    "            y_pred[i] = y_pred3[i]\n",
    "        for i in range(len(y_pred3)):\n",
    "            y_true[i] = y_val[i]\n",
    "        val_s = mean_absolute_error(y_true, y_pred)\n",
    "        logs['val_score'] = val_s\n",
    "        print('trn_score', trn_s, 'val_score', val_s)\n",
    "\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_epoch_end(batch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:11:51.050702Z",
     "start_time": "2020-06-22T10:11:51.036689Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "  \n",
    "def scheduler(epoch):\n",
    "    # 每隔100个epoch，学习率减小为原来的1/10\n",
    "    if epoch % 20 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.6)\n",
    "        print(\"lr changed to {}\".format(lr * 0.6))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "reduce_lr = LearningRateScheduler(scheduler)\n",
    "# model.fit(train_x, train_y, batch_size=32, epochs=5, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:43:01.174727Z",
     "start_time": "2020-06-22T10:11:51.051694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "WARNING:tensorflow:From C:\\Users\\z\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\z\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 125000 samples, validate on 25000 samples\n",
      "Epoch 1/145\n",
      " - 2s - loss: 2742.0239 - mae: 2742.0237 - val_loss: 980.4724 - val_mae: 980.4724\n",
      "Epoch 2/145\n",
      " - 2s - loss: 861.4175 - mae: 861.4174 - val_loss: 761.1672 - val_mae: 761.1672\n",
      "Epoch 3/145\n",
      " - 2s - loss: 690.1249 - mae: 690.1250 - val_loss: 652.8037 - val_mae: 652.8036\n",
      "Epoch 4/145\n",
      " - 2s - loss: 624.0540 - mae: 624.0540 - val_loss: 660.0627 - val_mae: 660.0627\n",
      "Epoch 5/145\n",
      " - 2s - loss: 577.0155 - mae: 577.0156 - val_loss: 578.1737 - val_mae: 578.1737\n",
      "Epoch 6/145\n",
      " - 2s - loss: 579.0853 - mae: 579.0853 - val_loss: 539.6563 - val_mae: 539.6563\n",
      "Epoch 7/145\n",
      " - 2s - loss: 541.2288 - mae: 541.2288 - val_loss: 543.8874 - val_mae: 543.8874\n",
      "Epoch 8/145\n",
      " - 2s - loss: 538.8325 - mae: 538.8325 - val_loss: 520.2231 - val_mae: 520.2231\n",
      "Epoch 9/145\n",
      " - 2s - loss: 523.1769 - mae: 523.1769 - val_loss: 511.0036 - val_mae: 511.0036\n",
      "Epoch 10/145\n",
      " - 2s - loss: 568.2984 - mae: 568.2983 - val_loss: 567.7292 - val_mae: 567.7292\n",
      "Epoch 11/145\n",
      " - 2s - loss: 525.6150 - mae: 525.6150 - val_loss: 517.2984 - val_mae: 517.2984\n",
      "Epoch 12/145\n",
      " - 2s - loss: 518.2771 - mae: 518.2771 - val_loss: 549.1803 - val_mae: 549.1804\n",
      "Epoch 13/145\n",
      " - 2s - loss: 510.3362 - mae: 510.3362 - val_loss: 495.7113 - val_mae: 495.7112\n",
      "Epoch 14/145\n",
      " - 2s - loss: 492.3020 - mae: 492.3019 - val_loss: 501.1250 - val_mae: 501.1250\n",
      "Epoch 15/145\n",
      " - 2s - loss: 524.5404 - mae: 524.5403 - val_loss: 494.7378 - val_mae: 494.7379\n",
      "Epoch 16/145\n",
      " - 2s - loss: 476.0011 - mae: 476.0011 - val_loss: 502.3376 - val_mae: 502.3376\n",
      "Epoch 17/145\n",
      " - 2s - loss: 512.9854 - mae: 512.9853 - val_loss: 502.5708 - val_mae: 502.5708\n",
      "Epoch 18/145\n",
      " - 2s - loss: 501.6827 - mae: 501.6826 - val_loss: 486.5187 - val_mae: 486.5186\n",
      "Epoch 19/145\n",
      " - 2s - loss: 485.5816 - mae: 485.5817 - val_loss: 489.0056 - val_mae: 489.0056\n",
      "Epoch 20/145\n",
      " - 2s - loss: 467.2109 - mae: 467.2108 - val_loss: 517.3610 - val_mae: 517.3610\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      " - 2s - loss: 447.8904 - mae: 447.8904 - val_loss: 457.5826 - val_mae: 457.5826\n",
      "Epoch 22/145\n",
      " - 2s - loss: 439.0514 - mae: 439.0514 - val_loss: 453.0894 - val_mae: 453.0894\n",
      "Epoch 23/145\n",
      " - 2s - loss: 449.1092 - mae: 449.1092 - val_loss: 449.8687 - val_mae: 449.8687\n",
      "Epoch 24/145\n",
      " - 2s - loss: 437.5255 - mae: 437.5255 - val_loss: 457.7837 - val_mae: 457.7837\n",
      "Epoch 25/145\n",
      " - 2s - loss: 437.1643 - mae: 437.1643 - val_loss: 444.5531 - val_mae: 444.5531\n",
      "Epoch 26/145\n",
      " - 2s - loss: 435.6893 - mae: 435.6894 - val_loss: 450.4754 - val_mae: 450.4754\n",
      "Epoch 27/145\n",
      " - 2s - loss: 434.0162 - mae: 434.0162 - val_loss: 442.0713 - val_mae: 442.0713\n",
      "Epoch 28/145\n",
      " - 2s - loss: 431.6431 - mae: 431.6431 - val_loss: 442.3938 - val_mae: 442.3937\n",
      "Epoch 29/145\n",
      " - 2s - loss: 439.5190 - mae: 439.5189 - val_loss: 446.7643 - val_mae: 446.7643\n",
      "Epoch 30/145\n",
      " - 2s - loss: 431.0731 - mae: 431.0731 - val_loss: 440.9032 - val_mae: 440.9032\n",
      "Epoch 31/145\n",
      " - 2s - loss: 448.8947 - mae: 448.8948 - val_loss: 471.0633 - val_mae: 471.0634\n",
      "Epoch 32/145\n",
      " - 2s - loss: 459.4315 - mae: 459.4314 - val_loss: 446.7245 - val_mae: 446.7244\n",
      "Epoch 33/145\n",
      " - 2s - loss: 439.6549 - mae: 439.6549 - val_loss: 501.0140 - val_mae: 501.0140\n",
      "Epoch 34/145\n",
      " - 2s - loss: 431.2191 - mae: 431.2191 - val_loss: 474.0817 - val_mae: 474.0817\n",
      "Epoch 35/145\n",
      " - 2s - loss: 431.7044 - mae: 431.7044 - val_loss: 432.3008 - val_mae: 432.3008\n",
      "Epoch 36/145\n",
      " - 2s - loss: 416.7198 - mae: 416.7198 - val_loss: 456.6675 - val_mae: 456.6675\n",
      "Epoch 37/145\n",
      " - 2s - loss: 422.9524 - mae: 422.9524 - val_loss: 436.0838 - val_mae: 436.0837\n",
      "Epoch 38/145\n",
      " - 2s - loss: 431.0077 - mae: 431.0078 - val_loss: 486.0439 - val_mae: 486.0439\n",
      "Epoch 39/145\n",
      " - 2s - loss: 438.5674 - mae: 438.5674 - val_loss: 434.5889 - val_mae: 434.5889\n",
      "Epoch 40/145\n",
      " - 2s - loss: 415.0937 - mae: 415.0937 - val_loss: 443.7155 - val_mae: 443.7155\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      " - 2s - loss: 409.1551 - mae: 409.1552 - val_loss: 428.3227 - val_mae: 428.3227\n",
      "Epoch 42/145\n",
      " - 2s - loss: 403.8742 - mae: 403.8742 - val_loss: 427.9063 - val_mae: 427.9063\n",
      "Epoch 43/145\n",
      " - 2s - loss: 406.4566 - mae: 406.4567 - val_loss: 426.2917 - val_mae: 426.2917\n",
      "Epoch 44/145\n",
      " - 2s - loss: 403.5537 - mae: 403.5538 - val_loss: 428.8078 - val_mae: 428.8078\n",
      "Epoch 45/145\n",
      " - 2s - loss: 402.2222 - mae: 402.2222 - val_loss: 425.4317 - val_mae: 425.4317\n",
      "Epoch 46/145\n",
      " - 2s - loss: 401.6264 - mae: 401.6265 - val_loss: 427.4936 - val_mae: 427.4937\n",
      "Epoch 47/145\n",
      " - 2s - loss: 401.5452 - mae: 401.5452 - val_loss: 430.4689 - val_mae: 430.4689\n",
      "Epoch 48/145\n",
      " - 2s - loss: 401.4409 - mae: 401.4409 - val_loss: 447.5902 - val_mae: 447.5902\n",
      "Epoch 49/145\n",
      " - 2s - loss: 403.2339 - mae: 403.2339 - val_loss: 422.8434 - val_mae: 422.8434\n",
      "Epoch 50/145\n",
      " - 2s - loss: 399.1834 - mae: 399.1834 - val_loss: 424.5845 - val_mae: 424.5845\n",
      "Epoch 51/145\n",
      " - 2s - loss: 397.7730 - mae: 397.7730 - val_loss: 424.1054 - val_mae: 424.1054\n",
      "Epoch 52/145\n",
      " - 2s - loss: 398.1226 - mae: 398.1225 - val_loss: 426.7606 - val_mae: 426.7606\n",
      "Epoch 53/145\n",
      " - 2s - loss: 399.0253 - mae: 399.0253 - val_loss: 436.3795 - val_mae: 436.3795\n",
      "Epoch 54/145\n",
      " - 2s - loss: 405.2406 - mae: 405.2406 - val_loss: 424.1878 - val_mae: 424.1878\n",
      "Epoch 55/145\n",
      " - 2s - loss: 396.8640 - mae: 396.8640 - val_loss: 421.3487 - val_mae: 421.3487\n",
      "Epoch 56/145\n",
      " - 2s - loss: 396.1166 - mae: 396.1166 - val_loss: 429.1894 - val_mae: 429.1894\n",
      "Epoch 57/145\n",
      " - 2s - loss: 396.5333 - mae: 396.5334 - val_loss: 424.5041 - val_mae: 424.5041\n",
      "Epoch 58/145\n",
      " - 2s - loss: 395.4546 - mae: 395.4546 - val_loss: 455.1291 - val_mae: 455.1291\n",
      "Epoch 59/145\n",
      " - 2s - loss: 399.0731 - mae: 399.0731 - val_loss: 439.8043 - val_mae: 439.8043\n",
      "Epoch 60/145\n",
      " - 2s - loss: 396.4704 - mae: 396.4705 - val_loss: 432.8781 - val_mae: 432.8781\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      " - 2s - loss: 388.6363 - mae: 388.6363 - val_loss: 418.4073 - val_mae: 418.4073\n",
      "Epoch 62/145\n",
      " - 2s - loss: 389.0490 - mae: 389.0490 - val_loss: 425.2043 - val_mae: 425.2044\n",
      "Epoch 63/145\n",
      " - 2s - loss: 390.4584 - mae: 390.4584 - val_loss: 425.8527 - val_mae: 425.8526\n",
      "Epoch 64/145\n",
      " - 2s - loss: 385.4265 - mae: 385.4266 - val_loss: 418.3791 - val_mae: 418.3791\n",
      "Epoch 65/145\n",
      " - 2s - loss: 384.6223 - mae: 384.6224 - val_loss: 420.9739 - val_mae: 420.9739\n",
      "Epoch 66/145\n",
      " - 2s - loss: 385.1100 - mae: 385.1099 - val_loss: 420.7556 - val_mae: 420.7556\n",
      "Epoch 67/145\n",
      " - 2s - loss: 384.7745 - mae: 384.7745 - val_loss: 421.3537 - val_mae: 421.3537\n",
      "Epoch 68/145\n",
      " - 2s - loss: 385.3685 - mae: 385.3685 - val_loss: 427.2827 - val_mae: 427.2827\n",
      "Epoch 69/145\n",
      " - 2s - loss: 384.4529 - mae: 384.4529 - val_loss: 417.2541 - val_mae: 417.2540\n",
      "Epoch 70/145\n",
      " - 2s - loss: 382.4096 - mae: 382.4096 - val_loss: 419.7780 - val_mae: 419.7780\n",
      "Epoch 71/145\n",
      " - 2s - loss: 382.7795 - mae: 382.7795 - val_loss: 418.4695 - val_mae: 418.4695\n",
      "Epoch 72/145\n",
      " - 2s - loss: 382.2143 - mae: 382.2142 - val_loss: 416.7673 - val_mae: 416.7672\n",
      "Epoch 73/145\n",
      " - 2s - loss: 381.9524 - mae: 381.9524 - val_loss: 416.3568 - val_mae: 416.3568\n",
      "Epoch 74/145\n",
      " - 2s - loss: 381.9981 - mae: 381.9981 - val_loss: 421.3039 - val_mae: 421.3039\n",
      "Epoch 75/145\n",
      " - 2s - loss: 384.2048 - mae: 384.2048 - val_loss: 419.6419 - val_mae: 419.6419\n",
      "Epoch 76/145\n",
      " - 2s - loss: 381.0057 - mae: 381.0057 - val_loss: 418.0613 - val_mae: 418.0612\n",
      "Epoch 77/145\n",
      " - 2s - loss: 388.3595 - mae: 388.3595 - val_loss: 436.1412 - val_mae: 436.1412\n",
      "Epoch 78/145\n",
      " - 2s - loss: 386.2211 - mae: 386.2211 - val_loss: 416.9047 - val_mae: 416.9046\n",
      "Epoch 79/145\n",
      " - 2s - loss: 377.9767 - mae: 377.9767 - val_loss: 419.1515 - val_mae: 419.1515\n",
      "Epoch 80/145\n",
      " - 2s - loss: 381.7771 - mae: 381.7771 - val_loss: 425.2179 - val_mae: 425.2179\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 376.6207 - mae: 376.6208 - val_loss: 415.0958 - val_mae: 415.0958\n",
      "Epoch 82/145\n",
      " - 2s - loss: 374.8157 - mae: 374.8157 - val_loss: 415.2751 - val_mae: 415.2751\n",
      "Epoch 83/145\n",
      " - 2s - loss: 374.0000 - mae: 373.9999 - val_loss: 414.5203 - val_mae: 414.5202\n",
      "Epoch 84/145\n",
      " - 2s - loss: 375.3843 - mae: 375.3843 - val_loss: 414.8520 - val_mae: 414.8520\n",
      "Epoch 85/145\n",
      " - 2s - loss: 374.4219 - mae: 374.4220 - val_loss: 415.9802 - val_mae: 415.9803\n",
      "Epoch 86/145\n",
      " - 2s - loss: 374.2627 - mae: 374.2627 - val_loss: 415.8082 - val_mae: 415.8082\n",
      "Epoch 87/145\n",
      " - 2s - loss: 373.0823 - mae: 373.0823 - val_loss: 416.6590 - val_mae: 416.6590\n",
      "Epoch 88/145\n",
      " - 2s - loss: 373.2995 - mae: 373.2994 - val_loss: 415.5583 - val_mae: 415.5583\n",
      "Epoch 89/145\n",
      " - 2s - loss: 373.0149 - mae: 373.0149 - val_loss: 416.0504 - val_mae: 416.0504\n",
      "Epoch 90/145\n",
      " - 2s - loss: 373.7417 - mae: 373.7417 - val_loss: 415.7240 - val_mae: 415.7240\n",
      "Epoch 91/145\n",
      " - 2s - loss: 372.4730 - mae: 372.4730 - val_loss: 416.6339 - val_mae: 416.6339\n",
      "Epoch 92/145\n",
      " - 2s - loss: 372.1222 - mae: 372.1222 - val_loss: 414.4784 - val_mae: 414.4784\n",
      "Epoch 93/145\n",
      " - 2s - loss: 373.6749 - mae: 373.6749 - val_loss: 415.7323 - val_mae: 415.7323\n",
      "Epoch 94/145\n",
      " - 2s - loss: 373.0945 - mae: 373.0945 - val_loss: 418.4995 - val_mae: 418.4995\n",
      "Epoch 95/145\n",
      " - 2s - loss: 372.3915 - mae: 372.3914 - val_loss: 413.9060 - val_mae: 413.9061\n",
      "Epoch 96/145\n",
      " - 2s - loss: 371.7099 - mae: 371.7098 - val_loss: 415.3930 - val_mae: 415.3930\n",
      "Epoch 97/145\n",
      " - 2s - loss: 371.6206 - mae: 371.6206 - val_loss: 416.0477 - val_mae: 416.0477\n",
      "Epoch 98/145\n",
      " - 2s - loss: 372.9467 - mae: 372.9467 - val_loss: 418.5801 - val_mae: 418.5801\n",
      "Epoch 99/145\n",
      " - 2s - loss: 372.6661 - mae: 372.6661 - val_loss: 414.1745 - val_mae: 414.1744\n",
      "Epoch 100/145\n",
      " - 2s - loss: 371.0026 - mae: 371.0027 - val_loss: 416.2815 - val_mae: 416.2816\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      " - 2s - loss: 368.3001 - mae: 368.3001 - val_loss: 413.4267 - val_mae: 413.4267\n",
      "Epoch 102/145\n",
      " - 2s - loss: 367.8553 - mae: 367.8552 - val_loss: 414.4672 - val_mae: 414.4673\n",
      "Epoch 103/145\n",
      " - 2s - loss: 367.5236 - mae: 367.5237 - val_loss: 416.5093 - val_mae: 416.5093\n",
      "Epoch 104/145\n",
      " - 2s - loss: 367.5697 - mae: 367.5697 - val_loss: 414.8212 - val_mae: 414.8211\n",
      "Epoch 105/145\n",
      " - 2s - loss: 366.8438 - mae: 366.8438 - val_loss: 413.4829 - val_mae: 413.4829\n",
      "Epoch 106/145\n",
      " - 2s - loss: 367.0928 - mae: 367.0927 - val_loss: 412.9272 - val_mae: 412.9272\n",
      "Epoch 107/145\n",
      " - 2s - loss: 366.8831 - mae: 366.8830 - val_loss: 413.2416 - val_mae: 413.2416\n",
      "Epoch 108/145\n",
      " - 2s - loss: 366.4252 - mae: 366.4253 - val_loss: 412.6795 - val_mae: 412.6796\n",
      "Epoch 109/145\n",
      " - 2s - loss: 366.4684 - mae: 366.4684 - val_loss: 413.6961 - val_mae: 413.6960\n",
      "Epoch 110/145\n",
      " - 2s - loss: 366.0125 - mae: 366.0125 - val_loss: 413.9400 - val_mae: 413.9400\n",
      "Epoch 111/145\n",
      " - 2s - loss: 365.9465 - mae: 365.9465 - val_loss: 413.9764 - val_mae: 413.9764\n",
      "Epoch 112/145\n",
      " - 2s - loss: 366.5333 - mae: 366.5332 - val_loss: 414.2494 - val_mae: 414.2495\n",
      "Epoch 113/145\n",
      " - 2s - loss: 365.3278 - mae: 365.3278 - val_loss: 412.9080 - val_mae: 412.9080\n",
      "Epoch 114/145\n",
      " - 2s - loss: 365.7213 - mae: 365.7213 - val_loss: 412.6743 - val_mae: 412.6743\n",
      "Epoch 115/145\n",
      " - 2s - loss: 365.4034 - mae: 365.4033 - val_loss: 412.4387 - val_mae: 412.4387\n",
      "Epoch 116/145\n",
      " - 2s - loss: 364.8676 - mae: 364.8675 - val_loss: 413.8464 - val_mae: 413.8464\n",
      "Epoch 117/145\n",
      " - 2s - loss: 365.3850 - mae: 365.3850 - val_loss: 415.1015 - val_mae: 415.1016\n",
      "Epoch 118/145\n",
      " - 2s - loss: 365.1429 - mae: 365.1429 - val_loss: 412.6854 - val_mae: 412.6854\n",
      "Epoch 119/145\n",
      " - 2s - loss: 365.6923 - mae: 365.6923 - val_loss: 412.8942 - val_mae: 412.8942\n",
      "Epoch 120/145\n",
      " - 2s - loss: 366.0385 - mae: 366.0385 - val_loss: 413.8205 - val_mae: 413.8205\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      " - 3s - loss: 363.7289 - mae: 363.7290 - val_loss: 413.7156 - val_mae: 413.7156\n",
      "Epoch 122/145\n",
      " - 2s - loss: 363.0703 - mae: 363.0703 - val_loss: 413.3835 - val_mae: 413.3835\n",
      "Epoch 123/145\n",
      " - 2s - loss: 363.0919 - mae: 363.0918 - val_loss: 412.7541 - val_mae: 412.7541\n",
      "Epoch 124/145\n",
      " - 2s - loss: 363.1225 - mae: 363.1224 - val_loss: 412.8442 - val_mae: 412.8442\n",
      "Epoch 125/145\n",
      " - 2s - loss: 362.5937 - mae: 362.5937 - val_loss: 412.1548 - val_mae: 412.1548\n",
      "Epoch 126/145\n",
      " - 2s - loss: 362.3268 - mae: 362.3268 - val_loss: 412.6737 - val_mae: 412.6737\n",
      "Epoch 127/145\n",
      " - 2s - loss: 362.5392 - mae: 362.5392 - val_loss: 412.6938 - val_mae: 412.6938\n",
      "Epoch 128/145\n",
      " - 2s - loss: 362.0498 - mae: 362.0498 - val_loss: 412.0464 - val_mae: 412.0464\n",
      "Epoch 129/145\n",
      " - 2s - loss: 362.4073 - mae: 362.4073 - val_loss: 412.2879 - val_mae: 412.2880\n",
      "Epoch 130/145\n",
      " - 2s - loss: 362.0182 - mae: 362.0182 - val_loss: 412.7000 - val_mae: 412.7000\n",
      "Epoch 131/145\n",
      " - 2s - loss: 362.0947 - mae: 362.0946 - val_loss: 413.4148 - val_mae: 413.4148\n",
      "Epoch 132/145\n",
      " - 2s - loss: 361.7220 - mae: 361.7220 - val_loss: 412.9413 - val_mae: 412.9413\n",
      "Epoch 133/145\n",
      " - 2s - loss: 361.6455 - mae: 361.6455 - val_loss: 412.4804 - val_mae: 412.4803\n",
      "Epoch 134/145\n",
      " - 2s - loss: 361.1621 - mae: 361.1621 - val_loss: 412.5244 - val_mae: 412.5244\n",
      "Epoch 135/145\n",
      " - 2s - loss: 361.5514 - mae: 361.5514 - val_loss: 411.9572 - val_mae: 411.9572\n",
      "Epoch 136/145\n",
      " - 2s - loss: 362.0095 - mae: 362.0096 - val_loss: 411.9530 - val_mae: 411.9531\n",
      "Epoch 137/145\n",
      " - 2s - loss: 361.3019 - mae: 361.3018 - val_loss: 411.9142 - val_mae: 411.9141\n",
      "Epoch 138/145\n",
      " - 2s - loss: 361.3806 - mae: 361.3806 - val_loss: 413.2261 - val_mae: 413.2262\n",
      "Epoch 139/145\n",
      " - 2s - loss: 361.0068 - mae: 361.0068 - val_loss: 412.4427 - val_mae: 412.4427\n",
      "Epoch 140/145\n",
      " - 2s - loss: 361.2610 - mae: 361.2610 - val_loss: 412.4782 - val_mae: 412.4782\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      " - 2s - loss: 360.0087 - mae: 360.0087 - val_loss: 411.8438 - val_mae: 411.8438\n",
      "Epoch 142/145\n",
      " - 2s - loss: 360.2255 - mae: 360.2255 - val_loss: 411.8707 - val_mae: 411.8707\n",
      "Epoch 143/145\n",
      " - 2s - loss: 359.6316 - mae: 359.6315 - val_loss: 411.7964 - val_mae: 411.7964\n",
      "Epoch 144/145\n",
      " - 2s - loss: 359.7036 - mae: 359.7035 - val_loss: 412.4466 - val_mae: 412.4466\n",
      "Epoch 145/145\n",
      " - 2s - loss: 359.7380 - mae: 359.7379 - val_loss: 411.8674 - val_mae: 411.8674\n",
      "\n",
      "val_mae is:411.86739140586855\n",
      "\n",
      "fold: 1\n",
      "Train on 125000 samples, validate on 25000 samples\n",
      "Epoch 1/145\n",
      " - 2s - loss: 2527.2984 - mae: 2527.2981 - val_loss: 926.2834 - val_mae: 926.2834\n",
      "Epoch 2/145\n",
      " - 2s - loss: 814.8420 - mae: 814.8421 - val_loss: 791.8558 - val_mae: 791.8558\n",
      "Epoch 3/145\n",
      " - 2s - loss: 683.1326 - mae: 683.1326 - val_loss: 627.6906 - val_mae: 627.6906\n",
      "Epoch 4/145\n",
      " - 2s - loss: 628.3515 - mae: 628.3514 - val_loss: 628.4757 - val_mae: 628.4756\n",
      "Epoch 5/145\n",
      " - 2s - loss: 590.8406 - mae: 590.8406 - val_loss: 552.1078 - val_mae: 552.1078\n",
      "Epoch 6/145\n",
      " - 2s - loss: 563.5356 - mae: 563.5355 - val_loss: 524.7022 - val_mae: 524.7023\n",
      "Epoch 7/145\n",
      " - 2s - loss: 610.1710 - mae: 610.1711 - val_loss: 625.7949 - val_mae: 625.7950\n",
      "Epoch 8/145\n",
      " - 2s - loss: 534.0727 - mae: 534.0727 - val_loss: 536.2343 - val_mae: 536.2343\n",
      "Epoch 9/145\n",
      " - 2s - loss: 532.9558 - mae: 532.9558 - val_loss: 530.0054 - val_mae: 530.0053\n",
      "Epoch 10/145\n",
      " - 2s - loss: 577.7277 - mae: 577.7277 - val_loss: 562.0848 - val_mae: 562.0847\n",
      "Epoch 11/145\n",
      " - 2s - loss: 572.6534 - mae: 572.6534 - val_loss: 586.6082 - val_mae: 586.6082\n",
      "Epoch 12/145\n",
      " - 2s - loss: 505.9955 - mae: 505.9955 - val_loss: 508.8530 - val_mae: 508.8530\n",
      "Epoch 13/145\n",
      " - 2s - loss: 485.1077 - mae: 485.1078 - val_loss: 472.8571 - val_mae: 472.8570\n",
      "Epoch 14/145\n",
      " - 2s - loss: 486.3456 - mae: 486.3456 - val_loss: 475.3829 - val_mae: 475.3829\n",
      "Epoch 15/145\n",
      " - 2s - loss: 499.1582 - mae: 499.1582 - val_loss: 482.4034 - val_mae: 482.4033\n",
      "Epoch 16/145\n",
      " - 2s - loss: 481.6916 - mae: 481.6915 - val_loss: 467.9291 - val_mae: 467.9292\n",
      "Epoch 17/145\n",
      " - 2s - loss: 500.4071 - mae: 500.4072 - val_loss: 537.0998 - val_mae: 537.0998\n",
      "Epoch 18/145\n",
      " - 2s - loss: 516.4934 - mae: 516.4933 - val_loss: 464.6500 - val_mae: 464.6500\n",
      "Epoch 19/145\n",
      " - 2s - loss: 468.1425 - mae: 468.1424 - val_loss: 468.3590 - val_mae: 468.3591\n",
      "Epoch 20/145\n",
      " - 2s - loss: 473.0770 - mae: 473.0771 - val_loss: 465.1276 - val_mae: 465.1276\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      " - 2s - loss: 443.1307 - mae: 443.1307 - val_loss: 445.4883 - val_mae: 445.4883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/145\n",
      " - 2s - loss: 447.0935 - mae: 447.0934 - val_loss: 460.5277 - val_mae: 460.5278\n",
      "Epoch 23/145\n",
      " - 2s - loss: 443.3994 - mae: 443.3995 - val_loss: 439.3816 - val_mae: 439.3817\n",
      "Epoch 24/145\n",
      " - 2s - loss: 440.5118 - mae: 440.5117 - val_loss: 441.7517 - val_mae: 441.7517\n",
      "Epoch 25/145\n",
      " - 2s - loss: 443.6152 - mae: 443.6152 - val_loss: 443.5074 - val_mae: 443.5073\n",
      "Epoch 26/145\n",
      " - 2s - loss: 435.7277 - mae: 435.7278 - val_loss: 435.6730 - val_mae: 435.6730\n",
      "Epoch 27/145\n",
      " - 2s - loss: 442.3390 - mae: 442.3391 - val_loss: 456.7089 - val_mae: 456.7088\n",
      "Epoch 28/145\n",
      " - 2s - loss: 448.0997 - mae: 448.0996 - val_loss: 447.1020 - val_mae: 447.1020\n",
      "Epoch 29/145\n",
      " - 2s - loss: 438.7324 - mae: 438.7323 - val_loss: 449.1345 - val_mae: 449.1346\n",
      "Epoch 30/145\n",
      " - 2s - loss: 469.5106 - mae: 469.5106 - val_loss: 456.5953 - val_mae: 456.5953\n",
      "Epoch 31/145\n",
      " - 2s - loss: 448.0275 - mae: 448.0275 - val_loss: 435.0923 - val_mae: 435.0923\n",
      "Epoch 32/145\n",
      " - 2s - loss: 432.6475 - mae: 432.6475 - val_loss: 446.0690 - val_mae: 446.0690\n",
      "Epoch 33/145\n",
      " - 2s - loss: 439.0991 - mae: 439.0991 - val_loss: 466.9704 - val_mae: 466.9704\n",
      "Epoch 34/145\n",
      " - 2s - loss: 433.1621 - mae: 433.1621 - val_loss: 466.9625 - val_mae: 466.9625\n",
      "Epoch 35/145\n",
      " - 2s - loss: 459.0643 - mae: 459.0641 - val_loss: 464.5574 - val_mae: 464.5574\n",
      "Epoch 36/145\n",
      " - 2s - loss: 433.1506 - mae: 433.1507 - val_loss: 435.6149 - val_mae: 435.6149\n",
      "Epoch 37/145\n",
      " - 2s - loss: 428.2950 - mae: 428.2950 - val_loss: 441.4882 - val_mae: 441.4882\n",
      "Epoch 38/145\n",
      " - 2s - loss: 435.7209 - mae: 435.7209 - val_loss: 446.9760 - val_mae: 446.9760\n",
      "Epoch 39/145\n",
      " - 2s - loss: 432.4053 - mae: 432.4053 - val_loss: 448.4950 - val_mae: 448.4951\n",
      "Epoch 40/145\n",
      " - 2s - loss: 431.0598 - mae: 431.0598 - val_loss: 433.0165 - val_mae: 433.0165\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      " - 2s - loss: 413.8278 - mae: 413.8279 - val_loss: 429.2540 - val_mae: 429.2541\n",
      "Epoch 42/145\n",
      " - 2s - loss: 412.5765 - mae: 412.5764 - val_loss: 424.5640 - val_mae: 424.5640\n",
      "Epoch 43/145\n",
      " - 2s - loss: 411.4025 - mae: 411.4026 - val_loss: 424.0958 - val_mae: 424.0958\n",
      "Epoch 44/145\n",
      " - 2s - loss: 411.3167 - mae: 411.3168 - val_loss: 425.8479 - val_mae: 425.8478\n",
      "Epoch 45/145\n",
      " - 2s - loss: 410.6776 - mae: 410.6776 - val_loss: 425.8514 - val_mae: 425.8514\n",
      "Epoch 46/145\n",
      " - 2s - loss: 408.8606 - mae: 408.8605 - val_loss: 433.7421 - val_mae: 433.7420\n",
      "Epoch 47/145\n",
      " - 2s - loss: 418.4230 - mae: 418.4230 - val_loss: 486.6381 - val_mae: 486.6381\n",
      "Epoch 48/145\n",
      " - 2s - loss: 427.7040 - mae: 427.7040 - val_loss: 428.8019 - val_mae: 428.8019\n",
      "Epoch 49/145\n",
      " - 2s - loss: 409.0197 - mae: 409.0197 - val_loss: 423.6282 - val_mae: 423.6282\n",
      "Epoch 50/145\n",
      " - 2s - loss: 408.2612 - mae: 408.2613 - val_loss: 427.3495 - val_mae: 427.3495\n",
      "Epoch 51/145\n",
      " - 2s - loss: 407.1228 - mae: 407.1228 - val_loss: 422.6056 - val_mae: 422.6056\n",
      "Epoch 52/145\n",
      " - 2s - loss: 407.4290 - mae: 407.4289 - val_loss: 429.0847 - val_mae: 429.0846\n",
      "Epoch 53/145\n",
      " - 2s - loss: 404.7699 - mae: 404.7700 - val_loss: 421.5062 - val_mae: 421.5062\n",
      "Epoch 54/145\n",
      " - 2s - loss: 411.1686 - mae: 411.1686 - val_loss: 433.6456 - val_mae: 433.6456\n",
      "Epoch 55/145\n",
      " - 2s - loss: 415.0507 - mae: 415.0507 - val_loss: 422.9376 - val_mae: 422.9377\n",
      "Epoch 56/145\n",
      " - 2s - loss: 405.4151 - mae: 405.4151 - val_loss: 424.8092 - val_mae: 424.8092\n",
      "Epoch 57/145\n",
      " - 2s - loss: 406.8164 - mae: 406.8164 - val_loss: 431.6441 - val_mae: 431.6441\n",
      "Epoch 58/145\n",
      " - 2s - loss: 403.6594 - mae: 403.6595 - val_loss: 418.8868 - val_mae: 418.8868\n",
      "Epoch 59/145\n",
      " - 2s - loss: 402.8782 - mae: 402.8781 - val_loss: 436.5779 - val_mae: 436.5780\n",
      "Epoch 60/145\n",
      " - 2s - loss: 404.1565 - mae: 404.1566 - val_loss: 424.6294 - val_mae: 424.6293\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      " - 2s - loss: 396.1760 - mae: 396.1761 - val_loss: 419.2160 - val_mae: 419.2160\n",
      "Epoch 62/145\n",
      " - 2s - loss: 392.6414 - mae: 392.6414 - val_loss: 423.2839 - val_mae: 423.2840\n",
      "Epoch 63/145\n",
      " - 2s - loss: 393.6704 - mae: 393.6704 - val_loss: 417.3225 - val_mae: 417.3226\n",
      "Epoch 64/145\n",
      " - 2s - loss: 393.7496 - mae: 393.7497 - val_loss: 419.2384 - val_mae: 419.2384\n",
      "Epoch 65/145\n",
      " - 2s - loss: 393.6663 - mae: 393.6663 - val_loss: 420.2354 - val_mae: 420.2354\n",
      "Epoch 66/145\n",
      " - 2s - loss: 399.2092 - mae: 399.2092 - val_loss: 425.4613 - val_mae: 425.4612\n",
      "Epoch 67/145\n",
      " - 2s - loss: 393.3598 - mae: 393.3598 - val_loss: 417.8788 - val_mae: 417.8788\n",
      "Epoch 68/145\n",
      " - 2s - loss: 391.7585 - mae: 391.7587 - val_loss: 422.4510 - val_mae: 422.4510\n",
      "Epoch 69/145\n",
      " - 2s - loss: 393.2811 - mae: 393.2811 - val_loss: 422.7231 - val_mae: 422.7231\n",
      "Epoch 70/145\n",
      " - 2s - loss: 390.7345 - mae: 390.7346 - val_loss: 418.0346 - val_mae: 418.0346\n",
      "Epoch 71/145\n",
      " - 2s - loss: 390.9414 - mae: 390.9414 - val_loss: 421.6871 - val_mae: 421.6870\n",
      "Epoch 72/145\n",
      " - 2s - loss: 397.4975 - mae: 397.4975 - val_loss: 431.3361 - val_mae: 431.3360\n",
      "Epoch 73/145\n",
      " - 2s - loss: 391.3957 - mae: 391.3957 - val_loss: 418.4243 - val_mae: 418.4243\n",
      "Epoch 74/145\n",
      " - 2s - loss: 388.9919 - mae: 388.9919 - val_loss: 414.9582 - val_mae: 414.9581\n",
      "Epoch 75/145\n",
      " - 2s - loss: 388.0219 - mae: 388.0218 - val_loss: 416.5478 - val_mae: 416.5479\n",
      "Epoch 76/145\n",
      " - 2s - loss: 388.5809 - mae: 388.5809 - val_loss: 422.1185 - val_mae: 422.1185\n",
      "Epoch 77/145\n",
      " - 2s - loss: 390.5832 - mae: 390.5831 - val_loss: 415.1970 - val_mae: 415.1971\n",
      "Epoch 78/145\n",
      " - 2s - loss: 389.2380 - mae: 389.2380 - val_loss: 417.0611 - val_mae: 417.0611\n",
      "Epoch 79/145\n",
      " - 2s - loss: 387.4631 - mae: 387.4632 - val_loss: 414.5983 - val_mae: 414.5982\n",
      "Epoch 80/145\n",
      " - 2s - loss: 386.4029 - mae: 386.4029 - val_loss: 415.2027 - val_mae: 415.2026\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      " - 2s - loss: 382.7811 - mae: 382.7812 - val_loss: 415.8445 - val_mae: 415.8445\n",
      "Epoch 82/145\n",
      " - 2s - loss: 382.3453 - mae: 382.3452 - val_loss: 411.9885 - val_mae: 411.9885\n",
      "Epoch 83/145\n",
      " - 2s - loss: 381.2047 - mae: 381.2046 - val_loss: 412.5152 - val_mae: 412.5152\n",
      "Epoch 84/145\n",
      " - 2s - loss: 380.6689 - mae: 380.6689 - val_loss: 411.5684 - val_mae: 411.5685\n",
      "Epoch 85/145\n",
      " - 2s - loss: 380.2555 - mae: 380.2556 - val_loss: 410.6643 - val_mae: 410.6644\n",
      "Epoch 86/145\n",
      " - 2s - loss: 381.3120 - mae: 381.3120 - val_loss: 412.6414 - val_mae: 412.6414\n",
      "Epoch 87/145\n",
      " - 2s - loss: 381.5563 - mae: 381.5562 - val_loss: 413.4645 - val_mae: 413.4645\n",
      "Epoch 88/145\n",
      " - 2s - loss: 380.8136 - mae: 380.8137 - val_loss: 417.7322 - val_mae: 417.7321\n",
      "Epoch 89/145\n",
      " - 2s - loss: 380.1081 - mae: 380.1082 - val_loss: 412.0946 - val_mae: 412.0947\n",
      "Epoch 90/145\n",
      " - 2s - loss: 380.3728 - mae: 380.3729 - val_loss: 414.8365 - val_mae: 414.8365\n",
      "Epoch 91/145\n",
      " - 2s - loss: 379.6112 - mae: 379.6112 - val_loss: 412.6446 - val_mae: 412.6446\n",
      "Epoch 92/145\n",
      " - 2s - loss: 378.9277 - mae: 378.9277 - val_loss: 413.2248 - val_mae: 413.2247\n",
      "Epoch 93/145\n",
      " - 2s - loss: 378.4760 - mae: 378.4760 - val_loss: 414.4930 - val_mae: 414.4930\n",
      "Epoch 94/145\n",
      " - 2s - loss: 379.2348 - mae: 379.2349 - val_loss: 412.7323 - val_mae: 412.7322\n",
      "Epoch 95/145\n",
      " - 2s - loss: 378.6601 - mae: 378.6600 - val_loss: 413.4936 - val_mae: 413.4937\n",
      "Epoch 96/145\n",
      " - 2s - loss: 377.9176 - mae: 377.9176 - val_loss: 412.0372 - val_mae: 412.0372\n",
      "Epoch 97/145\n",
      " - 2s - loss: 376.7294 - mae: 376.7295 - val_loss: 411.0736 - val_mae: 411.0736\n",
      "Epoch 98/145\n",
      " - 2s - loss: 376.8090 - mae: 376.8089 - val_loss: 411.4632 - val_mae: 411.4632\n",
      "Epoch 99/145\n",
      " - 2s - loss: 376.0589 - mae: 376.0589 - val_loss: 410.9092 - val_mae: 410.9092\n",
      "Epoch 100/145\n",
      " - 2s - loss: 377.4554 - mae: 377.4555 - val_loss: 412.4321 - val_mae: 412.4321\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      " - 2s - loss: 373.5268 - mae: 373.5268 - val_loss: 411.8040 - val_mae: 411.8040\n",
      "Epoch 102/145\n",
      " - 2s - loss: 373.8485 - mae: 373.8485 - val_loss: 410.5210 - val_mae: 410.5210\n",
      "Epoch 103/145\n",
      " - 2s - loss: 373.7599 - mae: 373.7599 - val_loss: 410.2390 - val_mae: 410.2389\n",
      "Epoch 104/145\n",
      " - 2s - loss: 372.5635 - mae: 372.5635 - val_loss: 409.3986 - val_mae: 409.3987\n",
      "Epoch 105/145\n",
      " - 2s - loss: 372.3796 - mae: 372.3796 - val_loss: 410.8090 - val_mae: 410.8091\n",
      "Epoch 106/145\n",
      " - 2s - loss: 373.5814 - mae: 373.5815 - val_loss: 409.4886 - val_mae: 409.4886\n",
      "Epoch 107/145\n",
      " - 2s - loss: 372.8792 - mae: 372.8791 - val_loss: 411.0160 - val_mae: 411.0161\n",
      "Epoch 108/145\n",
      " - 2s - loss: 372.1070 - mae: 372.1070 - val_loss: 410.4104 - val_mae: 410.4104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/145\n",
      " - 2s - loss: 371.7241 - mae: 371.7240 - val_loss: 410.8354 - val_mae: 410.8354\n",
      "Epoch 110/145\n",
      " - 2s - loss: 371.7239 - mae: 371.7239 - val_loss: 409.6559 - val_mae: 409.6559\n",
      "Epoch 111/145\n",
      " - 2s - loss: 371.6427 - mae: 371.6428 - val_loss: 409.1714 - val_mae: 409.1714\n",
      "Epoch 112/145\n",
      " - 2s - loss: 371.5793 - mae: 371.5793 - val_loss: 412.6486 - val_mae: 412.6486\n",
      "Epoch 113/145\n",
      " - 2s - loss: 371.4274 - mae: 371.4274 - val_loss: 412.5255 - val_mae: 412.5255\n",
      "Epoch 114/145\n",
      " - 2s - loss: 371.1470 - mae: 371.1470 - val_loss: 411.3296 - val_mae: 411.3297\n",
      "Epoch 115/145\n",
      " - 2s - loss: 370.8207 - mae: 370.8206 - val_loss: 409.6770 - val_mae: 409.6770\n",
      "Epoch 116/145\n",
      " - 2s - loss: 371.2231 - mae: 371.2231 - val_loss: 410.3326 - val_mae: 410.3326\n",
      "Epoch 117/145\n",
      " - 2s - loss: 370.8570 - mae: 370.8568 - val_loss: 410.4394 - val_mae: 410.4394\n",
      "Epoch 118/145\n",
      " - 2s - loss: 369.9770 - mae: 369.9770 - val_loss: 412.4129 - val_mae: 412.4128\n",
      "Epoch 119/145\n",
      " - 2s - loss: 370.3593 - mae: 370.3593 - val_loss: 411.9048 - val_mae: 411.9048\n",
      "Epoch 120/145\n",
      " - 2s - loss: 370.7865 - mae: 370.7865 - val_loss: 414.0177 - val_mae: 414.0177\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      " - 2s - loss: 369.1744 - mae: 369.1744 - val_loss: 409.8554 - val_mae: 409.8555\n",
      "Epoch 122/145\n",
      " - 2s - loss: 368.1508 - mae: 368.1507 - val_loss: 409.3869 - val_mae: 409.3869\n",
      "Epoch 123/145\n",
      " - 2s - loss: 367.8817 - mae: 367.8818 - val_loss: 409.9016 - val_mae: 409.9016\n",
      "Epoch 124/145\n",
      " - 2s - loss: 367.8571 - mae: 367.8571 - val_loss: 411.1968 - val_mae: 411.1968\n",
      "Epoch 125/145\n",
      " - 2s - loss: 367.6146 - mae: 367.6146 - val_loss: 410.0370 - val_mae: 410.0370\n",
      "Epoch 126/145\n",
      " - 2s - loss: 367.3103 - mae: 367.3103 - val_loss: 409.4567 - val_mae: 409.4566\n",
      "Epoch 127/145\n",
      " - 2s - loss: 367.6301 - mae: 367.6302 - val_loss: 409.2791 - val_mae: 409.2791\n",
      "Epoch 128/145\n",
      " - 2s - loss: 367.3077 - mae: 367.3077 - val_loss: 410.7549 - val_mae: 410.7549\n",
      "Epoch 129/145\n",
      " - 2s - loss: 366.9070 - mae: 366.9070 - val_loss: 410.4298 - val_mae: 410.4297\n",
      "Epoch 130/145\n",
      " - 2s - loss: 366.8641 - mae: 366.8641 - val_loss: 409.2922 - val_mae: 409.2921\n",
      "Epoch 131/145\n",
      " - 2s - loss: 366.7522 - mae: 366.7523 - val_loss: 409.6091 - val_mae: 409.6091\n",
      "Epoch 132/145\n",
      " - 2s - loss: 366.5621 - mae: 366.5622 - val_loss: 409.5197 - val_mae: 409.5197\n",
      "Epoch 133/145\n",
      " - 2s - loss: 367.0463 - mae: 367.0463 - val_loss: 411.7271 - val_mae: 411.7271\n",
      "Epoch 134/145\n",
      " - 2s - loss: 366.4567 - mae: 366.4567 - val_loss: 409.8023 - val_mae: 409.8023\n",
      "Epoch 135/145\n",
      " - 2s - loss: 366.5648 - mae: 366.5648 - val_loss: 409.1423 - val_mae: 409.1424\n",
      "Epoch 136/145\n",
      " - 2s - loss: 366.3815 - mae: 366.3814 - val_loss: 408.9772 - val_mae: 408.9772\n",
      "Epoch 137/145\n",
      " - 2s - loss: 366.6731 - mae: 366.6731 - val_loss: 409.7276 - val_mae: 409.7276\n",
      "Epoch 138/145\n",
      " - 2s - loss: 366.3443 - mae: 366.3443 - val_loss: 410.6500 - val_mae: 410.6500\n",
      "Epoch 139/145\n",
      " - 2s - loss: 365.8425 - mae: 365.8425 - val_loss: 409.9379 - val_mae: 409.9379\n",
      "Epoch 140/145\n",
      " - 2s - loss: 365.7093 - mae: 365.7094 - val_loss: 409.6760 - val_mae: 409.6760\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      " - 2s - loss: 365.1103 - mae: 365.1103 - val_loss: 409.6670 - val_mae: 409.6670\n",
      "Epoch 142/145\n",
      " - 2s - loss: 364.5294 - mae: 364.5294 - val_loss: 409.2639 - val_mae: 409.2639\n",
      "Epoch 143/145\n",
      " - 2s - loss: 364.4799 - mae: 364.4800 - val_loss: 409.0117 - val_mae: 409.0117\n",
      "Epoch 144/145\n",
      " - 2s - loss: 364.5355 - mae: 364.5355 - val_loss: 410.0089 - val_mae: 410.0089\n",
      "Epoch 145/145\n",
      " - 2s - loss: 364.4725 - mae: 364.4725 - val_loss: 409.3545 - val_mae: 409.3544\n",
      "\n",
      "val_mae is:409.35449205989835\n",
      "\n",
      "fold: 2\n",
      "Train on 125000 samples, validate on 25000 samples\n",
      "Epoch 1/145\n",
      " - 2s - loss: 2459.6667 - mae: 2459.6663 - val_loss: 1076.7426 - val_mae: 1076.7427\n",
      "Epoch 2/145\n",
      " - 2s - loss: 808.8678 - mae: 808.8678 - val_loss: 769.0058 - val_mae: 769.0057\n",
      "Epoch 3/145\n",
      " - 2s - loss: 750.6553 - mae: 750.6552 - val_loss: 841.6348 - val_mae: 841.6348\n",
      "Epoch 4/145\n",
      " - 2s - loss: 665.5489 - mae: 665.5490 - val_loss: 745.9715 - val_mae: 745.9716\n",
      "Epoch 5/145\n",
      " - 2s - loss: 612.4914 - mae: 612.4915 - val_loss: 601.2955 - val_mae: 601.2955\n",
      "Epoch 6/145\n",
      " - 2s - loss: 580.6792 - mae: 580.6793 - val_loss: 589.1031 - val_mae: 589.1030\n",
      "Epoch 7/145\n",
      " - 2s - loss: 570.8928 - mae: 570.8928 - val_loss: 526.5382 - val_mae: 526.5383\n",
      "Epoch 8/145\n",
      " - 2s - loss: 563.2257 - mae: 563.2256 - val_loss: 528.9813 - val_mae: 528.9813\n",
      "Epoch 9/145\n",
      " - 2s - loss: 515.8749 - mae: 515.8749 - val_loss: 492.3795 - val_mae: 492.3795\n",
      "Epoch 10/145\n",
      " - 2s - loss: 530.6687 - mae: 530.6686 - val_loss: 564.4338 - val_mae: 564.4338\n",
      "Epoch 11/145\n",
      " - 2s - loss: 558.3446 - mae: 558.3447 - val_loss: 496.9922 - val_mae: 496.9922\n",
      "Epoch 12/145\n",
      " - 2s - loss: 533.8029 - mae: 533.8029 - val_loss: 490.6875 - val_mae: 490.6875\n",
      "Epoch 13/145\n",
      " - 2s - loss: 499.0670 - mae: 499.0671 - val_loss: 562.1621 - val_mae: 562.1621\n",
      "Epoch 14/145\n",
      " - 2s - loss: 489.8616 - mae: 489.8616 - val_loss: 480.9486 - val_mae: 480.9485\n",
      "Epoch 15/145\n",
      " - 2s - loss: 484.5034 - mae: 484.5034 - val_loss: 530.5087 - val_mae: 530.5087\n",
      "Epoch 16/145\n",
      " - 2s - loss: 486.7963 - mae: 486.7962 - val_loss: 477.2605 - val_mae: 477.2605\n",
      "Epoch 17/145\n",
      " - 2s - loss: 510.8940 - mae: 510.8940 - val_loss: 482.9060 - val_mae: 482.9060\n",
      "Epoch 18/145\n",
      " - 2s - loss: 496.7931 - mae: 496.7931 - val_loss: 467.3354 - val_mae: 467.3354\n",
      "Epoch 19/145\n",
      " - 2s - loss: 473.4685 - mae: 473.4685 - val_loss: 486.2939 - val_mae: 486.2939\n",
      "Epoch 20/145\n",
      " - 2s - loss: 470.5225 - mae: 470.5225 - val_loss: 506.8220 - val_mae: 506.8221\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      " - 2s - loss: 450.7217 - mae: 450.7216 - val_loss: 450.4372 - val_mae: 450.4372\n",
      "Epoch 22/145\n",
      " - 2s - loss: 465.7957 - mae: 465.7957 - val_loss: 473.7083 - val_mae: 473.7083\n",
      "Epoch 23/145\n",
      " - 2s - loss: 465.6387 - mae: 465.6386 - val_loss: 450.3404 - val_mae: 450.3404\n",
      "Epoch 24/145\n",
      " - 2s - loss: 439.8675 - mae: 439.8675 - val_loss: 451.3137 - val_mae: 451.3137\n",
      "Epoch 25/145\n",
      " - 2s - loss: 448.1939 - mae: 448.1940 - val_loss: 504.2812 - val_mae: 504.2812\n",
      "Epoch 26/145\n",
      " - 2s - loss: 479.6617 - mae: 479.6616 - val_loss: 468.9464 - val_mae: 468.9464\n",
      "Epoch 27/145\n",
      " - 2s - loss: 463.3283 - mae: 463.3283 - val_loss: 448.7907 - val_mae: 448.7906\n",
      "Epoch 28/145\n",
      " - 2s - loss: 440.4352 - mae: 440.4352 - val_loss: 451.6915 - val_mae: 451.6916\n",
      "Epoch 29/145\n",
      " - 2s - loss: 432.3881 - mae: 432.3880 - val_loss: 447.0179 - val_mae: 447.0179\n",
      "Epoch 30/145\n",
      " - 2s - loss: 431.1654 - mae: 431.1653 - val_loss: 458.4540 - val_mae: 458.4539\n",
      "Epoch 31/145\n",
      " - 2s - loss: 436.9428 - mae: 436.9428 - val_loss: 444.8971 - val_mae: 444.8971\n",
      "Epoch 32/145\n",
      " - 2s - loss: 432.3529 - mae: 432.3529 - val_loss: 449.4697 - val_mae: 449.4697\n",
      "Epoch 33/145\n",
      " - 2s - loss: 429.2469 - mae: 429.2469 - val_loss: 450.1631 - val_mae: 450.1631\n",
      "Epoch 34/145\n",
      " - 2s - loss: 432.3432 - mae: 432.3432 - val_loss: 462.7821 - val_mae: 462.7820\n",
      "Epoch 35/145\n",
      " - 2s - loss: 430.5633 - mae: 430.5633 - val_loss: 444.3551 - val_mae: 444.3551\n",
      "Epoch 36/145\n",
      " - 2s - loss: 435.3581 - mae: 435.3581 - val_loss: 472.5752 - val_mae: 472.5753\n",
      "Epoch 37/145\n",
      " - 2s - loss: 433.1642 - mae: 433.1643 - val_loss: 444.8823 - val_mae: 444.8823\n",
      "Epoch 38/145\n",
      " - 2s - loss: 433.9181 - mae: 433.9182 - val_loss: 456.3941 - val_mae: 456.3941\n",
      "Epoch 39/145\n",
      " - 2s - loss: 427.5825 - mae: 427.5825 - val_loss: 440.2954 - val_mae: 440.2954\n",
      "Epoch 40/145\n",
      " - 2s - loss: 430.7040 - mae: 430.7041 - val_loss: 440.4429 - val_mae: 440.4430\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      " - 2s - loss: 411.9352 - mae: 411.9352 - val_loss: 432.8983 - val_mae: 432.8983\n",
      "Epoch 42/145\n",
      " - 2s - loss: 410.7615 - mae: 410.7615 - val_loss: 437.6384 - val_mae: 437.6384\n",
      "Epoch 43/145\n",
      " - 2s - loss: 409.8051 - mae: 409.8051 - val_loss: 431.4977 - val_mae: 431.4977\n",
      "Epoch 44/145\n",
      " - 2s - loss: 407.7764 - mae: 407.7764 - val_loss: 437.4453 - val_mae: 437.4453\n",
      "Epoch 45/145\n",
      " - 2s - loss: 414.4378 - mae: 414.4378 - val_loss: 445.8830 - val_mae: 445.8830\n",
      "Epoch 46/145\n",
      " - 2s - loss: 411.0725 - mae: 411.0725 - val_loss: 432.9197 - val_mae: 432.9196\n",
      "Epoch 47/145\n",
      " - 2s - loss: 416.7779 - mae: 416.7779 - val_loss: 464.5163 - val_mae: 464.5162\n",
      "Epoch 48/145\n",
      " - 2s - loss: 417.3696 - mae: 417.3696 - val_loss: 436.8832 - val_mae: 436.8831\n",
      "Epoch 49/145\n",
      " - 2s - loss: 405.3914 - mae: 405.3914 - val_loss: 429.6464 - val_mae: 429.6464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/145\n",
      " - 2s - loss: 407.7019 - mae: 407.7019 - val_loss: 472.5515 - val_mae: 472.5515\n",
      "Epoch 51/145\n",
      " - 2s - loss: 415.0025 - mae: 415.0025 - val_loss: 430.0688 - val_mae: 430.0688\n",
      "Epoch 52/145\n",
      " - 2s - loss: 404.9827 - mae: 404.9826 - val_loss: 434.9214 - val_mae: 434.9214\n",
      "Epoch 53/145\n",
      " - 2s - loss: 404.8511 - mae: 404.8510 - val_loss: 436.8258 - val_mae: 436.8258\n",
      "Epoch 54/145\n",
      " - 2s - loss: 403.3484 - mae: 403.3484 - val_loss: 431.6231 - val_mae: 431.6231\n",
      "Epoch 55/145\n",
      " - 2s - loss: 402.0684 - mae: 402.0684 - val_loss: 430.7159 - val_mae: 430.7159\n",
      "Epoch 56/145\n",
      " - 2s - loss: 403.2676 - mae: 403.2676 - val_loss: 436.2518 - val_mae: 436.2518\n",
      "Epoch 57/145\n",
      " - 2s - loss: 404.0948 - mae: 404.0948 - val_loss: 438.6568 - val_mae: 438.6568\n",
      "Epoch 58/145\n",
      " - 2s - loss: 401.0609 - mae: 401.0609 - val_loss: 425.1663 - val_mae: 425.1664\n",
      "Epoch 59/145\n",
      " - 2s - loss: 405.2765 - mae: 405.2766 - val_loss: 445.1706 - val_mae: 445.1707\n",
      "Epoch 60/145\n",
      " - 2s - loss: 400.0802 - mae: 400.0802 - val_loss: 427.0861 - val_mae: 427.0861\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      " - 2s - loss: 392.0323 - mae: 392.0323 - val_loss: 421.9695 - val_mae: 421.9695\n",
      "Epoch 62/145\n",
      " - 2s - loss: 392.4524 - mae: 392.4524 - val_loss: 424.8895 - val_mae: 424.8895\n",
      "Epoch 63/145\n",
      " - 2s - loss: 388.6975 - mae: 388.6974 - val_loss: 423.6710 - val_mae: 423.6710\n",
      "Epoch 64/145\n",
      " - 2s - loss: 389.4669 - mae: 389.4670 - val_loss: 433.5321 - val_mae: 433.5321\n",
      "Epoch 65/145\n",
      " - 2s - loss: 389.2455 - mae: 389.2455 - val_loss: 423.8941 - val_mae: 423.8941\n",
      "Epoch 66/145\n",
      " - 2s - loss: 387.1056 - mae: 387.1055 - val_loss: 422.3642 - val_mae: 422.3642\n",
      "Epoch 67/145\n",
      " - 2s - loss: 386.8435 - mae: 386.8436 - val_loss: 421.0351 - val_mae: 421.0351\n",
      "Epoch 68/145\n",
      " - 2s - loss: 387.1209 - mae: 387.1209 - val_loss: 422.9406 - val_mae: 422.9406\n",
      "Epoch 69/145\n",
      " - 2s - loss: 388.0836 - mae: 388.0836 - val_loss: 426.6489 - val_mae: 426.6489\n",
      "Epoch 70/145\n",
      " - 2s - loss: 388.4042 - mae: 388.4041 - val_loss: 428.6756 - val_mae: 428.6756\n",
      "Epoch 71/145\n",
      " - 2s - loss: 391.4648 - mae: 391.4647 - val_loss: 422.2687 - val_mae: 422.2687\n",
      "Epoch 72/145\n",
      " - 2s - loss: 399.0738 - mae: 399.0738 - val_loss: 423.0620 - val_mae: 423.0620\n",
      "Epoch 73/145\n",
      " - 2s - loss: 386.0902 - mae: 386.0903 - val_loss: 426.1496 - val_mae: 426.1495\n",
      "Epoch 74/145\n",
      " - 2s - loss: 385.3511 - mae: 385.3512 - val_loss: 425.0131 - val_mae: 425.0131\n",
      "Epoch 75/145\n",
      " - 2s - loss: 384.3811 - mae: 384.3810 - val_loss: 422.2617 - val_mae: 422.2617\n",
      "Epoch 76/145\n",
      " - 2s - loss: 384.0630 - mae: 384.0630 - val_loss: 420.4010 - val_mae: 420.4010\n",
      "Epoch 77/145\n",
      " - 2s - loss: 383.7071 - mae: 383.7071 - val_loss: 425.9672 - val_mae: 425.9673\n",
      "Epoch 78/145\n",
      " - 2s - loss: 383.2317 - mae: 383.2316 - val_loss: 427.2753 - val_mae: 427.2753\n",
      "Epoch 79/145\n",
      " - 2s - loss: 384.5134 - mae: 384.5135 - val_loss: 421.5460 - val_mae: 421.5461\n",
      "Epoch 80/145\n",
      " - 2s - loss: 381.6205 - mae: 381.6205 - val_loss: 425.6882 - val_mae: 425.6882\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      " - 2s - loss: 378.2743 - mae: 378.2743 - val_loss: 420.2308 - val_mae: 420.2309\n",
      "Epoch 82/145\n",
      " - 2s - loss: 378.2098 - mae: 378.2097 - val_loss: 419.8553 - val_mae: 419.8553\n",
      "Epoch 83/145\n",
      " - 2s - loss: 376.7450 - mae: 376.7451 - val_loss: 418.1164 - val_mae: 418.1164\n",
      "Epoch 84/145\n",
      " - 2s - loss: 376.8581 - mae: 376.8580 - val_loss: 417.2028 - val_mae: 417.2029\n",
      "Epoch 85/145\n",
      " - 2s - loss: 375.6297 - mae: 375.6297 - val_loss: 418.4847 - val_mae: 418.4846\n",
      "Epoch 86/145\n",
      " - 2s - loss: 375.1423 - mae: 375.1423 - val_loss: 417.0599 - val_mae: 417.0598\n",
      "Epoch 87/145\n",
      " - 2s - loss: 375.0996 - mae: 375.0996 - val_loss: 419.3476 - val_mae: 419.3476\n",
      "Epoch 88/145\n",
      " - 2s - loss: 375.1385 - mae: 375.1384 - val_loss: 418.5951 - val_mae: 418.5951\n",
      "Epoch 89/145\n",
      " - 2s - loss: 374.4938 - mae: 374.4938 - val_loss: 421.6186 - val_mae: 421.6186\n",
      "Epoch 90/145\n",
      " - 2s - loss: 374.1873 - mae: 374.1874 - val_loss: 419.9279 - val_mae: 419.9279\n",
      "Epoch 91/145\n",
      " - 2s - loss: 373.3426 - mae: 373.3427 - val_loss: 418.1135 - val_mae: 418.1134\n",
      "Epoch 92/145\n",
      " - 2s - loss: 373.8296 - mae: 373.8296 - val_loss: 419.8916 - val_mae: 419.8916\n",
      "Epoch 93/145\n",
      " - 2s - loss: 372.9674 - mae: 372.9675 - val_loss: 419.0984 - val_mae: 419.0984\n",
      "Epoch 94/145\n",
      " - 2s - loss: 373.2533 - mae: 373.2532 - val_loss: 418.2483 - val_mae: 418.2482\n",
      "Epoch 95/145\n",
      " - 2s - loss: 373.1472 - mae: 373.1472 - val_loss: 416.7494 - val_mae: 416.7495\n",
      "Epoch 96/145\n",
      " - 2s - loss: 374.8406 - mae: 374.8406 - val_loss: 416.9210 - val_mae: 416.9210\n",
      "Epoch 97/145\n",
      " - 2s - loss: 374.4167 - mae: 374.4167 - val_loss: 418.6543 - val_mae: 418.6542\n",
      "Epoch 98/145\n",
      " - 2s - loss: 373.4525 - mae: 373.4525 - val_loss: 417.1310 - val_mae: 417.1310\n",
      "Epoch 99/145\n",
      " - 2s - loss: 371.7062 - mae: 371.7062 - val_loss: 420.7159 - val_mae: 420.7159\n",
      "Epoch 100/145\n",
      " - 2s - loss: 372.2165 - mae: 372.2165 - val_loss: 418.4368 - val_mae: 418.4368\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      " - 2s - loss: 368.4970 - mae: 368.4971 - val_loss: 416.2283 - val_mae: 416.2284\n",
      "Epoch 102/145\n",
      " - 2s - loss: 368.0822 - mae: 368.0822 - val_loss: 416.4338 - val_mae: 416.4338\n",
      "Epoch 103/145\n",
      " - 2s - loss: 367.7598 - mae: 367.7598 - val_loss: 415.6362 - val_mae: 415.6362\n",
      "Epoch 104/145\n",
      " - 2s - loss: 367.4848 - mae: 367.4848 - val_loss: 418.6293 - val_mae: 418.6294\n",
      "Epoch 105/145\n",
      " - 2s - loss: 367.2603 - mae: 367.2603 - val_loss: 415.9821 - val_mae: 415.9821\n",
      "Epoch 106/145\n",
      " - 2s - loss: 367.8324 - mae: 367.8323 - val_loss: 416.6344 - val_mae: 416.6344\n",
      "Epoch 107/145\n",
      " - 2s - loss: 367.5593 - mae: 367.5593 - val_loss: 417.8648 - val_mae: 417.8647\n",
      "Epoch 108/145\n",
      " - 2s - loss: 366.6270 - mae: 366.6270 - val_loss: 416.3545 - val_mae: 416.3546\n",
      "Epoch 109/145\n",
      " - 2s - loss: 366.7198 - mae: 366.7198 - val_loss: 415.7354 - val_mae: 415.7354\n",
      "Epoch 110/145\n",
      " - 2s - loss: 365.7994 - mae: 365.7994 - val_loss: 417.7110 - val_mae: 417.7110\n",
      "Epoch 111/145\n",
      " - 2s - loss: 366.7867 - mae: 366.7866 - val_loss: 415.5426 - val_mae: 415.5426\n",
      "Epoch 112/145\n",
      " - 2s - loss: 365.3987 - mae: 365.3987 - val_loss: 415.6895 - val_mae: 415.6896\n",
      "Epoch 113/145\n",
      " - 2s - loss: 365.2525 - mae: 365.2525 - val_loss: 416.1225 - val_mae: 416.1224\n",
      "Epoch 114/145\n",
      " - 2s - loss: 366.5719 - mae: 366.5719 - val_loss: 415.2400 - val_mae: 415.2400\n",
      "Epoch 115/145\n",
      " - 2s - loss: 365.6182 - mae: 365.6182 - val_loss: 415.3501 - val_mae: 415.3501\n",
      "Epoch 116/145\n",
      " - 2s - loss: 365.3205 - mae: 365.3206 - val_loss: 417.0852 - val_mae: 417.0852\n",
      "Epoch 117/145\n",
      " - 2s - loss: 365.4066 - mae: 365.4065 - val_loss: 417.0180 - val_mae: 417.0180\n",
      "Epoch 118/145\n",
      " - 2s - loss: 364.9505 - mae: 364.9504 - val_loss: 416.1793 - val_mae: 416.1793\n",
      "Epoch 119/145\n",
      " - 2s - loss: 365.6542 - mae: 365.6541 - val_loss: 421.2506 - val_mae: 421.2506\n",
      "Epoch 120/145\n",
      " - 2s - loss: 365.7578 - mae: 365.7579 - val_loss: 415.9673 - val_mae: 415.9673\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      " - 2s - loss: 362.3220 - mae: 362.3220 - val_loss: 415.5838 - val_mae: 415.5838\n",
      "Epoch 122/145\n",
      " - 2s - loss: 361.6271 - mae: 361.6271 - val_loss: 415.2529 - val_mae: 415.2529\n",
      "Epoch 123/145\n",
      " - 2s - loss: 362.4053 - mae: 362.4053 - val_loss: 416.6781 - val_mae: 416.6781\n",
      "Epoch 124/145\n",
      " - 2s - loss: 362.0999 - mae: 362.0999 - val_loss: 416.8954 - val_mae: 416.8954\n",
      "Epoch 125/145\n",
      " - 2s - loss: 361.7827 - mae: 361.7827 - val_loss: 414.8652 - val_mae: 414.8652\n",
      "Epoch 126/145\n",
      " - 2s - loss: 361.4867 - mae: 361.4867 - val_loss: 415.5305 - val_mae: 415.5305\n",
      "Epoch 127/145\n",
      " - 2s - loss: 361.8974 - mae: 361.8974 - val_loss: 416.4293 - val_mae: 416.4293\n",
      "Epoch 128/145\n",
      " - 2s - loss: 360.7292 - mae: 360.7292 - val_loss: 415.3621 - val_mae: 415.3621\n",
      "Epoch 129/145\n",
      " - 2s - loss: 360.9184 - mae: 360.9184 - val_loss: 415.4991 - val_mae: 415.4991\n",
      "Epoch 130/145\n",
      " - 2s - loss: 360.9145 - mae: 360.9146 - val_loss: 417.9079 - val_mae: 417.9079\n",
      "Epoch 131/145\n",
      " - 2s - loss: 361.2374 - mae: 361.2374 - val_loss: 414.9871 - val_mae: 414.9871\n",
      "Epoch 132/145\n",
      " - 2s - loss: 360.4260 - mae: 360.4260 - val_loss: 416.6423 - val_mae: 416.6423\n",
      "Epoch 133/145\n",
      " - 2s - loss: 360.5630 - mae: 360.5630 - val_loss: 414.6221 - val_mae: 414.6220\n",
      "Epoch 134/145\n",
      " - 2s - loss: 360.0704 - mae: 360.0704 - val_loss: 414.6824 - val_mae: 414.6824\n",
      "Epoch 135/145\n",
      " - 2s - loss: 359.6872 - mae: 359.6872 - val_loss: 414.9998 - val_mae: 414.9998\n",
      "Epoch 136/145\n",
      " - 2s - loss: 359.7999 - mae: 359.7999 - val_loss: 416.6138 - val_mae: 416.6138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/145\n",
      " - 2s - loss: 359.9695 - mae: 359.9695 - val_loss: 417.3195 - val_mae: 417.3194\n",
      "Epoch 138/145\n",
      " - 2s - loss: 360.3234 - mae: 360.3234 - val_loss: 415.0403 - val_mae: 415.0403\n",
      "Epoch 139/145\n",
      " - 2s - loss: 359.1211 - mae: 359.1212 - val_loss: 415.4134 - val_mae: 415.4135\n",
      "Epoch 140/145\n",
      " - 2s - loss: 358.7639 - mae: 358.7639 - val_loss: 414.9116 - val_mae: 414.9116\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      " - 2s - loss: 358.1596 - mae: 358.1596 - val_loss: 414.9217 - val_mae: 414.9217\n",
      "Epoch 142/145\n",
      " - 2s - loss: 357.7752 - mae: 357.7753 - val_loss: 415.1181 - val_mae: 415.1180\n",
      "Epoch 143/145\n",
      " - 2s - loss: 357.5750 - mae: 357.5750 - val_loss: 414.3006 - val_mae: 414.3006\n",
      "Epoch 144/145\n",
      " - 2s - loss: 357.5714 - mae: 357.5714 - val_loss: 414.8384 - val_mae: 414.8384\n",
      "Epoch 145/145\n",
      " - 2s - loss: 357.5117 - mae: 357.5117 - val_loss: 416.1987 - val_mae: 416.1987\n",
      "\n",
      "val_mae is:416.1987109146881\n",
      "\n",
      "fold: 3\n",
      "Train on 125000 samples, validate on 25000 samples\n",
      "Epoch 1/145\n",
      " - 2s - loss: 2846.1933 - mae: 2846.1936 - val_loss: 1011.9769 - val_mae: 1011.9769\n",
      "Epoch 2/145\n",
      " - 2s - loss: 868.2788 - mae: 868.2788 - val_loss: 794.1817 - val_mae: 794.1817\n",
      "Epoch 3/145\n",
      " - 2s - loss: 754.7176 - mae: 754.7173 - val_loss: 708.6858 - val_mae: 708.6857\n",
      "Epoch 4/145\n",
      " - 2s - loss: 622.7126 - mae: 622.7126 - val_loss: 571.7863 - val_mae: 571.7863\n",
      "Epoch 5/145\n",
      " - 2s - loss: 575.9889 - mae: 575.9888 - val_loss: 553.2502 - val_mae: 553.2502\n",
      "Epoch 6/145\n",
      " - 2s - loss: 560.7595 - mae: 560.7594 - val_loss: 532.9635 - val_mae: 532.9636\n",
      "Epoch 7/145\n",
      " - 2s - loss: 552.9740 - mae: 552.9741 - val_loss: 534.9147 - val_mae: 534.9147\n",
      "Epoch 8/145\n",
      " - 2s - loss: 533.1106 - mae: 533.1106 - val_loss: 518.5722 - val_mae: 518.5721\n",
      "Epoch 9/145\n",
      " - 2s - loss: 526.0912 - mae: 526.0912 - val_loss: 541.1246 - val_mae: 541.1245\n",
      "Epoch 10/145\n",
      " - 2s - loss: 539.5247 - mae: 539.5248 - val_loss: 509.8942 - val_mae: 509.8942\n",
      "Epoch 11/145\n",
      " - 2s - loss: 509.4246 - mae: 509.4247 - val_loss: 590.7681 - val_mae: 590.7681\n",
      "Epoch 12/145\n",
      " - 2s - loss: 572.8640 - mae: 572.8641 - val_loss: 525.9747 - val_mae: 525.9747\n",
      "Epoch 13/145\n",
      " - 2s - loss: 500.5183 - mae: 500.5182 - val_loss: 476.1000 - val_mae: 476.0999\n",
      "Epoch 14/145\n",
      " - 2s - loss: 485.5897 - mae: 485.5896 - val_loss: 531.2286 - val_mae: 531.2286\n",
      "Epoch 15/145\n",
      " - 2s - loss: 494.8856 - mae: 494.8856 - val_loss: 474.9899 - val_mae: 474.9898\n",
      "Epoch 16/145\n",
      " - 2s - loss: 488.9747 - mae: 488.9747 - val_loss: 474.5855 - val_mae: 474.5855\n",
      "Epoch 17/145\n",
      " - 2s - loss: 485.4476 - mae: 485.4476 - val_loss: 495.8906 - val_mae: 495.8906\n",
      "Epoch 18/145\n",
      " - 2s - loss: 501.7217 - mae: 501.7216 - val_loss: 518.0188 - val_mae: 518.0188\n",
      "Epoch 19/145\n",
      " - 2s - loss: 497.9467 - mae: 497.9467 - val_loss: 476.6169 - val_mae: 476.6170\n",
      "Epoch 20/145\n",
      " - 2s - loss: 468.4431 - mae: 468.4431 - val_loss: 467.9805 - val_mae: 467.9804\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      " - 2s - loss: 456.9589 - mae: 456.9590 - val_loss: 457.3625 - val_mae: 457.3625\n",
      "Epoch 22/145\n",
      " - 2s - loss: 442.7934 - mae: 442.7934 - val_loss: 445.3870 - val_mae: 445.3870\n",
      "Epoch 23/145\n",
      " - 2s - loss: 440.0468 - mae: 440.0468 - val_loss: 447.6314 - val_mae: 447.6313\n",
      "Epoch 24/145\n",
      " - 2s - loss: 443.0400 - mae: 443.0400 - val_loss: 441.7832 - val_mae: 441.7832\n",
      "Epoch 25/145\n",
      " - 2s - loss: 437.0847 - mae: 437.0846 - val_loss: 457.9729 - val_mae: 457.9729\n",
      "Epoch 26/145\n",
      " - 2s - loss: 441.1192 - mae: 441.1191 - val_loss: 460.8726 - val_mae: 460.8727\n",
      "Epoch 27/145\n",
      " - 2s - loss: 437.5173 - mae: 437.5172 - val_loss: 451.2480 - val_mae: 451.2480\n",
      "Epoch 28/145\n",
      " - 2s - loss: 431.7969 - mae: 431.7968 - val_loss: 438.0644 - val_mae: 438.0644\n",
      "Epoch 29/145\n",
      " - 2s - loss: 431.6368 - mae: 431.6367 - val_loss: 504.4934 - val_mae: 504.4934\n",
      "Epoch 30/145\n",
      " - 2s - loss: 440.0665 - mae: 440.0665 - val_loss: 447.4762 - val_mae: 447.4762\n",
      "Epoch 31/145\n",
      " - 2s - loss: 430.8503 - mae: 430.8504 - val_loss: 434.0931 - val_mae: 434.0932\n",
      "Epoch 32/145\n",
      " - 2s - loss: 427.1265 - mae: 427.1265 - val_loss: 450.9233 - val_mae: 450.9233\n",
      "Epoch 33/145\n",
      " - 2s - loss: 428.3780 - mae: 428.3781 - val_loss: 454.7547 - val_mae: 454.7547\n",
      "Epoch 34/145\n",
      " - 2s - loss: 444.0605 - mae: 444.0605 - val_loss: 460.6219 - val_mae: 460.6219\n",
      "Epoch 35/145\n",
      " - 2s - loss: 424.5852 - mae: 424.5851 - val_loss: 441.8233 - val_mae: 441.8233\n",
      "Epoch 36/145\n",
      " - 2s - loss: 427.0761 - mae: 427.0761 - val_loss: 438.0474 - val_mae: 438.0475\n",
      "Epoch 37/145\n",
      " - 2s - loss: 428.6838 - mae: 428.6838 - val_loss: 429.6872 - val_mae: 429.6872\n",
      "Epoch 38/145\n",
      " - 2s - loss: 450.1560 - mae: 450.1561 - val_loss: 452.7474 - val_mae: 452.7474\n",
      "Epoch 39/145\n",
      " - 2s - loss: 428.7314 - mae: 428.7314 - val_loss: 437.6447 - val_mae: 437.6447\n",
      "Epoch 40/145\n",
      " - 2s - loss: 417.9240 - mae: 417.9241 - val_loss: 431.2178 - val_mae: 431.2178\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      " - 2s - loss: 408.1543 - mae: 408.1543 - val_loss: 425.3534 - val_mae: 425.3534\n",
      "Epoch 42/145\n",
      " - 2s - loss: 405.6489 - mae: 405.6490 - val_loss: 427.1112 - val_mae: 427.1111\n",
      "Epoch 43/145\n",
      " - 2s - loss: 403.9845 - mae: 403.9845 - val_loss: 422.4410 - val_mae: 422.4410\n",
      "Epoch 44/145\n",
      " - 2s - loss: 407.4960 - mae: 407.4960 - val_loss: 427.4403 - val_mae: 427.4403\n",
      "Epoch 45/145\n",
      " - 2s - loss: 402.4974 - mae: 402.4974 - val_loss: 422.9819 - val_mae: 422.9819\n",
      "Epoch 46/145\n",
      " - 2s - loss: 401.9164 - mae: 401.9164 - val_loss: 421.6424 - val_mae: 421.6424\n",
      "Epoch 47/145\n",
      " - 2s - loss: 401.7231 - mae: 401.7231 - val_loss: 428.7087 - val_mae: 428.7087\n",
      "Epoch 48/145\n",
      " - 2s - loss: 401.6784 - mae: 401.6785 - val_loss: 438.3173 - val_mae: 438.3174\n",
      "Epoch 49/145\n",
      " - 2s - loss: 408.7720 - mae: 408.7721 - val_loss: 473.5131 - val_mae: 473.5131\n",
      "Epoch 50/145\n",
      " - 2s - loss: 402.2295 - mae: 402.2294 - val_loss: 422.9750 - val_mae: 422.9750\n",
      "Epoch 51/145\n",
      " - 2s - loss: 398.3876 - mae: 398.3875 - val_loss: 427.8935 - val_mae: 427.8936\n",
      "Epoch 52/145\n",
      " - 2s - loss: 399.9067 - mae: 399.9067 - val_loss: 433.2211 - val_mae: 433.2211\n",
      "Epoch 53/145\n",
      " - 2s - loss: 399.5535 - mae: 399.5536 - val_loss: 422.8414 - val_mae: 422.8414\n",
      "Epoch 54/145\n",
      " - 2s - loss: 398.8759 - mae: 398.8760 - val_loss: 422.8226 - val_mae: 422.8226\n",
      "Epoch 55/145\n",
      " - 2s - loss: 398.8211 - mae: 398.8212 - val_loss: 428.4764 - val_mae: 428.4764\n",
      "Epoch 56/145\n",
      " - 2s - loss: 397.0604 - mae: 397.0604 - val_loss: 423.9679 - val_mae: 423.9680\n",
      "Epoch 57/145\n",
      " - 2s - loss: 399.6487 - mae: 399.6487 - val_loss: 443.8881 - val_mae: 443.8881\n",
      "Epoch 58/145\n",
      " - 2s - loss: 412.3794 - mae: 412.3794 - val_loss: 443.9068 - val_mae: 443.9068\n",
      "Epoch 59/145\n",
      " - 2s - loss: 400.3522 - mae: 400.3521 - val_loss: 428.3094 - val_mae: 428.3094\n",
      "Epoch 60/145\n",
      " - 2s - loss: 392.4543 - mae: 392.4543 - val_loss: 419.8395 - val_mae: 419.8395\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      " - 2s - loss: 387.9502 - mae: 387.9502 - val_loss: 420.0967 - val_mae: 420.0967\n",
      "Epoch 62/145\n",
      " - 2s - loss: 389.7345 - mae: 389.7345 - val_loss: 417.8739 - val_mae: 417.8739\n",
      "Epoch 63/145\n",
      " - 2s - loss: 385.6110 - mae: 385.6111 - val_loss: 419.5302 - val_mae: 419.5302\n",
      "Epoch 64/145\n",
      " - 2s - loss: 385.2411 - mae: 385.2411 - val_loss: 417.9183 - val_mae: 417.9183\n",
      "Epoch 65/145\n",
      " - 2s - loss: 384.9571 - mae: 384.9572 - val_loss: 417.7323 - val_mae: 417.7323\n",
      "Epoch 66/145\n",
      " - 2s - loss: 384.1004 - mae: 384.1004 - val_loss: 417.6155 - val_mae: 417.6154\n",
      "Epoch 67/145\n",
      " - 2s - loss: 383.7558 - mae: 383.7557 - val_loss: 418.9312 - val_mae: 418.9312\n",
      "Epoch 68/145\n",
      " - 2s - loss: 383.5607 - mae: 383.5607 - val_loss: 420.2676 - val_mae: 420.2676\n",
      "Epoch 69/145\n",
      " - 2s - loss: 384.9196 - mae: 384.9196 - val_loss: 419.2933 - val_mae: 419.2934\n",
      "Epoch 70/145\n",
      " - 2s - loss: 382.8517 - mae: 382.8518 - val_loss: 417.4886 - val_mae: 417.4886\n",
      "Epoch 71/145\n",
      " - 2s - loss: 388.6068 - mae: 388.6069 - val_loss: 422.5435 - val_mae: 422.5435\n",
      "Epoch 72/145\n",
      " - 2s - loss: 381.4941 - mae: 381.4941 - val_loss: 419.6129 - val_mae: 419.6129\n",
      "Epoch 73/145\n",
      " - 2s - loss: 381.9092 - mae: 381.9092 - val_loss: 420.5866 - val_mae: 420.5866\n",
      "Epoch 74/145\n",
      " - 2s - loss: 383.2285 - mae: 383.2285 - val_loss: 420.3145 - val_mae: 420.3145\n",
      "Epoch 75/145\n",
      " - 2s - loss: 381.6259 - mae: 381.6258 - val_loss: 419.3518 - val_mae: 419.3517\n",
      "Epoch 76/145\n",
      " - 2s - loss: 382.0069 - mae: 382.0068 - val_loss: 421.7268 - val_mae: 421.7268\n",
      "Epoch 77/145\n",
      " - 2s - loss: 380.8100 - mae: 380.8100 - val_loss: 420.9979 - val_mae: 420.9979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/145\n",
      " - 2s - loss: 381.5580 - mae: 381.5580 - val_loss: 419.4184 - val_mae: 419.4184\n",
      "Epoch 79/145\n",
      " - 2s - loss: 379.3259 - mae: 379.3260 - val_loss: 417.3739 - val_mae: 417.3739\n",
      "Epoch 80/145\n",
      " - 2s - loss: 380.6265 - mae: 380.6266 - val_loss: 418.5114 - val_mae: 418.5113\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      " - 2s - loss: 375.4766 - mae: 375.4766 - val_loss: 417.8200 - val_mae: 417.8199\n",
      "Epoch 82/145\n",
      " - 2s - loss: 374.5808 - mae: 374.5808 - val_loss: 416.8601 - val_mae: 416.8602\n",
      "Epoch 83/145\n",
      " - 2s - loss: 374.3008 - mae: 374.3009 - val_loss: 416.2990 - val_mae: 416.2991\n",
      "Epoch 84/145\n",
      " - 2s - loss: 373.9483 - mae: 373.9482 - val_loss: 419.9259 - val_mae: 419.9259\n",
      "Epoch 85/145\n",
      " - 2s - loss: 374.6487 - mae: 374.6488 - val_loss: 416.9067 - val_mae: 416.9067\n",
      "Epoch 86/145\n",
      " - 2s - loss: 373.9052 - mae: 373.9052 - val_loss: 417.4309 - val_mae: 417.4309\n",
      "Epoch 87/145\n",
      " - 2s - loss: 373.8739 - mae: 373.8740 - val_loss: 419.1764 - val_mae: 419.1765\n",
      "Epoch 88/145\n",
      " - 2s - loss: 373.6712 - mae: 373.6711 - val_loss: 416.9735 - val_mae: 416.9735\n",
      "Epoch 89/145\n",
      " - 2s - loss: 374.1292 - mae: 374.1293 - val_loss: 419.7742 - val_mae: 419.7742\n",
      "Epoch 90/145\n",
      " - 2s - loss: 373.8123 - mae: 373.8123 - val_loss: 418.9513 - val_mae: 418.9513\n",
      "Epoch 91/145\n",
      " - 2s - loss: 373.3387 - mae: 373.3388 - val_loss: 416.7329 - val_mae: 416.7328\n",
      "Epoch 92/145\n",
      " - 2s - loss: 372.5829 - mae: 372.5829 - val_loss: 416.2247 - val_mae: 416.2247\n",
      "Epoch 93/145\n",
      " - 2s - loss: 372.1875 - mae: 372.1876 - val_loss: 419.6252 - val_mae: 419.6252\n",
      "Epoch 94/145\n",
      " - 2s - loss: 371.5143 - mae: 371.5144 - val_loss: 416.2735 - val_mae: 416.2735\n",
      "Epoch 95/145\n",
      " - 2s - loss: 371.4893 - mae: 371.4892 - val_loss: 415.7929 - val_mae: 415.7928\n",
      "Epoch 96/145\n",
      " - 2s - loss: 372.0325 - mae: 372.0325 - val_loss: 417.6577 - val_mae: 417.6577\n",
      "Epoch 97/145\n",
      " - 2s - loss: 372.1573 - mae: 372.1573 - val_loss: 423.0773 - val_mae: 423.0774\n",
      "Epoch 98/145\n",
      " - 2s - loss: 373.3955 - mae: 373.3955 - val_loss: 416.5073 - val_mae: 416.5073\n",
      "Epoch 99/145\n",
      " - 2s - loss: 370.9588 - mae: 370.9589 - val_loss: 415.2444 - val_mae: 415.2444\n",
      "Epoch 100/145\n",
      " - 2s - loss: 370.2324 - mae: 370.2324 - val_loss: 416.1287 - val_mae: 416.1288\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      " - 2s - loss: 367.9047 - mae: 367.9047 - val_loss: 415.5980 - val_mae: 415.5981\n",
      "Epoch 102/145\n",
      " - 2s - loss: 367.3806 - mae: 367.3806 - val_loss: 416.0790 - val_mae: 416.0790\n",
      "Epoch 103/145\n",
      " - 2s - loss: 367.0266 - mae: 367.0266 - val_loss: 414.9546 - val_mae: 414.9546\n",
      "Epoch 104/145\n",
      " - 2s - loss: 367.3692 - mae: 367.3693 - val_loss: 415.2529 - val_mae: 415.2529\n",
      "Epoch 105/145\n",
      " - 2s - loss: 367.3271 - mae: 367.3271 - val_loss: 415.5511 - val_mae: 415.5511\n",
      "Epoch 106/145\n",
      " - 2s - loss: 367.1019 - mae: 367.1019 - val_loss: 415.0827 - val_mae: 415.0826\n",
      "Epoch 107/145\n",
      " - 2s - loss: 366.7571 - mae: 366.7571 - val_loss: 415.7628 - val_mae: 415.7628\n",
      "Epoch 108/145\n",
      " - 2s - loss: 367.3556 - mae: 367.3555 - val_loss: 416.1592 - val_mae: 416.1592\n",
      "Epoch 109/145\n",
      " - 2s - loss: 365.9863 - mae: 365.9864 - val_loss: 416.0429 - val_mae: 416.0429\n",
      "Epoch 110/145\n",
      " - 2s - loss: 366.1038 - mae: 366.1038 - val_loss: 416.4054 - val_mae: 416.4054\n",
      "Epoch 111/145\n",
      " - 2s - loss: 365.9281 - mae: 365.9281 - val_loss: 418.7328 - val_mae: 418.7328\n",
      "Epoch 112/145\n",
      " - 2s - loss: 366.2709 - mae: 366.2708 - val_loss: 415.7114 - val_mae: 415.7114\n",
      "Epoch 113/145\n",
      " - 2s - loss: 365.5451 - mae: 365.5451 - val_loss: 415.0127 - val_mae: 415.0127\n",
      "Epoch 114/145\n",
      " - 2s - loss: 364.8655 - mae: 364.8655 - val_loss: 416.9113 - val_mae: 416.9113\n",
      "Epoch 115/145\n",
      " - 2s - loss: 365.2577 - mae: 365.2577 - val_loss: 418.7859 - val_mae: 418.7858\n",
      "Epoch 116/145\n",
      " - 2s - loss: 365.6295 - mae: 365.6295 - val_loss: 415.7438 - val_mae: 415.7438\n",
      "Epoch 117/145\n",
      " - 2s - loss: 364.6677 - mae: 364.6678 - val_loss: 415.3043 - val_mae: 415.3043\n",
      "Epoch 118/145\n",
      " - 2s - loss: 365.1441 - mae: 365.1440 - val_loss: 415.5666 - val_mae: 415.5666\n",
      "Epoch 119/145\n",
      " - 2s - loss: 364.3390 - mae: 364.3391 - val_loss: 415.5317 - val_mae: 415.5317\n",
      "Epoch 120/145\n",
      " - 2s - loss: 364.1091 - mae: 364.1092 - val_loss: 416.1126 - val_mae: 416.1126\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      " - 2s - loss: 363.2281 - mae: 363.2281 - val_loss: 415.1134 - val_mae: 415.1135\n",
      "Epoch 122/145\n",
      " - 2s - loss: 362.8123 - mae: 362.8124 - val_loss: 415.2627 - val_mae: 415.2627\n",
      "Epoch 123/145\n",
      " - 2s - loss: 363.2684 - mae: 363.2684 - val_loss: 415.5755 - val_mae: 415.5755\n",
      "Epoch 124/145\n",
      " - 2s - loss: 362.9506 - mae: 362.9506 - val_loss: 415.2591 - val_mae: 415.2591\n",
      "Epoch 125/145\n",
      " - 2s - loss: 361.9786 - mae: 361.9785 - val_loss: 415.3466 - val_mae: 415.3466\n",
      "Epoch 126/145\n",
      " - 2s - loss: 361.7925 - mae: 361.7926 - val_loss: 415.3893 - val_mae: 415.3893\n",
      "Epoch 127/145\n",
      " - 2s - loss: 361.8895 - mae: 361.8895 - val_loss: 415.3644 - val_mae: 415.3644\n",
      "Epoch 128/145\n",
      " - 2s - loss: 362.0165 - mae: 362.0166 - val_loss: 414.8458 - val_mae: 414.8458\n",
      "Epoch 129/145\n",
      " - 2s - loss: 361.7631 - mae: 361.7631 - val_loss: 415.4846 - val_mae: 415.4846\n",
      "Epoch 130/145\n",
      " - 2s - loss: 361.8844 - mae: 361.8846 - val_loss: 415.4575 - val_mae: 415.4575\n",
      "Epoch 131/145\n",
      " - 2s - loss: 361.4861 - mae: 361.4860 - val_loss: 415.3636 - val_mae: 415.3635\n",
      "Epoch 132/145\n",
      " - 2s - loss: 361.7606 - mae: 361.7606 - val_loss: 415.1443 - val_mae: 415.1442\n",
      "Epoch 133/145\n",
      " - 2s - loss: 361.6616 - mae: 361.6616 - val_loss: 415.7364 - val_mae: 415.7365\n",
      "Epoch 134/145\n",
      " - 2s - loss: 361.2575 - mae: 361.2575 - val_loss: 415.1543 - val_mae: 415.1543\n",
      "Epoch 135/145\n",
      " - 2s - loss: 361.2987 - mae: 361.2987 - val_loss: 415.4041 - val_mae: 415.4042\n",
      "Epoch 136/145\n",
      " - 2s - loss: 360.9824 - mae: 360.9824 - val_loss: 415.5533 - val_mae: 415.5533\n",
      "Epoch 137/145\n",
      " - 2s - loss: 360.6712 - mae: 360.6712 - val_loss: 415.4645 - val_mae: 415.4645\n",
      "Epoch 138/145\n",
      " - 2s - loss: 360.7382 - mae: 360.7382 - val_loss: 415.2903 - val_mae: 415.2904\n",
      "Epoch 139/145\n",
      " - 2s - loss: 360.7123 - mae: 360.7123 - val_loss: 415.5769 - val_mae: 415.5769\n",
      "Epoch 140/145\n",
      " - 2s - loss: 360.6179 - mae: 360.6179 - val_loss: 416.3121 - val_mae: 416.3121\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      " - 2s - loss: 359.8444 - mae: 359.8445 - val_loss: 414.6838 - val_mae: 414.6838\n",
      "Epoch 142/145\n",
      " - 2s - loss: 359.5067 - mae: 359.5066 - val_loss: 415.9608 - val_mae: 415.9608\n",
      "Epoch 143/145\n",
      " - 2s - loss: 359.9806 - mae: 359.9807 - val_loss: 415.0341 - val_mae: 415.0341\n",
      "Epoch 144/145\n",
      " - 2s - loss: 359.3430 - mae: 359.3430 - val_loss: 415.0100 - val_mae: 415.0100\n",
      "Epoch 145/145\n",
      " - 2s - loss: 359.2507 - mae: 359.2508 - val_loss: 415.1916 - val_mae: 415.1916\n",
      "\n",
      "val_mae is:415.19160322332976\n",
      "\n",
      "fold: 4\n",
      "Train on 125000 samples, validate on 25000 samples\n",
      "Epoch 1/145\n",
      " - 2s - loss: 2689.4489 - mae: 2689.4492 - val_loss: 953.8700 - val_mae: 953.8700\n",
      "Epoch 2/145\n",
      " - 2s - loss: 859.9154 - mae: 859.9154 - val_loss: 756.4133 - val_mae: 756.4133\n",
      "Epoch 3/145\n",
      " - 2s - loss: 714.0985 - mae: 714.0985 - val_loss: 639.7343 - val_mae: 639.7343\n",
      "Epoch 4/145\n",
      " - 2s - loss: 653.0912 - mae: 653.0911 - val_loss: 583.4428 - val_mae: 583.4428\n",
      "Epoch 5/145\n",
      " - 2s - loss: 607.3533 - mae: 607.3533 - val_loss: 796.0987 - val_mae: 796.0986\n",
      "Epoch 6/145\n",
      " - 2s - loss: 657.9451 - mae: 657.9450 - val_loss: 551.1361 - val_mae: 551.1361\n",
      "Epoch 7/145\n",
      " - 2s - loss: 548.3136 - mae: 548.3136 - val_loss: 580.4014 - val_mae: 580.4016\n",
      "Epoch 8/145\n",
      " - 2s - loss: 543.0688 - mae: 543.0688 - val_loss: 518.1078 - val_mae: 518.1078\n",
      "Epoch 9/145\n",
      " - 2s - loss: 530.1781 - mae: 530.1782 - val_loss: 509.8254 - val_mae: 509.8253\n",
      "Epoch 10/145\n",
      " - 2s - loss: 582.8067 - mae: 582.8066 - val_loss: 588.2903 - val_mae: 588.2903\n",
      "Epoch 11/145\n",
      " - 2s - loss: 541.6917 - mae: 541.6917 - val_loss: 517.7916 - val_mae: 517.7916\n",
      "Epoch 12/145\n",
      " - 2s - loss: 541.4515 - mae: 541.4515 - val_loss: 493.4574 - val_mae: 493.4574\n",
      "Epoch 13/145\n",
      " - 2s - loss: 501.6131 - mae: 501.6130 - val_loss: 482.6197 - val_mae: 482.6197\n",
      "Epoch 14/145\n",
      " - 2s - loss: 521.9260 - mae: 521.9261 - val_loss: 527.4511 - val_mae: 527.4510\n",
      "Epoch 15/145\n",
      " - 2s - loss: 501.6872 - mae: 501.6871 - val_loss: 515.2143 - val_mae: 515.2144\n",
      "Epoch 16/145\n",
      " - 2s - loss: 491.1097 - mae: 491.1096 - val_loss: 481.3777 - val_mae: 481.3777\n",
      "Epoch 17/145\n",
      " - 2s - loss: 484.9023 - mae: 484.9023 - val_loss: 519.6836 - val_mae: 519.6835\n",
      "Epoch 18/145\n",
      " - 2s - loss: 480.9802 - mae: 480.9802 - val_loss: 466.7359 - val_mae: 466.7359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/145\n",
      " - 2s - loss: 486.3210 - mae: 486.3210 - val_loss: 671.6370 - val_mae: 671.6370\n",
      "Epoch 20/145\n",
      " - 2s - loss: 508.8490 - mae: 508.8491 - val_loss: 496.9035 - val_mae: 496.9035\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      " - 2s - loss: 474.2849 - mae: 474.2849 - val_loss: 446.6661 - val_mae: 446.6660\n",
      "Epoch 22/145\n",
      " - 2s - loss: 446.3491 - mae: 446.3492 - val_loss: 448.8685 - val_mae: 448.8685\n",
      "Epoch 23/145\n",
      " - 2s - loss: 441.5642 - mae: 441.5641 - val_loss: 438.9290 - val_mae: 438.9290\n",
      "Epoch 24/145\n",
      " - 2s - loss: 441.5792 - mae: 441.5792 - val_loss: 447.0553 - val_mae: 447.0554\n",
      "Epoch 25/145\n",
      " - 2s - loss: 444.5205 - mae: 444.5205 - val_loss: 442.6793 - val_mae: 442.6793\n",
      "Epoch 26/145\n",
      " - 2s - loss: 439.3547 - mae: 439.3547 - val_loss: 443.9923 - val_mae: 443.9922\n",
      "Epoch 27/145\n",
      " - 2s - loss: 440.9442 - mae: 440.9442 - val_loss: 437.3435 - val_mae: 437.3436\n",
      "Epoch 28/145\n",
      " - 2s - loss: 436.5928 - mae: 436.5928 - val_loss: 436.6674 - val_mae: 436.6674\n",
      "Epoch 29/145\n",
      " - 2s - loss: 450.3144 - mae: 450.3144 - val_loss: 463.3211 - val_mae: 463.3210\n",
      "Epoch 30/145\n",
      " - 2s - loss: 455.6207 - mae: 455.6206 - val_loss: 444.9739 - val_mae: 444.9739\n",
      "Epoch 31/145\n",
      " - 2s - loss: 435.1675 - mae: 435.1676 - val_loss: 441.4708 - val_mae: 441.4708\n",
      "Epoch 32/145\n",
      " - 2s - loss: 458.3095 - mae: 458.3095 - val_loss: 485.6621 - val_mae: 485.6620\n",
      "Epoch 33/145\n",
      " - 2s - loss: 448.7981 - mae: 448.7981 - val_loss: 436.6631 - val_mae: 436.6631\n",
      "Epoch 34/145\n",
      " - 2s - loss: 429.8756 - mae: 429.8756 - val_loss: 443.9784 - val_mae: 443.9784\n",
      "Epoch 35/145\n",
      " - 2s - loss: 441.1680 - mae: 441.1680 - val_loss: 460.4999 - val_mae: 460.4998\n",
      "Epoch 36/145\n",
      " - 2s - loss: 435.2134 - mae: 435.2133 - val_loss: 429.0093 - val_mae: 429.0093\n",
      "Epoch 37/145\n",
      " - 2s - loss: 422.4128 - mae: 422.4129 - val_loss: 428.3365 - val_mae: 428.3365\n",
      "Epoch 38/145\n",
      " - 2s - loss: 427.0381 - mae: 427.0381 - val_loss: 505.1766 - val_mae: 505.1766\n",
      "Epoch 39/145\n",
      " - 2s - loss: 431.1221 - mae: 431.1220 - val_loss: 430.1613 - val_mae: 430.1613\n",
      "Epoch 40/145\n",
      " - 2s - loss: 422.4296 - mae: 422.4294 - val_loss: 433.8122 - val_mae: 433.8122\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      " - 2s - loss: 411.3841 - mae: 411.3842 - val_loss: 423.6140 - val_mae: 423.6140\n",
      "Epoch 42/145\n",
      " - 2s - loss: 408.7861 - mae: 408.7861 - val_loss: 427.0355 - val_mae: 427.0355\n",
      "Epoch 43/145\n",
      " - 2s - loss: 409.8650 - mae: 409.8650 - val_loss: 433.6898 - val_mae: 433.6898\n",
      "Epoch 44/145\n",
      " - 2s - loss: 422.3545 - mae: 422.3545 - val_loss: 420.9519 - val_mae: 420.9518\n",
      "Epoch 45/145\n",
      " - 2s - loss: 407.4602 - mae: 407.4603 - val_loss: 423.2407 - val_mae: 423.2407\n",
      "Epoch 46/145\n",
      " - 2s - loss: 408.6916 - mae: 408.6916 - val_loss: 418.0638 - val_mae: 418.0639\n",
      "Epoch 47/145\n",
      " - 2s - loss: 405.3367 - mae: 405.3367 - val_loss: 418.7368 - val_mae: 418.7368\n",
      "Epoch 48/145\n",
      " - 2s - loss: 405.2387 - mae: 405.2387 - val_loss: 416.4123 - val_mae: 416.4123\n",
      "Epoch 49/145\n",
      " - 2s - loss: 403.7556 - mae: 403.7556 - val_loss: 419.0500 - val_mae: 419.0500\n",
      "Epoch 50/145\n",
      " - 2s - loss: 403.5967 - mae: 403.5967 - val_loss: 416.2105 - val_mae: 416.2105\n",
      "Epoch 51/145\n",
      " - 2s - loss: 404.2473 - mae: 404.2473 - val_loss: 419.2722 - val_mae: 419.2722\n",
      "Epoch 52/145\n",
      " - 2s - loss: 405.4782 - mae: 405.4782 - val_loss: 428.3243 - val_mae: 428.3243\n",
      "Epoch 53/145\n",
      " - 2s - loss: 405.7523 - mae: 405.7523 - val_loss: 437.2964 - val_mae: 437.2964\n",
      "Epoch 54/145\n",
      " - 2s - loss: 402.2463 - mae: 402.2463 - val_loss: 417.2575 - val_mae: 417.2575\n",
      "Epoch 55/145\n",
      " - 2s - loss: 413.5688 - mae: 413.5689 - val_loss: 422.5941 - val_mae: 422.5941\n",
      "Epoch 56/145\n",
      " - 2s - loss: 408.8452 - mae: 408.8452 - val_loss: 420.8929 - val_mae: 420.8929\n",
      "Epoch 57/145\n",
      " - 2s - loss: 401.0482 - mae: 401.0482 - val_loss: 425.2326 - val_mae: 425.2326\n",
      "Epoch 58/145\n",
      " - 2s - loss: 400.5125 - mae: 400.5125 - val_loss: 418.7709 - val_mae: 418.7709\n",
      "Epoch 59/145\n",
      " - 2s - loss: 401.6180 - mae: 401.6180 - val_loss: 415.0066 - val_mae: 415.0067\n",
      "Epoch 60/145\n",
      " - 2s - loss: 397.8384 - mae: 397.8383 - val_loss: 416.9722 - val_mae: 416.9722\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      " - 2s - loss: 391.9283 - mae: 391.9283 - val_loss: 413.7183 - val_mae: 413.7184\n",
      "Epoch 62/145\n",
      " - 2s - loss: 391.1402 - mae: 391.1402 - val_loss: 412.3107 - val_mae: 412.3107\n",
      "Epoch 63/145\n",
      " - 2s - loss: 391.3441 - mae: 391.3440 - val_loss: 412.3428 - val_mae: 412.3428\n",
      "Epoch 64/145\n",
      " - 2s - loss: 390.9089 - mae: 390.9090 - val_loss: 429.3578 - val_mae: 429.3578\n",
      "Epoch 65/145\n",
      " - 2s - loss: 393.6148 - mae: 393.6147 - val_loss: 412.9782 - val_mae: 412.9782\n",
      "Epoch 66/145\n",
      " - 2s - loss: 389.8789 - mae: 389.8789 - val_loss: 412.6305 - val_mae: 412.6305\n",
      "Epoch 67/145\n",
      " - 2s - loss: 388.2017 - mae: 388.2017 - val_loss: 412.0040 - val_mae: 412.0040\n",
      "Epoch 68/145\n",
      " - 2s - loss: 389.7329 - mae: 389.7329 - val_loss: 411.4337 - val_mae: 411.4337\n",
      "Epoch 69/145\n",
      " - 2s - loss: 388.1138 - mae: 388.1139 - val_loss: 418.0529 - val_mae: 418.0529\n",
      "Epoch 70/145\n",
      " - 2s - loss: 387.9290 - mae: 387.9290 - val_loss: 412.0007 - val_mae: 412.0007\n",
      "Epoch 71/145\n",
      " - 2s - loss: 387.6395 - mae: 387.6395 - val_loss: 411.0246 - val_mae: 411.0247\n",
      "Epoch 72/145\n",
      " - 2s - loss: 386.2710 - mae: 386.2710 - val_loss: 419.9799 - val_mae: 419.9799\n",
      "Epoch 73/145\n",
      " - 2s - loss: 387.2334 - mae: 387.2334 - val_loss: 416.0840 - val_mae: 416.0840\n",
      "Epoch 74/145\n",
      " - 2s - loss: 386.8588 - mae: 386.8589 - val_loss: 413.9347 - val_mae: 413.9347\n",
      "Epoch 75/145\n",
      " - 2s - loss: 386.3072 - mae: 386.3072 - val_loss: 413.9608 - val_mae: 413.9608\n",
      "Epoch 76/145\n",
      " - 2s - loss: 386.3228 - mae: 386.3228 - val_loss: 411.4115 - val_mae: 411.4115\n",
      "Epoch 77/145\n",
      " - 2s - loss: 385.7748 - mae: 385.7747 - val_loss: 414.0725 - val_mae: 414.0725\n",
      "Epoch 78/145\n",
      " - 2s - loss: 384.5123 - mae: 384.5124 - val_loss: 410.2638 - val_mae: 410.2638\n",
      "Epoch 79/145\n",
      " - 2s - loss: 389.3621 - mae: 389.3622 - val_loss: 412.0751 - val_mae: 412.0751\n",
      "Epoch 80/145\n",
      " - 2s - loss: 383.7715 - mae: 383.7714 - val_loss: 415.7912 - val_mae: 415.7912\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      " - 2s - loss: 380.1645 - mae: 380.1646 - val_loss: 410.8984 - val_mae: 410.8984\n",
      "Epoch 82/145\n",
      " - 2s - loss: 378.9526 - mae: 378.9525 - val_loss: 409.6449 - val_mae: 409.6450\n",
      "Epoch 83/145\n",
      " - 2s - loss: 379.5625 - mae: 379.5625 - val_loss: 410.5770 - val_mae: 410.5770\n",
      "Epoch 84/145\n",
      " - 2s - loss: 377.8669 - mae: 377.8669 - val_loss: 411.2596 - val_mae: 411.2596\n",
      "Epoch 85/145\n",
      " - 2s - loss: 378.2560 - mae: 378.2560 - val_loss: 411.3396 - val_mae: 411.3396\n",
      "Epoch 86/145\n",
      " - 2s - loss: 377.7694 - mae: 377.7694 - val_loss: 408.9265 - val_mae: 408.9265\n",
      "Epoch 87/145\n",
      " - 2s - loss: 377.1287 - mae: 377.1287 - val_loss: 409.8707 - val_mae: 409.8707\n",
      "Epoch 88/145\n",
      " - 2s - loss: 378.2803 - mae: 378.2803 - val_loss: 413.0472 - val_mae: 413.0472\n",
      "Epoch 89/145\n",
      " - 2s - loss: 377.1924 - mae: 377.1924 - val_loss: 409.7759 - val_mae: 409.7759\n",
      "Epoch 90/145\n",
      " - 2s - loss: 376.9681 - mae: 376.9680 - val_loss: 409.7587 - val_mae: 409.7588\n",
      "Epoch 91/145\n",
      " - 2s - loss: 376.8455 - mae: 376.8455 - val_loss: 409.9813 - val_mae: 409.9813\n",
      "Epoch 92/145\n",
      " - 2s - loss: 377.2141 - mae: 377.2141 - val_loss: 409.6402 - val_mae: 409.6402\n",
      "Epoch 93/145\n",
      " - 2s - loss: 376.2352 - mae: 376.2353 - val_loss: 410.5975 - val_mae: 410.5974\n",
      "Epoch 94/145\n",
      " - 2s - loss: 376.6237 - mae: 376.6237 - val_loss: 415.7021 - val_mae: 415.7021\n",
      "Epoch 95/145\n",
      " - 2s - loss: 376.6790 - mae: 376.6791 - val_loss: 409.4704 - val_mae: 409.4704\n",
      "Epoch 96/145\n",
      " - 2s - loss: 378.1680 - mae: 378.1680 - val_loss: 418.5419 - val_mae: 418.5418\n",
      "Epoch 97/145\n",
      " - 2s - loss: 375.3290 - mae: 375.3291 - val_loss: 409.5259 - val_mae: 409.5259\n",
      "Epoch 98/145\n",
      " - 2s - loss: 374.6481 - mae: 374.6481 - val_loss: 409.6544 - val_mae: 409.6544\n",
      "Epoch 99/145\n",
      " - 2s - loss: 374.8823 - mae: 374.8824 - val_loss: 409.2282 - val_mae: 409.2281\n",
      "Epoch 100/145\n",
      " - 2s - loss: 373.9544 - mae: 373.9544 - val_loss: 409.4589 - val_mae: 409.4588\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      " - 2s - loss: 372.0340 - mae: 372.0340 - val_loss: 408.4071 - val_mae: 408.4072\n",
      "Epoch 102/145\n",
      " - 2s - loss: 371.3173 - mae: 371.3174 - val_loss: 409.4800 - val_mae: 409.4799\n",
      "Epoch 103/145\n",
      " - 2s - loss: 371.1143 - mae: 371.1143 - val_loss: 409.1876 - val_mae: 409.1876\n",
      "Epoch 104/145\n",
      " - 2s - loss: 371.2876 - mae: 371.2875 - val_loss: 408.8287 - val_mae: 408.8287\n",
      "Epoch 105/145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s - loss: 370.4476 - mae: 370.4475 - val_loss: 409.0104 - val_mae: 409.0104\n",
      "Epoch 106/145\n",
      " - 2s - loss: 371.5492 - mae: 371.5493 - val_loss: 408.6798 - val_mae: 408.6798\n",
      "Epoch 107/145\n",
      " - 2s - loss: 371.2585 - mae: 371.2585 - val_loss: 409.6740 - val_mae: 409.6740\n",
      "Epoch 108/145\n",
      " - 2s - loss: 370.5381 - mae: 370.5381 - val_loss: 408.9044 - val_mae: 408.9044\n",
      "Epoch 109/145\n",
      " - 2s - loss: 369.7541 - mae: 369.7541 - val_loss: 409.0427 - val_mae: 409.0427\n",
      "Epoch 110/145\n",
      " - 2s - loss: 370.2755 - mae: 370.2755 - val_loss: 408.5195 - val_mae: 408.5195\n",
      "Epoch 111/145\n",
      " - 2s - loss: 369.5392 - mae: 369.5393 - val_loss: 408.2675 - val_mae: 408.2675\n",
      "Epoch 112/145\n",
      " - 2s - loss: 369.4269 - mae: 369.4269 - val_loss: 409.1914 - val_mae: 409.1914\n",
      "Epoch 113/145\n",
      " - 2s - loss: 369.3304 - mae: 369.3304 - val_loss: 407.9892 - val_mae: 407.9892\n",
      "Epoch 114/145\n",
      " - 2s - loss: 368.7506 - mae: 368.7505 - val_loss: 408.5433 - val_mae: 408.5432\n",
      "Epoch 115/145\n",
      " - 2s - loss: 368.7279 - mae: 368.7280 - val_loss: 410.7921 - val_mae: 410.7921\n",
      "Epoch 116/145\n",
      " - 2s - loss: 369.0957 - mae: 369.0956 - val_loss: 411.0135 - val_mae: 411.0135\n",
      "Epoch 117/145\n",
      " - 2s - loss: 369.4208 - mae: 369.4208 - val_loss: 408.2566 - val_mae: 408.2565\n",
      "Epoch 118/145\n",
      " - 2s - loss: 368.4927 - mae: 368.4928 - val_loss: 411.2842 - val_mae: 411.2842\n",
      "Epoch 119/145\n",
      " - 2s - loss: 369.1932 - mae: 369.1933 - val_loss: 411.7601 - val_mae: 411.7601\n",
      "Epoch 120/145\n",
      " - 2s - loss: 368.3333 - mae: 368.3333 - val_loss: 408.9009 - val_mae: 408.9009\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      " - 2s - loss: 366.4216 - mae: 366.4216 - val_loss: 409.4916 - val_mae: 409.4916\n",
      "Epoch 122/145\n",
      " - 2s - loss: 366.2950 - mae: 366.2950 - val_loss: 409.5118 - val_mae: 409.5118\n",
      "Epoch 123/145\n",
      " - 2s - loss: 366.5641 - mae: 366.5641 - val_loss: 409.2201 - val_mae: 409.2201\n",
      "Epoch 124/145\n",
      " - 2s - loss: 366.3630 - mae: 366.3631 - val_loss: 408.6534 - val_mae: 408.6534\n",
      "Epoch 125/145\n",
      " - 2s - loss: 366.2799 - mae: 366.2798 - val_loss: 408.0790 - val_mae: 408.0789\n",
      "Epoch 126/145\n",
      " - 2s - loss: 365.6271 - mae: 365.6271 - val_loss: 408.2195 - val_mae: 408.2195\n",
      "Epoch 127/145\n",
      " - 2s - loss: 365.9291 - mae: 365.9290 - val_loss: 408.5687 - val_mae: 408.5687\n",
      "Epoch 128/145\n",
      " - 2s - loss: 365.9376 - mae: 365.9376 - val_loss: 408.4139 - val_mae: 408.4140\n",
      "Epoch 129/145\n",
      " - 2s - loss: 365.5571 - mae: 365.5571 - val_loss: 408.3704 - val_mae: 408.3704\n",
      "Epoch 130/145\n",
      " - 2s - loss: 365.0855 - mae: 365.0856 - val_loss: 408.3681 - val_mae: 408.3682\n",
      "Epoch 131/145\n",
      " - 2s - loss: 365.3515 - mae: 365.3515 - val_loss: 408.0747 - val_mae: 408.0748\n",
      "Epoch 132/145\n",
      " - 2s - loss: 364.8609 - mae: 364.8609 - val_loss: 408.3541 - val_mae: 408.3541\n",
      "Epoch 133/145\n",
      " - 2s - loss: 365.4922 - mae: 365.4922 - val_loss: 408.4745 - val_mae: 408.4745\n",
      "Epoch 134/145\n",
      " - 2s - loss: 364.6892 - mae: 364.6891 - val_loss: 408.5133 - val_mae: 408.5133\n",
      "Epoch 135/145\n",
      " - 2s - loss: 364.4927 - mae: 364.4928 - val_loss: 408.8411 - val_mae: 408.8411\n",
      "Epoch 136/145\n",
      " - 2s - loss: 364.8330 - mae: 364.8330 - val_loss: 409.2078 - val_mae: 409.2078\n",
      "Epoch 137/145\n",
      " - 2s - loss: 364.6197 - mae: 364.6197 - val_loss: 409.3870 - val_mae: 409.3871\n",
      "Epoch 138/145\n",
      " - 2s - loss: 364.3595 - mae: 364.3595 - val_loss: 409.8597 - val_mae: 409.8596\n",
      "Epoch 139/145\n",
      " - 2s - loss: 364.6718 - mae: 364.6718 - val_loss: 408.3590 - val_mae: 408.3590\n",
      "Epoch 140/145\n",
      " - 2s - loss: 364.1477 - mae: 364.1477 - val_loss: 409.0017 - val_mae: 409.0017\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      " - 2s - loss: 363.5104 - mae: 363.5105 - val_loss: 408.4062 - val_mae: 408.4062\n",
      "Epoch 142/145\n",
      " - 2s - loss: 363.0939 - mae: 363.0940 - val_loss: 408.5192 - val_mae: 408.5193\n",
      "Epoch 143/145\n",
      " - 2s - loss: 362.8925 - mae: 362.8925 - val_loss: 408.1224 - val_mae: 408.1224\n",
      "Epoch 144/145\n",
      " - 2s - loss: 362.9228 - mae: 362.9229 - val_loss: 409.0953 - val_mae: 409.0953\n",
      "Epoch 145/145\n",
      " - 2s - loss: 363.1944 - mae: 363.1944 - val_loss: 408.1483 - val_mae: 408.1483\n",
      "\n",
      "val_mae is:408.14827887924196\n",
      "\n",
      "fold: 5\n",
      "Train on 125000 samples, validate on 25000 samples\n",
      "Epoch 1/145\n",
      " - 2s - loss: 2597.1047 - mae: 2597.1055 - val_loss: 960.4126 - val_mae: 960.4125\n",
      "Epoch 2/145\n",
      " - 2s - loss: 843.1069 - mae: 843.1068 - val_loss: 738.7420 - val_mae: 738.7421\n",
      "Epoch 3/145\n",
      " - 2s - loss: 696.4864 - mae: 696.4865 - val_loss: 736.2944 - val_mae: 736.2945\n",
      "Epoch 4/145\n",
      " - 2s - loss: 682.0618 - mae: 682.0619 - val_loss: 707.6533 - val_mae: 707.6533\n",
      "Epoch 5/145\n",
      " - 2s - loss: 620.6536 - mae: 620.6536 - val_loss: 658.0936 - val_mae: 658.0938\n",
      "Epoch 6/145\n",
      " - 2s - loss: 605.2876 - mae: 605.2875 - val_loss: 657.2362 - val_mae: 657.2362\n",
      "Epoch 7/145\n",
      " - 2s - loss: 576.6242 - mae: 576.6241 - val_loss: 591.5922 - val_mae: 591.5922\n",
      "Epoch 8/145\n",
      " - 2s - loss: 576.2318 - mae: 576.2319 - val_loss: 548.2185 - val_mae: 548.2185\n",
      "Epoch 9/145\n",
      " - 2s - loss: 529.7041 - mae: 529.7042 - val_loss: 612.6083 - val_mae: 612.6083\n",
      "Epoch 10/145\n",
      " - 2s - loss: 551.8363 - mae: 551.8363 - val_loss: 545.2810 - val_mae: 545.2809\n",
      "Epoch 11/145\n",
      " - 2s - loss: 509.5142 - mae: 509.5141 - val_loss: 513.2534 - val_mae: 513.2534\n",
      "Epoch 12/145\n",
      " - 2s - loss: 505.8163 - mae: 505.8164 - val_loss: 529.3519 - val_mae: 529.3519\n",
      "Epoch 13/145\n",
      " - 2s - loss: 528.1795 - mae: 528.1794 - val_loss: 570.4130 - val_mae: 570.4130\n",
      "Epoch 14/145\n",
      " - 2s - loss: 500.3013 - mae: 500.3012 - val_loss: 547.2768 - val_mae: 547.2769\n",
      "Epoch 15/145\n",
      " - 2s - loss: 495.4771 - mae: 495.4771 - val_loss: 493.1932 - val_mae: 493.1931\n",
      "Epoch 16/145\n",
      " - 2s - loss: 485.8417 - mae: 485.8417 - val_loss: 484.9042 - val_mae: 484.9042\n",
      "Epoch 17/145\n",
      " - 2s - loss: 497.0576 - mae: 497.0575 - val_loss: 490.3609 - val_mae: 490.3609\n",
      "Epoch 18/145\n",
      " - 2s - loss: 478.2652 - mae: 478.2653 - val_loss: 486.6840 - val_mae: 486.6840\n",
      "Epoch 19/145\n",
      " - 2s - loss: 560.9539 - mae: 560.9540 - val_loss: 530.7492 - val_mae: 530.7493\n",
      "Epoch 20/145\n",
      " - 2s - loss: 511.0029 - mae: 511.0029 - val_loss: 499.3839 - val_mae: 499.3839\n",
      "Epoch 21/145\n",
      "lr changed to 0.008999999798834323\n",
      " - 2s - loss: 467.0980 - mae: 467.0980 - val_loss: 476.5513 - val_mae: 476.5512\n",
      "Epoch 22/145\n",
      " - 2s - loss: 444.3611 - mae: 444.3611 - val_loss: 464.7191 - val_mae: 464.7191\n",
      "Epoch 23/145\n",
      " - 2s - loss: 440.5158 - mae: 440.5158 - val_loss: 456.4418 - val_mae: 456.4417\n",
      "Epoch 24/145\n",
      " - 2s - loss: 443.3044 - mae: 443.3044 - val_loss: 458.1161 - val_mae: 458.1160\n",
      "Epoch 25/145\n",
      " - 2s - loss: 439.1589 - mae: 439.1589 - val_loss: 482.4198 - val_mae: 482.4198\n",
      "Epoch 26/145\n",
      " - 2s - loss: 439.1990 - mae: 439.1990 - val_loss: 460.0407 - val_mae: 460.0407\n",
      "Epoch 27/145\n",
      " - 2s - loss: 436.8551 - mae: 436.8551 - val_loss: 461.2107 - val_mae: 461.2107\n",
      "Epoch 28/145\n",
      " - 2s - loss: 456.5632 - mae: 456.5633 - val_loss: 459.0242 - val_mae: 459.0242\n",
      "Epoch 29/145\n",
      " - 2s - loss: 445.3379 - mae: 445.3379 - val_loss: 450.6070 - val_mae: 450.6070\n",
      "Epoch 30/145\n",
      " - 2s - loss: 433.6777 - mae: 433.6778 - val_loss: 460.3985 - val_mae: 460.3985\n",
      "Epoch 31/145\n",
      " - 2s - loss: 435.9308 - mae: 435.9308 - val_loss: 485.7524 - val_mae: 485.7523\n",
      "Epoch 32/145\n",
      " - 2s - loss: 447.0927 - mae: 447.0927 - val_loss: 447.0306 - val_mae: 447.0305\n",
      "Epoch 33/145\n",
      " - 2s - loss: 440.8403 - mae: 440.8404 - val_loss: 465.8262 - val_mae: 465.8262\n",
      "Epoch 34/145\n",
      " - 2s - loss: 428.9191 - mae: 428.9191 - val_loss: 444.8798 - val_mae: 444.8798\n",
      "Epoch 35/145\n",
      " - 2s - loss: 431.6275 - mae: 431.6275 - val_loss: 444.9842 - val_mae: 444.9842\n",
      "Epoch 36/145\n",
      " - 2s - loss: 429.9582 - mae: 429.9583 - val_loss: 452.7312 - val_mae: 452.7312\n",
      "Epoch 37/145\n",
      " - 2s - loss: 472.0936 - mae: 472.0937 - val_loss: 479.8413 - val_mae: 479.8413\n",
      "Epoch 38/145\n",
      " - 2s - loss: 436.5419 - mae: 436.5419 - val_loss: 448.8603 - val_mae: 448.8603\n",
      "Epoch 39/145\n",
      " - 2s - loss: 425.3665 - mae: 425.3664 - val_loss: 443.6025 - val_mae: 443.6025\n",
      "Epoch 40/145\n",
      " - 2s - loss: 422.2428 - mae: 422.2428 - val_loss: 446.4384 - val_mae: 446.4384\n",
      "Epoch 41/145\n",
      "lr changed to 0.005399999767541885\n",
      " - 2s - loss: 410.8004 - mae: 410.8004 - val_loss: 437.3522 - val_mae: 437.3522\n",
      "Epoch 42/145\n",
      " - 2s - loss: 408.9640 - mae: 408.9640 - val_loss: 437.8480 - val_mae: 437.8480\n",
      "Epoch 43/145\n",
      " - 2s - loss: 410.8629 - mae: 410.8629 - val_loss: 437.7213 - val_mae: 437.7213\n",
      "Epoch 44/145\n",
      " - 2s - loss: 408.0185 - mae: 408.0185 - val_loss: 438.2059 - val_mae: 438.2058\n",
      "Epoch 45/145\n",
      " - 2s - loss: 417.8907 - mae: 417.8907 - val_loss: 437.2969 - val_mae: 437.2969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/145\n",
      " - 2s - loss: 407.9708 - mae: 407.9709 - val_loss: 434.8809 - val_mae: 434.8809\n",
      "Epoch 47/145\n",
      " - 2s - loss: 406.4875 - mae: 406.4874 - val_loss: 435.9003 - val_mae: 435.9004\n",
      "Epoch 48/145\n",
      " - 2s - loss: 414.7071 - mae: 414.7072 - val_loss: 437.6923 - val_mae: 437.6923\n",
      "Epoch 49/145\n",
      " - 2s - loss: 405.7546 - mae: 405.7546 - val_loss: 437.2132 - val_mae: 437.2132\n",
      "Epoch 50/145\n",
      " - 2s - loss: 411.1179 - mae: 411.1179 - val_loss: 438.4213 - val_mae: 438.4213\n",
      "Epoch 51/145\n",
      " - 2s - loss: 404.7778 - mae: 404.7778 - val_loss: 435.5018 - val_mae: 435.5019\n",
      "Epoch 52/145\n",
      " - 2s - loss: 402.4938 - mae: 402.4938 - val_loss: 436.0253 - val_mae: 436.0253\n",
      "Epoch 53/145\n",
      " - 2s - loss: 400.5695 - mae: 400.5694 - val_loss: 433.4523 - val_mae: 433.4523\n",
      "Epoch 54/145\n",
      " - 2s - loss: 401.3205 - mae: 401.3206 - val_loss: 433.6615 - val_mae: 433.6615\n",
      "Epoch 55/145\n",
      " - 2s - loss: 400.8299 - mae: 400.8299 - val_loss: 431.1335 - val_mae: 431.1335\n",
      "Epoch 56/145\n",
      " - 2s - loss: 400.1628 - mae: 400.1628 - val_loss: 452.1078 - val_mae: 452.1078\n",
      "Epoch 57/145\n",
      " - 2s - loss: 401.2816 - mae: 401.2816 - val_loss: 433.1830 - val_mae: 433.1830\n",
      "Epoch 58/145\n",
      " - 2s - loss: 409.5241 - mae: 409.5242 - val_loss: 433.4871 - val_mae: 433.4871\n",
      "Epoch 59/145\n",
      " - 2s - loss: 404.0152 - mae: 404.0152 - val_loss: 471.7585 - val_mae: 471.7585\n",
      "Epoch 60/145\n",
      " - 2s - loss: 404.7066 - mae: 404.7066 - val_loss: 433.5457 - val_mae: 433.5457\n",
      "Epoch 61/145\n",
      "lr changed to 0.0032399998046457766\n",
      " - 2s - loss: 393.1473 - mae: 393.1473 - val_loss: 435.3257 - val_mae: 435.3257\n",
      "Epoch 62/145\n",
      " - 2s - loss: 389.2714 - mae: 389.2713 - val_loss: 425.4979 - val_mae: 425.4978\n",
      "Epoch 63/145\n",
      " - 2s - loss: 388.1677 - mae: 388.1677 - val_loss: 426.3929 - val_mae: 426.3929\n",
      "Epoch 64/145\n",
      " - 2s - loss: 387.1054 - mae: 387.1055 - val_loss: 425.1244 - val_mae: 425.1244\n",
      "Epoch 65/145\n",
      " - 2s - loss: 391.1566 - mae: 391.1566 - val_loss: 440.8443 - val_mae: 440.8443\n",
      "Epoch 66/145\n",
      " - 2s - loss: 389.9648 - mae: 389.9647 - val_loss: 424.7799 - val_mae: 424.7798\n",
      "Epoch 67/145\n",
      " - 2s - loss: 386.3525 - mae: 386.3524 - val_loss: 430.4090 - val_mae: 430.4090\n",
      "Epoch 68/145\n",
      " - 2s - loss: 387.1273 - mae: 387.1272 - val_loss: 426.6579 - val_mae: 426.6579\n",
      "Epoch 69/145\n",
      " - 2s - loss: 384.6013 - mae: 384.6013 - val_loss: 425.0611 - val_mae: 425.0611\n",
      "Epoch 70/145\n",
      " - 2s - loss: 384.8404 - mae: 384.8404 - val_loss: 424.0097 - val_mae: 424.0097\n",
      "Epoch 71/145\n",
      " - 2s - loss: 383.6210 - mae: 383.6211 - val_loss: 425.8006 - val_mae: 425.8006\n",
      "Epoch 72/145\n",
      " - 2s - loss: 382.4558 - mae: 382.4558 - val_loss: 429.3446 - val_mae: 429.3446\n",
      "Epoch 73/145\n",
      " - 2s - loss: 383.5065 - mae: 383.5066 - val_loss: 425.1842 - val_mae: 425.1842\n",
      "Epoch 74/145\n",
      " - 2s - loss: 384.0458 - mae: 384.0458 - val_loss: 426.5875 - val_mae: 426.5875\n",
      "Epoch 75/145\n",
      " - 2s - loss: 382.4902 - mae: 382.4902 - val_loss: 423.4759 - val_mae: 423.4759\n",
      "Epoch 76/145\n",
      " - 2s - loss: 381.5663 - mae: 381.5665 - val_loss: 422.3566 - val_mae: 422.3566\n",
      "Epoch 77/145\n",
      " - 2s - loss: 381.7957 - mae: 381.7957 - val_loss: 430.3223 - val_mae: 430.3223\n",
      "Epoch 78/145\n",
      " - 2s - loss: 382.6402 - mae: 382.6402 - val_loss: 428.4138 - val_mae: 428.4138\n",
      "Epoch 79/145\n",
      " - 2s - loss: 381.0289 - mae: 381.0288 - val_loss: 424.8137 - val_mae: 424.8138\n",
      "Epoch 80/145\n",
      " - 2s - loss: 381.3051 - mae: 381.3050 - val_loss: 424.1052 - val_mae: 424.1052\n",
      "Epoch 81/145\n",
      "lr changed to 0.0019439998548477888\n",
      " - 2s - loss: 376.7186 - mae: 376.7187 - val_loss: 424.8632 - val_mae: 424.8632\n",
      "Epoch 82/145\n",
      " - 2s - loss: 375.3938 - mae: 375.3938 - val_loss: 421.5808 - val_mae: 421.5808\n",
      "Epoch 83/145\n",
      " - 2s - loss: 374.1524 - mae: 374.1524 - val_loss: 421.0298 - val_mae: 421.0297\n",
      "Epoch 84/145\n",
      " - 2s - loss: 374.6851 - mae: 374.6852 - val_loss: 421.8930 - val_mae: 421.8929\n",
      "Epoch 85/145\n",
      " - 2s - loss: 375.1563 - mae: 375.1564 - val_loss: 421.6456 - val_mae: 421.6456\n",
      "Epoch 86/145\n",
      " - 2s - loss: 374.5912 - mae: 374.5912 - val_loss: 427.7377 - val_mae: 427.7377\n",
      "Epoch 87/145\n",
      " - 2s - loss: 374.0789 - mae: 374.0789 - val_loss: 425.0594 - val_mae: 425.0594\n",
      "Epoch 88/145\n",
      " - 2s - loss: 374.1215 - mae: 374.1215 - val_loss: 423.0790 - val_mae: 423.0790\n",
      "Epoch 89/145\n",
      " - 2s - loss: 372.6733 - mae: 372.6733 - val_loss: 424.2025 - val_mae: 424.2026\n",
      "Epoch 90/145\n",
      " - 2s - loss: 371.5177 - mae: 371.5178 - val_loss: 419.9351 - val_mae: 419.9351\n",
      "Epoch 91/145\n",
      " - 2s - loss: 371.6431 - mae: 371.6431 - val_loss: 422.8444 - val_mae: 422.8444\n",
      "Epoch 92/145\n",
      " - 2s - loss: 373.9714 - mae: 373.9714 - val_loss: 421.5276 - val_mae: 421.5276\n",
      "Epoch 93/145\n",
      " - 2s - loss: 372.7701 - mae: 372.7701 - val_loss: 422.5671 - val_mae: 422.5671\n",
      "Epoch 94/145\n",
      " - 2s - loss: 370.6370 - mae: 370.6370 - val_loss: 421.1778 - val_mae: 421.1779\n",
      "Epoch 95/145\n",
      " - 2s - loss: 370.4647 - mae: 370.4648 - val_loss: 419.8114 - val_mae: 419.8114\n",
      "Epoch 96/145\n",
      " - 2s - loss: 369.5629 - mae: 369.5629 - val_loss: 421.7427 - val_mae: 421.7427\n",
      "Epoch 97/145\n",
      " - 2s - loss: 370.2477 - mae: 370.2477 - val_loss: 420.8979 - val_mae: 420.8978\n",
      "Epoch 98/145\n",
      " - 2s - loss: 369.7426 - mae: 369.7426 - val_loss: 421.9186 - val_mae: 421.9186\n",
      "Epoch 99/145\n",
      " - 2s - loss: 369.2089 - mae: 369.2088 - val_loss: 423.5433 - val_mae: 423.5433\n",
      "Epoch 100/145\n",
      " - 2s - loss: 371.3505 - mae: 371.3504 - val_loss: 425.6763 - val_mae: 425.6762\n",
      "Epoch 101/145\n",
      "lr changed to 0.0011663999408483504\n",
      " - 2s - loss: 366.3053 - mae: 366.3053 - val_loss: 418.7235 - val_mae: 418.7236\n",
      "Epoch 102/145\n",
      " - 2s - loss: 365.4407 - mae: 365.4407 - val_loss: 418.6135 - val_mae: 418.6135\n",
      "Epoch 103/145\n",
      " - 2s - loss: 365.2590 - mae: 365.2590 - val_loss: 417.8425 - val_mae: 417.8425\n",
      "Epoch 104/145\n",
      " - 2s - loss: 365.7476 - mae: 365.7475 - val_loss: 418.7212 - val_mae: 418.7213\n",
      "Epoch 105/145\n",
      " - 2s - loss: 364.8234 - mae: 364.8234 - val_loss: 419.4852 - val_mae: 419.4852\n",
      "Epoch 106/145\n",
      " - 2s - loss: 364.7201 - mae: 364.7201 - val_loss: 419.3199 - val_mae: 419.3199\n",
      "Epoch 107/145\n",
      " - 2s - loss: 364.6338 - mae: 364.6338 - val_loss: 419.9523 - val_mae: 419.9523\n",
      "Epoch 108/145\n",
      " - 2s - loss: 364.4271 - mae: 364.4272 - val_loss: 421.7285 - val_mae: 421.7285\n",
      "Epoch 109/145\n",
      " - 2s - loss: 364.9257 - mae: 364.9257 - val_loss: 419.4186 - val_mae: 419.4186\n",
      "Epoch 110/145\n",
      " - 2s - loss: 364.0384 - mae: 364.0384 - val_loss: 419.2753 - val_mae: 419.2753\n",
      "Epoch 111/145\n",
      " - 2s - loss: 364.0562 - mae: 364.0562 - val_loss: 420.6953 - val_mae: 420.6953\n",
      "Epoch 112/145\n",
      " - 2s - loss: 363.3784 - mae: 363.3783 - val_loss: 420.0086 - val_mae: 420.0086\n",
      "Epoch 113/145\n",
      " - 2s - loss: 362.9294 - mae: 362.9294 - val_loss: 418.6921 - val_mae: 418.6921\n",
      "Epoch 114/145\n",
      " - 2s - loss: 362.6022 - mae: 362.6022 - val_loss: 420.6958 - val_mae: 420.6958\n",
      "Epoch 115/145\n",
      " - 2s - loss: 362.1738 - mae: 362.1739 - val_loss: 417.9811 - val_mae: 417.9811\n",
      "Epoch 116/145\n",
      " - 2s - loss: 362.1795 - mae: 362.1795 - val_loss: 420.6379 - val_mae: 420.6379\n",
      "Epoch 117/145\n",
      " - 2s - loss: 362.4352 - mae: 362.4352 - val_loss: 418.9702 - val_mae: 418.9702\n",
      "Epoch 118/145\n",
      " - 2s - loss: 362.3154 - mae: 362.3153 - val_loss: 418.3288 - val_mae: 418.3288\n",
      "Epoch 119/145\n",
      " - 2s - loss: 361.5250 - mae: 361.5250 - val_loss: 419.7631 - val_mae: 419.7631\n",
      "Epoch 120/145\n",
      " - 2s - loss: 361.9870 - mae: 361.9870 - val_loss: 417.4964 - val_mae: 417.4964\n",
      "Epoch 121/145\n",
      "lr changed to 0.0006998399505391716\n",
      " - 2s - loss: 359.9979 - mae: 359.9979 - val_loss: 418.1473 - val_mae: 418.1473\n",
      "Epoch 122/145\n",
      " - 2s - loss: 359.6510 - mae: 359.6510 - val_loss: 417.5620 - val_mae: 417.5620\n",
      "Epoch 123/145\n",
      " - 2s - loss: 359.1266 - mae: 359.1266 - val_loss: 417.8943 - val_mae: 417.8943\n",
      "Epoch 124/145\n",
      " - 2s - loss: 359.1038 - mae: 359.1038 - val_loss: 418.5329 - val_mae: 418.5330\n",
      "Epoch 125/145\n",
      " - 2s - loss: 358.9500 - mae: 358.9501 - val_loss: 417.9160 - val_mae: 417.9160\n",
      "Epoch 126/145\n",
      " - 2s - loss: 358.8235 - mae: 358.8235 - val_loss: 417.8334 - val_mae: 417.8334\n",
      "Epoch 127/145\n",
      " - 2s - loss: 358.8496 - mae: 358.8496 - val_loss: 418.1831 - val_mae: 418.1832\n",
      "Epoch 128/145\n",
      " - 2s - loss: 359.0230 - mae: 359.0230 - val_loss: 419.4334 - val_mae: 419.4334\n",
      "Epoch 129/145\n",
      " - 2s - loss: 358.7976 - mae: 358.7976 - val_loss: 418.1266 - val_mae: 418.1266\n",
      "Epoch 130/145\n",
      " - 2s - loss: 358.0855 - mae: 358.0855 - val_loss: 417.9639 - val_mae: 417.9639\n",
      "Epoch 131/145\n",
      " - 2s - loss: 358.3278 - mae: 358.3279 - val_loss: 418.9836 - val_mae: 418.9836\n",
      "Epoch 132/145\n",
      " - 2s - loss: 358.0677 - mae: 358.0677 - val_loss: 418.4499 - val_mae: 418.4498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/145\n",
      " - 2s - loss: 357.5971 - mae: 357.5971 - val_loss: 417.5558 - val_mae: 417.5558\n",
      "Epoch 134/145\n",
      " - 2s - loss: 358.3248 - mae: 358.3248 - val_loss: 419.3984 - val_mae: 419.3983\n",
      "Epoch 135/145\n",
      " - 2s - loss: 357.6371 - mae: 357.6371 - val_loss: 418.6940 - val_mae: 418.6940\n",
      "Epoch 136/145\n",
      " - 2s - loss: 357.4288 - mae: 357.4288 - val_loss: 417.7966 - val_mae: 417.7966\n",
      "Epoch 137/145\n",
      " - 2s - loss: 357.4254 - mae: 357.4254 - val_loss: 419.3640 - val_mae: 419.3640\n",
      "Epoch 138/145\n",
      " - 2s - loss: 357.2710 - mae: 357.2710 - val_loss: 418.5160 - val_mae: 418.5160\n",
      "Epoch 139/145\n",
      " - 2s - loss: 356.9453 - mae: 356.9453 - val_loss: 418.4182 - val_mae: 418.4182\n",
      "Epoch 140/145\n",
      " - 2s - loss: 356.7634 - mae: 356.7634 - val_loss: 417.5063 - val_mae: 417.5063\n",
      "Epoch 141/145\n",
      "lr changed to 0.0004199039773084223\n",
      " - 2s - loss: 355.7663 - mae: 355.7663 - val_loss: 417.9978 - val_mae: 417.9978\n",
      "Epoch 142/145\n",
      " - 2s - loss: 355.5031 - mae: 355.5032 - val_loss: 418.1028 - val_mae: 418.1028\n",
      "Epoch 143/145\n",
      " - 2s - loss: 355.4726 - mae: 355.4726 - val_loss: 417.9050 - val_mae: 417.9050\n",
      "Epoch 144/145\n",
      " - 2s - loss: 355.9160 - mae: 355.9160 - val_loss: 417.3708 - val_mae: 417.3708\n",
      "Epoch 145/145\n",
      " - 2s - loss: 355.3620 - mae: 355.3620 - val_loss: 417.9844 - val_mae: 417.9844\n",
      "\n",
      "val_mae is:417.9844057938695\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "413.12414704614935"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_splits = 6\n",
    "kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "import keras \n",
    "\n",
    "b_size = 2000\n",
    "max_epochs = 145\n",
    "oof_pred = np.zeros((len(X_pca), ))\n",
    "\n",
    "sub = pd.read_csv('input/used_car_testB_20200421.csv', sep=' ')[['SaleID']].copy()\n",
    "sub['price'] = 0\n",
    "\n",
    "avg_mae = 0\n",
    "for fold, (trn_idx, val_idx) in enumerate(kf.split(X_pca, y)):\n",
    "    print('fold:', fold)\n",
    "    X_train, y_train = X_pca[trn_idx], y[trn_idx]\n",
    "    X_val, y_val = X_pca[val_idx], y[val_idx]\n",
    "    \n",
    "    model = NN_model(X_train.shape[1])\n",
    "    simple_adam = keras.optimizers.Adam(lr = 0.015)\n",
    "    \n",
    "    model.compile(loss='mae', optimizer=simple_adam,metrics=['mae'])\n",
    "    es = EarlyStopping(monitor='val_score', patience=10, verbose=2, mode='min', restore_best_weights=True,)\n",
    "    es.set_model(model)\n",
    "    metric = Metric(model, [es], [(X_train, y_train), (X_val, y_val)])\n",
    "    model.fit(X_train, y_train, batch_size=b_size, epochs=max_epochs, \n",
    "              validation_data = [X_val, y_val],\n",
    "              callbacks=[reduce_lr], shuffle=True, verbose=2)\n",
    "    y_pred3 = model.predict(X_val)\n",
    "    y_pred = np.zeros((len(y_pred3), ))\n",
    "    sub['price'] += model.predict(test).reshape(-1,) / n_splits\n",
    "    for i in range(len(y_pred3)):\n",
    "        y_pred[i] = y_pred3[i]\n",
    "        \n",
    "    oof_pred[val_idx] = y_pred\n",
    "    val_mae = mean_absolute_error(y[val_idx], y_pred)\n",
    "    avg_mae += val_mae / n_splits\n",
    "    print()\n",
    "    print('val_mae is:{}'.format(val_mae))\n",
    "    print()\n",
    "mean_absolute_error(y, oof_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:43:01.185665Z",
     "start_time": "2020-06-22T10:43:01.175692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SaleID</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>1274.862350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>2006.420776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200002</td>\n",
       "      <td>8742.022217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003</td>\n",
       "      <td>1254.729889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200004</td>\n",
       "      <td>1982.045349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200005</td>\n",
       "      <td>1118.820114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200006</td>\n",
       "      <td>442.411957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200007</td>\n",
       "      <td>3531.691284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200008</td>\n",
       "      <td>13751.573242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200009</td>\n",
       "      <td>602.167061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>200010</td>\n",
       "      <td>641.236519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>200011</td>\n",
       "      <td>2759.430573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200012</td>\n",
       "      <td>5662.797791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>200013</td>\n",
       "      <td>7002.250488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200014</td>\n",
       "      <td>1117.684814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>200015</td>\n",
       "      <td>246.688683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200016</td>\n",
       "      <td>1748.410889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>200017</td>\n",
       "      <td>8320.903931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>200018</td>\n",
       "      <td>6777.327515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>200019</td>\n",
       "      <td>1219.183578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SaleID         price\n",
       "0   200000   1274.862350\n",
       "1   200001   2006.420776\n",
       "2   200002   8742.022217\n",
       "3   200003   1254.729889\n",
       "4   200004   1982.045349\n",
       "5   200005   1118.820114\n",
       "6   200006    442.411957\n",
       "7   200007   3531.691284\n",
       "8   200008  13751.573242\n",
       "9   200009    602.167061\n",
       "10  200010    641.236519\n",
       "11  200011   2759.430573\n",
       "12  200012   5662.797791\n",
       "13  200013   7002.250488\n",
       "14  200014   1117.684814\n",
       "15  200015    246.688683\n",
       "16  200016   1748.410889\n",
       "17  200017   8320.903931\n",
       "18  200018   6777.327515\n",
       "19  200019   1219.183578"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T10:43:01.342350Z",
     "start_time": "2020-06-22T10:43:01.187174Z"
    }
   },
   "outputs": [],
   "source": [
    "sub.to_csv('submit/nn_sub_{}_{}.csv'.format('mae', sub['price'].mean()), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
