{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import pickleshare\n",
    "\n",
    "\n",
    "def count_encode(df, cols=[]):\n",
    "    \"\"\"\n",
    "    count编码\n",
    "    @param df:\n",
    "    @param cols:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        print(col)\n",
    "        vc = df[col].value_counts(dropna=True, normalize=True)\n",
    "        df[col + '_count'] = df[col].map(vc).astype('float32')\n",
    "    return df\n",
    "\n",
    "\n",
    "def cross_cat_num(df, cat_col, num_col):\n",
    "    \"\"\"\n",
    "    类别特征与数据特征groupby统计\n",
    "    @param df:\n",
    "    @param cat_col: 类别特征\n",
    "    @param num_col: 数值特征\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    def max_min(s):\n",
    "        return s.max() - s.min()\n",
    "    def quantile(s, q=0.25):\n",
    "        return s.quantile(q)\n",
    "    for f1 in cat_col:\n",
    "        g = df.groupby(f1, as_index=False)\n",
    "        for f2 in num_col:\n",
    "            tmp = g[f2].agg({\n",
    "                '{}_{}_count'.format(f1, f2): 'count',\n",
    "                '{}_{}_max'.format(f1, f2): 'max',\n",
    "                '{}_{}_min'.format(f1, f2): 'min',\n",
    "                '{}_{}_median'.format(f1, f2): 'median',\n",
    "                '{}_{}_mean'.format(f1, f2): 'mean',\n",
    "                '{}_{}_sum'.format(f1, f2): 'sum',\n",
    "                '{}_{}_skew'.format(f1, f2): 'skew',\n",
    "                '{}_{}_std'.format(f1, f2): 'std',\n",
    "                '{}_{}_nunique'.format(f1, f2): 'nunique',\n",
    "                '{}_{}_max_min'.format(f1, f2): max_min,\n",
    "                '{}_{}_quantile_25'.format(f1, f2): lambda x: quantile(x, 0.25),\n",
    "                '{}_{}_quantile_75'.format(f1, f2): lambda x: quantile(x, 0.75)\n",
    "            })\n",
    "            df = df.merge(tmp, on=f1, how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "def arithmetic(df, cross_features):\n",
    "    \"\"\"\n",
    "    数值特征之间的加减乘除\n",
    "    @param df:\n",
    "    @param cross_features: 交叉用的数值特征\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    for i in range(len(cross_features)):\n",
    "        for j in range(i + 1, len(cross_features)):\n",
    "            colname_add = '{}_{}_add'.format(cross_features[i], cross_features[j])\n",
    "            colname_substract = '{}_{}_subtract'.format(cross_features[i], cross_features[j])\n",
    "            colname_multiply = '{}_{}c_multiply'.format(cross_features[i], cross_features[j])\n",
    "            df[colname_add] = df[cross_features[i]] + df[cross_features[j]]\n",
    "            df[colname_substract] = df[cross_features[i]] - df[cross_features[j]]\n",
    "            df[colname_multiply] = df[cross_features[i]] * df[cross_features[j]]\n",
    "\n",
    "    for f1 in cross_features:\n",
    "        for f2 in cross_features:\n",
    "            if f1 != f2:\n",
    "                colname_ratio = '{}_{}_ratio'.format(f1, f2)\n",
    "                df[colname_ratio] = df[f1].values / (df[f2].values + 0.001)\n",
    "    return df\n",
    "\n",
    "\n",
    "def auc_select(X_train, y_train, X_valid, y_valid, cols, threshold=0.52):\n",
    "    \"\"\"\n",
    "    基于AUC的单特征筛选\n",
    "    @param X_train:\n",
    "    @param y_train:\n",
    "    @param X_valid:\n",
    "    @param y_valid:\n",
    "    @param cols:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    useful_dict = dict()\n",
    "    useless_dict = dict()\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 31,\n",
    "        'lambda_l1': 0,\n",
    "        'lambda_l2': 1,\n",
    "        'num_threads': 23,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'first_metric_only': True,\n",
    "        'is_unbalance': True,\n",
    "        'max_depth': -1,\n",
    "        'seed': 2020\n",
    "    }\n",
    "    for i in cols:\n",
    "        print(i)\n",
    "        lgb_train = lgb.Dataset(X_train[[i]].values, y_train)\n",
    "        lgb_valid = lgb.Dataset(X_valid[[i]].values, y_valid, reference=lgb_train)\n",
    "        lgb_model = lgb.train(\n",
    "            params,\n",
    "            lgb_train,\n",
    "            valid_sets=[lgb_valid, lgb_train],\n",
    "            num_boost_round=1000,\n",
    "            early_stopping_rounds=50,\n",
    "            verbose_eval=20\n",
    "        )\n",
    "        print('*' * 10)\n",
    "        print(lgb_model.best_score['valid_0']['auc'])\n",
    "        if lgb_model.best_score['valid_0']['auc'] > threshold:\n",
    "            useful_dict[i] = lgb_model.best_score['valid_0']['auc']\n",
    "        else:\n",
    "            useless_dict[i] = lgb_model.best_score['valid_0']['auc']\n",
    "    useful_cols = list(useful_dict.keys())\n",
    "    useless_cols = list(useless_dict.keys())\n",
    "    return useful_dict, useless_dict, useful_cols, useless_cols\n",
    "\n",
    "\n",
    "def correlation(df, useful_dict, threshold=0.98):\n",
    "    \"\"\"\n",
    "    去除特征相关系数大于阈值的特征\n",
    "    @param df:\n",
    "    @param threshold:\n",
    "    @param useful_dict:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    col_corr = set()\n",
    "    corr_matrix = df.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                colName_i = corr_matrix.columns[i]\n",
    "                colName_j = corr_matrix.columns[j]\n",
    "                if useful_dict[colName_i] >= useful_dict[colName_j]:\n",
    "                    col_corr.add(colName_j)\n",
    "                else:\n",
    "                    col_corr.add(colName_i)\n",
    "    return col_corr\n",
    "\n",
    "\n",
    "def train_test_label_encode(df, cat_col, type='save', path='./'):\n",
    "    \"\"\"\n",
    "    train和test分开label encode\n",
    "    @param df:\n",
    "    @param cat_col:\n",
    "    @param type: 'save' 'load'\n",
    "    @param path:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    def save_obj(obj, name):\n",
    "        with open(name + '.pkl', 'wb') as f:\n",
    "            pickleshare.dump(obj, f)\n",
    "\n",
    "    def load_obj(name):\n",
    "        with open(name + '.pkl', 'rb') as f:\n",
    "            return pickleshare.load(f)\n",
    "\n",
    "    if type == 'save':\n",
    "        print(cat_col)\n",
    "        d = dict(zip(df[cat_col].unique(), range(df[cat_col].nunique())))\n",
    "        df[cat_col] = df[cat_col].map(d)\n",
    "        np.save(path + '{}.npy'.format(cat_col), d)\n",
    "        return df\n",
    "    elif type == 'load':\n",
    "        d = np.load(path + '{}.npy'.format(cat_col), allow_pickle=True).item()\n",
    "        return d\n",
    "\n",
    "\n",
    "def test_func(test_path,save_path):\n",
    "    # 请填写测试代码\n",
    "    test = pd.read_csv(test_path)\n",
    "    submission = test[['eventId']]\n",
    "    # 选手不得改变格式，测试代码跑不通分数以零算\n",
    "\n",
    "    single_cols = ['appProtocol']\n",
    "    test.drop(single_cols, axis=1, inplace=True)\n",
    "\n",
    "    cat_cols = ['srcAddress', 'destAddress',\n",
    "                'tlsVersion', 'tlsSubject', 'tlsIssuerDn', 'tlsSni']\n",
    "\n",
    "    test['srcAddressPort'] = test['srcAddress'].astype(str) + test['srcPort'].astype(str)\n",
    "    test['destAddressPort'] = test['destAddress'].astype(str) + test['destPort'].astype(str)\n",
    "\n",
    "    # srcAddress To destAddress\n",
    "    tmp = test.groupby('srcAddress', as_index=False)['destAddress'].agg({\n",
    "        's2d_count': 'count',\n",
    "        's2d_nunique': 'nunique'\n",
    "    })\n",
    "    test = test.merge(tmp, on='srcAddress', how='left')\n",
    "\n",
    "    # srcAddressPort To destAddressPort\n",
    "    tmp = test.groupby('srcAddressPort', as_index=False)['destAddressPort'].agg({\n",
    "        'sp2dp_count': 'count',\n",
    "        'sp2dp_nunique': 'nunique'\n",
    "    })\n",
    "    test = test.merge(tmp, on='srcAddressPort', how='left')\n",
    "\n",
    "    # srcAddress To destAddressPort\n",
    "    tmp = test.groupby('srcAddress', as_index=False)['destAddressPort'].agg({\n",
    "        's2dp_count': 'count',\n",
    "        's2dp_nunique': 'nunique'\n",
    "    })\n",
    "    test = test.merge(tmp, on='srcAddress', how='left')\n",
    "\n",
    "    # srcAddressPort To destAddress\n",
    "    tmp = test.groupby('srcAddressPort', as_index=False)['destAddress'].agg({\n",
    "        'sp2d_count': 'count',\n",
    "        'sp2d_nunique': 'nunique'\n",
    "    })\n",
    "    test = test.merge(tmp, on='srcAddressPort', how='left')\n",
    "\n",
    "    # destAddress To srcAddress\n",
    "    tmp = test.groupby('destAddress', as_index=False)['srcAddress'].agg({\n",
    "        'd2s_count': 'count',\n",
    "        'd2s_nunique': 'nunique'\n",
    "    })\n",
    "    test = test.merge(tmp, on='destAddress', how='left')\n",
    "\n",
    "    # destAddressPort To srcAddressPort\n",
    "    tmp = test.groupby('destAddressPort', as_index=False)['srcAddressPort'].agg({\n",
    "        'dp2sp_count': 'count',\n",
    "        'dp2sp_nunique': 'nunique'\n",
    "    })\n",
    "    test = test.merge(tmp, on='destAddressPort', how='left')\n",
    "\n",
    "    # destAddressPort To srcAddress\n",
    "    tmp = test.groupby('destAddressPort', as_index=False)['srcAddress'].agg({\n",
    "        'dp2s_count': 'count',\n",
    "        'dp2s_nunique': 'nunique'\n",
    "    })\n",
    "    test = test.merge(tmp, on='destAddressPort', how='left')\n",
    "\n",
    "    # destAddress To srcAddressProt\n",
    "    tmp = test.groupby('destAddress', as_index=False)['srcAddressPort'].agg({\n",
    "        'd2sp_count': 'count',\n",
    "        'd2sp_nunique': 'nunique'\n",
    "    })\n",
    "    test = test.merge(tmp, on='destAddress', how='left')\n",
    "\n",
    "    tlsVersion_map = {\n",
    "        'TLSv1': 1,\n",
    "        'TLS 1.2': 1,\n",
    "        'TLS 1.3': 1,\n",
    "        'SSLv2': 2,\n",
    "        'SSLv3': 3,\n",
    "        '0x4854': 4,\n",
    "        '0x4752': 4,\n",
    "        'UNDETERMINED': 5\n",
    "    }\n",
    "    test['tlsVersion_map'] = test['tlsVersion'].map(tlsVersion_map)\n",
    "    cat_cols.append('tlsVersion_map')\n",
    "\n",
    "    cat_cols += ['srcAddressPort', 'destAddressPort']\n",
    "    num_cols = ['bytesOut', 'bytesIn', 'pktsIn', 'pktsOut']\n",
    "\n",
    "    test = count_encode(test, cat_cols)\n",
    "    test = cross_cat_num(test, cat_cols, num_cols)\n",
    "    test = arithmetic(test, num_cols)\n",
    "\n",
    "    # for i in cat_cols:\n",
    "    #     d = train_test_label_encode(test, i, 'load', '../train_code/')\n",
    "    #     test[i] = test[i].map(d)\n",
    "    #     test[i] = test[i].astype('category')\n",
    "\n",
    "    used_cols = [i for i in test.columns if i not in ['eventId']]\n",
    "    test = test[used_cols].copy()\n",
    "\n",
    "    psi_drop_cols = ['tlsSubject', 'destAddress', 'srcAddress', 'srcAddressPort', 'tlsIssuerDn', 'tlsSni',\n",
    "                     'destAddressPort', 'tlsVersion']\n",
    "    test.drop(psi_drop_cols, axis=1, inplace=True)\n",
    "\n",
    "    print('test.shape: \\n', test.shape)\n",
    "    used_cols = ['srcPort', 'destPort', 's2d_nunique', 'd2sp_nunique', 'srcAddress_bytesOut_min', 'srcAddress_bytesOut_median', 'srcAddress_bytesOut_mean', 'srcAddress_bytesOut_skew', 'srcAddress_bytesOut_nunique', 'srcAddress_bytesOut_max_min', 'srcAddress_bytesOut_quantile_25', 'srcAddress_bytesOut_quantile_75', 'srcAddress_bytesIn_min', 'srcAddress_bytesIn_skew', 'srcAddress_bytesIn_std', 'srcAddress_bytesIn_nunique', 'srcAddress_bytesIn_quantile_25', 'srcAddress_pktsIn_mean', 'srcAddress_pktsIn_sum', 'srcAddress_pktsIn_skew', 'srcAddress_pktsIn_nunique', 'srcAddress_pktsIn_quantile_25', 'srcAddress_pktsOut_mean', 'srcAddress_pktsOut_sum', 'srcAddress_pktsOut_skew', 'srcAddress_pktsOut_std', 'srcAddress_pktsOut_nunique', 'srcAddress_pktsOut_quantile_75', 'destAddress_bytesOut_min', 'destAddress_bytesOut_median', 'destAddress_bytesOut_skew', 'destAddress_bytesOut_nunique', 'destAddress_bytesOut_quantile_25', 'destAddress_bytesIn_median', 'destAddress_bytesIn_skew', 'destAddress_pktsIn_sum', 'destAddress_pktsIn_skew', 'destAddress_pktsIn_std', 'destAddress_pktsIn_nunique', 'destAddress_pktsOut_min', 'destAddress_pktsOut_median', 'destAddress_pktsOut_mean', 'destAddress_pktsOut_skew', 'destAddress_pktsOut_std', 'destAddress_pktsOut_nunique', 'destAddress_pktsOut_quantile_25', 'tlsVersion_bytesOut_quantile_25', 'tlsVersion_bytesIn_quantile_25', 'tlsVersion_bytesIn_quantile_75', 'tlsVersion_pktsIn_min', 'tlsVersion_pktsIn_mean', 'tlsVersion_pktsIn_quantile_75', 'tlsVersion_pktsOut_min', 'tlsVersion_pktsOut_median', 'tlsVersion_pktsOut_skew', 'tlsVersion_pktsOut_std', 'tlsVersion_pktsOut_nunique', 'tlsVersion_pktsOut_max_min', 'tlsVersion_pktsOut_quantile_25', 'tlsVersion_pktsOut_quantile_75', 'tlsSubject_bytesOut_min', 'tlsSubject_bytesOut_median', 'tlsSubject_bytesOut_skew', 'tlsSubject_bytesOut_std', 'tlsSubject_bytesOut_nunique', 'tlsSubject_bytesOut_max_min', 'tlsSubject_bytesOut_quantile_25', 'tlsSubject_bytesOut_quantile_75', 'tlsSubject_bytesIn_min', 'tlsSubject_bytesIn_sum', 'tlsSubject_bytesIn_skew', 'tlsSubject_bytesIn_nunique', 'tlsSubject_bytesIn_max_min', 'tlsSubject_pktsIn_mean', 'tlsSubject_pktsIn_sum', 'tlsSubject_pktsIn_skew', 'tlsSubject_pktsIn_nunique', 'tlsSubject_pktsIn_max_min', 'tlsSubject_pktsIn_quantile_25', 'tlsSubject_pktsIn_quantile_75', 'tlsSubject_pktsOut_count', 'tlsSubject_pktsOut_median', 'tlsSubject_pktsOut_mean', 'tlsSubject_pktsOut_sum', 'tlsSubject_pktsOut_skew', 'tlsSubject_pktsOut_std', 'tlsSubject_pktsOut_nunique', 'tlsSubject_pktsOut_quantile_25', 'tlsIssuerDn_bytesOut_min', 'tlsIssuerDn_bytesOut_skew', 'tlsIssuerDn_bytesOut_std', 'tlsIssuerDn_bytesOut_quantile_25', 'tlsIssuerDn_bytesIn_min', 'tlsIssuerDn_bytesIn_median', 'tlsIssuerDn_bytesIn_mean', 'tlsIssuerDn_bytesIn_sum', 'tlsIssuerDn_bytesIn_skew', 'tlsIssuerDn_bytesIn_max_min', 'tlsIssuerDn_bytesIn_quantile_75', 'tlsIssuerDn_pktsIn_min', 'tlsIssuerDn_pktsIn_mean', 'tlsIssuerDn_pktsIn_sum', 'tlsIssuerDn_pktsIn_skew', 'tlsIssuerDn_pktsIn_std', 'tlsIssuerDn_pktsIn_nunique', 'tlsIssuerDn_pktsIn_max_min', 'tlsIssuerDn_pktsOut_count', 'tlsIssuerDn_pktsOut_sum', 'tlsIssuerDn_pktsOut_std', 'tlsIssuerDn_pktsOut_nunique', 'tlsIssuerDn_pktsOut_max_min', 'tlsSni_bytesOut_max', 'tlsSni_bytesOut_min', 'tlsSni_bytesOut_median', 'tlsSni_bytesOut_mean', 'tlsSni_bytesOut_sum', 'tlsSni_bytesOut_skew', 'tlsSni_bytesOut_std', 'tlsSni_bytesOut_nunique', 'tlsSni_bytesOut_max_min', 'tlsSni_bytesOut_quantile_25', 'tlsSni_bytesOut_quantile_75', 'tlsSni_bytesIn_min', 'tlsSni_bytesIn_sum', 'tlsSni_bytesIn_skew', 'tlsSni_bytesIn_nunique', 'tlsSni_pktsIn_min', 'tlsSni_pktsIn_median', 'tlsSni_pktsIn_sum', 'tlsSni_pktsIn_skew', 'tlsSni_pktsIn_nunique', 'tlsSni_pktsIn_max_min', 'tlsSni_pktsIn_quantile_25', 'tlsSni_pktsOut_count', 'tlsSni_pktsOut_median', 'tlsSni_pktsOut_mean', 'tlsSni_pktsOut_skew', 'tlsSni_pktsOut_std', 'tlsSni_pktsOut_nunique', 'tlsSni_pktsOut_quantile_75', 'srcAddressPort_bytesOut_std', 'srcAddressPort_bytesIn_std', 'srcAddressPort_pktsIn_sum', 'srcAddressPort_pktsIn_std', 'srcAddressPort_pktsOut_count', 'srcAddressPort_pktsOut_sum', 'srcAddressPort_pktsOut_std', 'destAddressPort_bytesOut_quantile_75', 'destAddressPort_bytesIn_min', 'destAddressPort_bytesIn_nunique', 'destAddressPort_bytesIn_quantile_25', 'destAddressPort_pktsIn_min', 'destAddressPort_pktsOut_sum', 'bytesOut_bytesIn_add', 'bytesOut_bytesIn_subtract', 'bytesOut_bytesInc_multiply', 'bytesOut_pktsIn_add', 'bytesOut_pktsInc_multiply', 'bytesOut_pktsOutc_multiply', 'bytesIn_pktsOut_subtract', 'pktsIn_pktsOut_add', 'pktsIn_pktsOut_subtract', 'bytesOut_bytesIn_ratio', 'bytesOut_pktsIn_ratio', 'bytesOut_pktsOut_ratio', 'bytesIn_bytesOut_ratio', 'bytesIn_pktsIn_ratio', 'bytesIn_pktsOut_ratio', 'pktsIn_bytesOut_ratio', 'pktsIn_bytesIn_ratio', 'pktsIn_pktsOut_ratio', 'pktsOut_bytesOut_ratio', 'pktsOut_bytesIn_ratio', 'pktsOut_pktsIn_ratio']\n",
    "    X_test = test[used_cols].copy()\n",
    "    print('X_test.shape: \\n', X_test.shape)\n",
    "\n",
    "    train_model = lgb.Booster(model_file='./lgb.txt')\n",
    "#     train_model.save_model('./lgb.txt')\n",
    "\n",
    "    pred = train_model.predict(X_test)\n",
    "    threshold = np.load('../train_code/threshold.npy')\n",
    "    y_pred = np.where(pred >= threshold, 1, 0)\n",
    "\n",
    "    submission['label'] = y_pred\n",
    "    print('y_pred.mean(): \\n', y_pred.mean())\n",
    "\n",
    "    submission.to_csv(save_path + '机器不学习原子弹也不学习_eta_submission_1022.csv', index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_path = '../../大数据队_eta_submission_1011/data/test_1.csv'\n",
    "    sava_path = './'\n",
    "    test_func(test_path, sava_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
