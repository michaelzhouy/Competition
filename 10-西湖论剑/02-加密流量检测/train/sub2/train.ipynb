{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T09:09:51.812168Z",
     "start_time": "2020-10-26T09:04:36.718667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srcAddress\n",
      "destAddress\n",
      "tlsVersion\n",
      "tlsSubject\n",
      "tlsIssuerDn\n",
      "tlsSni\n",
      "tlsVersion_map\n",
      "srcAddressPort\n",
      "destAddressPort\n",
      "Features:\n",
      " 500\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "\n",
    "# def count_encode(df, cols=[]):\n",
    "#     \"\"\"\n",
    "#     count编码\n",
    "#     @param df:\n",
    "#     @param cols:\n",
    "#     @return:\n",
    "#     \"\"\"\n",
    "#     for col in cols:\n",
    "#         print(col)\n",
    "#         vc = df[col].value_counts(dropna=True, normalize=True)\n",
    "#         df[col + '_count'] = df[col].map(vc).astype('float32')\n",
    "#     return df\n",
    "\n",
    "\n",
    "def cross_cat_num(df, cat_col, num_col):\n",
    "    \"\"\"\n",
    "    类别特征与数据特征groupby统计\n",
    "    @param df:\n",
    "    @param cat_col: 类别特征\n",
    "    @param num_col: 数值特征\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    def max_min(s):\n",
    "        return s.max() - s.min()\n",
    "    def quantile(s, q=0.25):\n",
    "        return s.quantile(q)\n",
    "    for f1 in cat_col:\n",
    "        g = df.groupby(f1, as_index=False)\n",
    "        for f2 in num_col:\n",
    "            tmp = g[f2].agg({\n",
    "                '{}_{}_count'.format(f1, f2): 'count',\n",
    "                '{}_{}_max'.format(f1, f2): 'max',\n",
    "                '{}_{}_min'.format(f1, f2): 'min',\n",
    "                '{}_{}_median'.format(f1, f2): 'median',\n",
    "                '{}_{}_mean'.format(f1, f2): 'mean',\n",
    "                '{}_{}_sum'.format(f1, f2): 'sum',\n",
    "                '{}_{}_skew'.format(f1, f2): 'skew',\n",
    "                '{}_{}_std'.format(f1, f2): 'std',\n",
    "                '{}_{}_nunique'.format(f1, f2): 'nunique',\n",
    "                '{}_{}_max_min'.format(f1, f2): max_min,\n",
    "                '{}_{}_quantile_25'.format(f1, f2): lambda x: quantile(x, 0.25),\n",
    "                '{}_{}_quantile_75'.format(f1, f2): lambda x: quantile(x, 0.75)\n",
    "            })\n",
    "            df = df.merge(tmp, on=f1, how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "def arithmetic(df, cross_features):\n",
    "    \"\"\"\n",
    "    数值特征之间的加减乘除\n",
    "    @param df:\n",
    "    @param cross_features: 交叉用的数值特征\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    for i in range(len(cross_features)):\n",
    "        for j in range(i + 1, len(cross_features)):\n",
    "            colname_add = '{}_{}_add'.format(cross_features[i], cross_features[j])\n",
    "            colname_substract = '{}_{}_subtract'.format(cross_features[i], cross_features[j])\n",
    "            colname_multiply = '{}_{}c_multiply'.format(cross_features[i], cross_features[j])\n",
    "            df[colname_add] = df[cross_features[i]] + df[cross_features[j]]\n",
    "            df[colname_substract] = df[cross_features[i]] - df[cross_features[j]]\n",
    "            df[colname_multiply] = df[cross_features[i]] * df[cross_features[j]]\n",
    "\n",
    "    for f1 in cross_features:\n",
    "        for f2 in cross_features:\n",
    "            if f1 != f2:\n",
    "                colname_ratio = '{}_{}_ratio'.format(f1, f2)\n",
    "                df[colname_ratio] = df[f1].values / (df[f2].values + 0.001)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_psi(c, x_train, x_test):\n",
    "    psi_res = pd.DataFrame()\n",
    "    psi_dict={}\n",
    "    # for c in tqdm(f_cols):\n",
    "    try:\n",
    "        t_train = x_train[c].fillna(-998)\n",
    "        t_test = x_test[c].fillna(-998)\n",
    "        # 获取切分点\n",
    "        bins=[]\n",
    "        for i in np.arange(0,1.1,0.2):\n",
    "            bins.append(t_train.quantile(i))\n",
    "        bins=sorted(set(bins))\n",
    "        bins[0]=-np.inf\n",
    "        bins[-1]=np.inf\n",
    "        # 计算psi\n",
    "        t_psi = pd.DataFrame()\n",
    "        t_psi['train'] = pd.cut(t_train,bins).value_counts().sort_index()\n",
    "        t_psi['test'] = pd.cut(t_test,bins).value_counts()\n",
    "        t_psi.index=[str(x) for x in t_psi.index]\n",
    "        t_psi.loc['总计',:] = t_psi.sum()\n",
    "        t_psi['train_rate'] = t_psi['train']/t_psi.loc['总计','train']\n",
    "        t_psi['test_rate'] = t_psi['test']/t_psi.loc['总计','test']\n",
    "        t_psi['psi'] = (t_psi['test_rate']-t_psi['train_rate'])*(np.log(t_psi['test_rate'])-np.log(t_psi['train_rate']))\n",
    "        t_psi.loc['总计','psi'] = t_psi['psi'].sum()\n",
    "        t_psi.index.name=c\n",
    "        #汇总\n",
    "        t_res = pd.DataFrame([[c,t_psi.loc['总计','psi']]],\n",
    "                             columns=['变量名','PSI'])\n",
    "        psi_res = pd.concat([psi_res,t_res])\n",
    "        psi_dict[c]=t_psi\n",
    "        print(c,'done')\n",
    "    except:\n",
    "        print(c,'error')\n",
    "    return psi_res #, psi_dict\n",
    "\n",
    "\n",
    "def auc_select(X_train, y_train, X_valid, y_valid, cols, threshold=0.52):\n",
    "    \"\"\"\n",
    "    基于AUC的单特征筛选\n",
    "    @param X_train:\n",
    "    @param y_train:\n",
    "    @param X_valid:\n",
    "    @param y_valid:\n",
    "    @param cols:\n",
    "    @param threshold:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    useful_dict = dict()\n",
    "    useless_dict = dict()\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 31,\n",
    "        'lambda_l1': 0,\n",
    "        'lambda_l2': 1,\n",
    "        'num_threads': 23,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'first_metric_only': True,\n",
    "        'is_unbalance': True,\n",
    "        'max_depth': -1,\n",
    "        'seed': 2020\n",
    "    }\n",
    "    for i in cols:\n",
    "        print(i)\n",
    "        lgb_train = lgb.Dataset(X_train[[i]].values, y_train)\n",
    "        lgb_valid = lgb.Dataset(X_valid[[i]].values, y_valid, reference=lgb_train)\n",
    "        lgb_model = lgb.train(\n",
    "            params,\n",
    "            lgb_train,\n",
    "            valid_sets=[lgb_valid, lgb_train],\n",
    "            num_boost_round=1000,\n",
    "            early_stopping_rounds=50,\n",
    "            verbose_eval=500\n",
    "        )\n",
    "        print('*' * 10)\n",
    "        print(lgb_model.best_score['valid_0']['auc'])\n",
    "        if lgb_model.best_score['valid_0']['auc'] > threshold:\n",
    "            useful_dict[i] = lgb_model.best_score['valid_0']['auc']\n",
    "        else:\n",
    "            useless_dict[i] = lgb_model.best_score['valid_0']['auc']\n",
    "    useful_cols = list(useful_dict.keys())\n",
    "    useless_cols = list(useless_dict.keys())\n",
    "    return useful_dict, useless_dict, useful_cols, useless_cols\n",
    "\n",
    "\n",
    "def correlation(df, useful_dict, threshold=0.98):\n",
    "    \"\"\"\n",
    "    去除特征相关系数大于阈值的特征\n",
    "    @param df:\n",
    "    @param threshold:\n",
    "    @param useful_dict:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    col_corr = set()\n",
    "    corr_matrix = df.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                colName_i = corr_matrix.columns[i]\n",
    "                colName_j = corr_matrix.columns[j]\n",
    "                if useful_dict[colName_i] >= useful_dict[colName_j]:\n",
    "                    col_corr.add(colName_j)\n",
    "                else:\n",
    "                    col_corr.add(colName_i)\n",
    "    return col_corr\n",
    "\n",
    "\n",
    "def train_func(train_path):\n",
    "    # 请填写训练代码\n",
    "    train = pd.read_csv(train_path)\n",
    "\n",
    "    single_cols = ['appProtocol']\n",
    "    train.drop(single_cols, axis=1, inplace=True)\n",
    "\n",
    "    cat_cols = ['srcAddress', 'destAddress',\n",
    "                'tlsVersion', 'tlsSubject', 'tlsIssuerDn', 'tlsSni']\n",
    "\n",
    "    train['srcAddressPort'] = train['srcAddress'].astype(str) + train['srcPort'].astype(str)\n",
    "    train['destAddressPort'] = train['destAddress'].astype(str) + train['destPort'].astype(str)\n",
    "\n",
    "    str_cols = ['srcAddress', 'destAddress', 'srcAddressPort', 'destAddressPort']\n",
    "    count_nunique = {}\n",
    "    for i in str_cols:\n",
    "        for j in str_cols:\n",
    "            if j == i:\n",
    "                continue\n",
    "            tr = train[i].groupby(train[j]).agg(['count', 'nunique'])\n",
    "            train['{}_gp_{}_count'.format(i, j)] = train[j].map(tr['count'])\n",
    "            train['{}_gp_{}_nunique'.format(i, j)] = train[j].map(tr['nunique'])\n",
    "            count_nunique['{}_gp_{}'.format(i, j)] = tr\n",
    "            train['{}_gp_{}_nunique_rate'.format(i, j)] = (train['{}_gp_{}_nunique'.format(i, j)]\n",
    "                                                           / train['{}_gp_{}_count'.format(i, j)])\n",
    "            train.drop(['{}_gp_{}_count'.format(i, j), '{}_gp_{}_nunique'.format(i, j)], axis=1, inplace=True)\n",
    "    # joblib.dump(res, './res.pkl')\n",
    "    # joblib.load('./res.pkl)\n",
    "    # joblib.dump(count_nunique, './count_nunique.pkl')\n",
    "\n",
    "    # # srcAddress To destAddress\n",
    "    # tmp = train.groupby('srcAddress', as_index=False)['destAddress'].agg({\n",
    "    #     's2d_count': 'count',\n",
    "    #     's2d_nunique': 'nunique'\n",
    "    # })\n",
    "    # train = train.merge(tmp, on='srcAddress', how='left')\n",
    "    #\n",
    "    # # srcAddressPort To destAddressPort\n",
    "    # tmp = train.groupby('srcAddressPort', as_index=False)['destAddressPort'].agg({\n",
    "    #     'sp2dp_count': 'count',\n",
    "    #     'sp2dp_nunique': 'nunique'\n",
    "    # })\n",
    "    # train = train.merge(tmp, on='srcAddressPort', how='left')\n",
    "    #\n",
    "    # # srcAddress To destAddressPort\n",
    "    # tmp = train.groupby('srcAddress', as_index=False)['destAddressPort'].agg({\n",
    "    #     's2dp_count': 'count',\n",
    "    #     's2dp_nunique': 'nunique'\n",
    "    # })\n",
    "    # train = train.merge(tmp, on='srcAddress', how='left')\n",
    "    #\n",
    "    # # srcAddressPort To destAddress\n",
    "    # tmp = train.groupby('srcAddressPort', as_index=False)['destAddress'].agg({\n",
    "    #     'sp2d_count': 'count',\n",
    "    #     'sp2d_nunique': 'nunique'\n",
    "    # })\n",
    "    # train = train.merge(tmp, on='srcAddressPort', how='left')\n",
    "    #\n",
    "    # # destAddress To srcAddress\n",
    "    # tmp = train.groupby('destAddress', as_index=False)['srcAddress'].agg({\n",
    "    #     'd2s_count': 'count',\n",
    "    #     'd2s_nunique': 'nunique'\n",
    "    # })\n",
    "    # train = train.merge(tmp, on='destAddress', how='left')\n",
    "    #\n",
    "    # # destAddressPort To srcAddressPort\n",
    "    # tmp = train.groupby('destAddressPort', as_index=False)['srcAddressPort'].agg({\n",
    "    #     'dp2sp_count': 'count',\n",
    "    #     'dp2sp_nunique': 'nunique'\n",
    "    # })\n",
    "    # train = train.merge(tmp, on='destAddressPort', how='left')\n",
    "    #\n",
    "    # # destAddressPort To srcAddress\n",
    "    # tmp = train.groupby('destAddressPort', as_index=False)['srcAddress'].agg({\n",
    "    #     'dp2s_count': 'count',\n",
    "    #     'dp2s_nunique': 'nunique'\n",
    "    # })\n",
    "    # train = train.merge(tmp, on='destAddressPort', how='left')\n",
    "    #\n",
    "    # # destAddress To srcAddressProt\n",
    "    # tmp = train.groupby('destAddress', as_index=False)['srcAddressPort'].agg({\n",
    "    #     'd2sp_count': 'count',\n",
    "    #     'd2sp_nunique': 'nunique'\n",
    "    # })\n",
    "    # train = train.merge(tmp, on='destAddress', how='left')\n",
    "\n",
    "    tlsVersion_map = {\n",
    "        'TLSv1': 1,\n",
    "        'TLS 1.2': 1,\n",
    "        'TLS 1.3': 1,\n",
    "        'SSLv2': 2,\n",
    "        'SSLv3': 3,\n",
    "        '0x4854': 4,\n",
    "        '0x4752': 4,\n",
    "        'UNDETERMINED': 5\n",
    "    }\n",
    "    train['tlsVersion_map'] = train['tlsVersion'].map(tlsVersion_map)\n",
    "    cat_cols.append('tlsVersion_map')\n",
    "\n",
    "    cat_cols += ['srcAddressPort', 'destAddressPort']\n",
    "    num_cols = ['bytesOut', 'bytesIn', 'pktsIn', 'pktsOut']\n",
    "\n",
    "    for i in num_cols:\n",
    "        train[i] = np.log1p(train[i])\n",
    "    \n",
    "    count_encode = {}\n",
    "    for col in cat_cols:\n",
    "        print(col)\n",
    "        vc = train[col].value_counts(dropna=True, normalize=True)\n",
    "        train[col + '_count'] = train[col].map(vc).astype('float32')\n",
    "        count_encode[col + '_count'] = vc\n",
    "    # joblib.dump(count_encode, './count_encode.pkl')\n",
    "\n",
    "    def max_min(s):\n",
    "        return s.max() - s.min()\n",
    "    def quantile(s, q=0.25):\n",
    "        return s.quantile(q)\n",
    "\n",
    "    cross_encode = {}\n",
    "    for f1 in cat_cols:\n",
    "        for f2 in num_cols:\n",
    "            tmp = train.groupby(f1, as_index=False)[f2].agg({\n",
    "                '{}_{}_count'.format(f1, f2): 'count',\n",
    "                '{}_{}_max'.format(f1, f2): 'max',\n",
    "                '{}_{}_min'.format(f1, f2): 'min',\n",
    "                '{}_{}_median'.format(f1, f2): 'median',\n",
    "                '{}_{}_mean'.format(f1, f2): 'mean',\n",
    "                '{}_{}_sum'.format(f1, f2): 'sum',\n",
    "                '{}_{}_skew'.format(f1, f2): 'skew',\n",
    "                '{}_{}_std'.format(f1, f2): 'std',\n",
    "                '{}_{}_nunique'.format(f1, f2): 'nunique',\n",
    "                '{}_{}_max_min'.format(f1, f2): max_min,\n",
    "                '{}_{}_quantile_25'.format(f1, f2): lambda x: quantile(x, 0.25),\n",
    "                '{}_{}_quantile_75'.format(f1, f2): lambda x: quantile(x, 0.75)\n",
    "            })\n",
    "            train = train.merge(tmp, on=f1, how='left')\n",
    "            cross_encode['{}_stats_{}'.format(f1, f2)] = tmp\n",
    "    # joblib.dump(cross_encode, './cross_encode.pkl')\n",
    "\n",
    "    train = arithmetic(train, num_cols)\n",
    "\n",
    "    cols = train.columns.to_list()\n",
    "    print('Features:\\n', len(cols))\n",
    "    train.to_csv('fe_train.csv', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_path = '../../大数据队_eta_submission_1011/data/train.csv'\n",
    "    train_func(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T09:14:00.314392Z",
     "start_time": "2020-10-26T09:09:51.814125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srcAddress\n",
      "destAddress\n",
      "tlsVersion\n",
      "tlsSubject\n",
      "tlsIssuerDn\n",
      "tlsSni\n",
      "tlsVersion_map\n",
      "srcAddressPort\n",
      "destAddressPort\n",
      "Features:\n",
      " 499\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "\n",
    "# def count_encode(df, cols=[]):\n",
    "#     \"\"\"\n",
    "#     count编码\n",
    "#     @param df:\n",
    "#     @param cols:\n",
    "#     @return:\n",
    "#     \"\"\"\n",
    "#     for col in cols:\n",
    "#         print(col)\n",
    "#         vc = df[col].value_counts(dropna=True, normalize=True)\n",
    "#         df[col + '_count'] = df[col].map(vc).astype('float32')\n",
    "#     return df\n",
    "\n",
    "\n",
    "def cross_cat_num(df, cat_col, num_col):\n",
    "    \"\"\"\n",
    "    类别特征与数据特征groupby统计\n",
    "    @param df:\n",
    "    @param cat_col: 类别特征\n",
    "    @param num_col: 数值特征\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    def max_min(s):\n",
    "        return s.max() - s.min()\n",
    "    def quantile(s, q=0.25):\n",
    "        return s.quantile(q)\n",
    "    for f1 in cat_col:\n",
    "        g = df.groupby(f1, as_index=False)\n",
    "        for f2 in num_col:\n",
    "            tmp = g[f2].agg({\n",
    "                '{}_{}_count'.format(f1, f2): 'count',\n",
    "                '{}_{}_max'.format(f1, f2): 'max',\n",
    "                '{}_{}_min'.format(f1, f2): 'min',\n",
    "                '{}_{}_median'.format(f1, f2): 'median',\n",
    "                '{}_{}_mean'.format(f1, f2): 'mean',\n",
    "                '{}_{}_sum'.format(f1, f2): 'sum',\n",
    "                '{}_{}_skew'.format(f1, f2): 'skew',\n",
    "                '{}_{}_std'.format(f1, f2): 'std',\n",
    "                '{}_{}_nunique'.format(f1, f2): 'nunique',\n",
    "                '{}_{}_max_min'.format(f1, f2): max_min,\n",
    "                '{}_{}_quantile_25'.format(f1, f2): lambda x: quantile(x, 0.25),\n",
    "                '{}_{}_quantile_75'.format(f1, f2): lambda x: quantile(x, 0.75)\n",
    "            })\n",
    "            df = df.merge(tmp, on=f1, how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "def arithmetic(df, cross_features):\n",
    "    \"\"\"\n",
    "    数值特征之间的加减乘除\n",
    "    @param df:\n",
    "    @param cross_features: 交叉用的数值特征\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    for i in range(len(cross_features)):\n",
    "        for j in range(i + 1, len(cross_features)):\n",
    "            colname_add = '{}_{}_add'.format(cross_features[i], cross_features[j])\n",
    "            colname_substract = '{}_{}_subtract'.format(cross_features[i], cross_features[j])\n",
    "            colname_multiply = '{}_{}c_multiply'.format(cross_features[i], cross_features[j])\n",
    "            df[colname_add] = df[cross_features[i]] + df[cross_features[j]]\n",
    "            df[colname_substract] = df[cross_features[i]] - df[cross_features[j]]\n",
    "            df[colname_multiply] = df[cross_features[i]] * df[cross_features[j]]\n",
    "\n",
    "    for f1 in cross_features:\n",
    "        for f2 in cross_features:\n",
    "            if f1 != f2:\n",
    "                colname_ratio = '{}_{}_ratio'.format(f1, f2)\n",
    "                df[colname_ratio] = df[f1].values / (df[f2].values + 0.001)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_psi(c, x_train, x_test):\n",
    "    psi_res = pd.DataFrame()\n",
    "    psi_dict={}\n",
    "    # for c in tqdm(f_cols):\n",
    "    try:\n",
    "        t_train = x_train[c].fillna(-998)\n",
    "        t_test = x_test[c].fillna(-998)\n",
    "        # 获取切分点\n",
    "        bins=[]\n",
    "        for i in np.arange(0,1.1,0.2):\n",
    "            bins.append(t_train.quantile(i))\n",
    "        bins=sorted(set(bins))\n",
    "        bins[0]=-np.inf\n",
    "        bins[-1]=np.inf\n",
    "        # 计算psi\n",
    "        t_psi = pd.DataFrame()\n",
    "        t_psi['train'] = pd.cut(t_train,bins).value_counts().sort_index()\n",
    "        t_psi['test'] = pd.cut(t_test,bins).value_counts()\n",
    "        t_psi.index=[str(x) for x in t_psi.index]\n",
    "        t_psi.loc['总计',:] = t_psi.sum()\n",
    "        t_psi['train_rate'] = t_psi['train']/t_psi.loc['总计','train']\n",
    "        t_psi['test_rate'] = t_psi['test']/t_psi.loc['总计','test']\n",
    "        t_psi['psi'] = (t_psi['test_rate']-t_psi['train_rate'])*(np.log(t_psi['test_rate'])-np.log(t_psi['train_rate']))\n",
    "        t_psi.loc['总计','psi'] = t_psi['psi'].sum()\n",
    "        t_psi.index.name=c\n",
    "        #汇总\n",
    "        t_res = pd.DataFrame([[c,t_psi.loc['总计','psi']]],\n",
    "                             columns=['变量名','PSI'])\n",
    "        psi_res = pd.concat([psi_res,t_res])\n",
    "        psi_dict[c]=t_psi\n",
    "        print(c,'done')\n",
    "    except:\n",
    "        print(c,'error')\n",
    "    return psi_res #, psi_dict\n",
    "\n",
    "\n",
    "def auc_select(X_train, y_train, X_valid, y_valid, cols, threshold=0.52):\n",
    "    \"\"\"\n",
    "    基于AUC的单特征筛选\n",
    "    @param X_train:\n",
    "    @param y_train:\n",
    "    @param X_valid:\n",
    "    @param y_valid:\n",
    "    @param cols:\n",
    "    @param threshold:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    useful_dict = dict()\n",
    "    useless_dict = dict()\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 31,\n",
    "        'lambda_l1': 0,\n",
    "        'lambda_l2': 1,\n",
    "        'num_threads': 23,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'first_metric_only': True,\n",
    "        'is_unbalance': True,\n",
    "        'max_depth': -1,\n",
    "        'seed': 2020\n",
    "    }\n",
    "    for i in cols:\n",
    "        print(i)\n",
    "        lgb_train = lgb.Dataset(X_train[[i]].values, y_train)\n",
    "        lgb_valid = lgb.Dataset(X_valid[[i]].values, y_valid, reference=lgb_train)\n",
    "        lgb_model = lgb.train(\n",
    "            params,\n",
    "            lgb_train,\n",
    "            valid_sets=[lgb_valid, lgb_train],\n",
    "            num_boost_round=1000,\n",
    "            early_stopping_rounds=50,\n",
    "            verbose_eval=500\n",
    "        )\n",
    "        print('*' * 10)\n",
    "        print(lgb_model.best_score['valid_0']['auc'])\n",
    "        if lgb_model.best_score['valid_0']['auc'] > threshold:\n",
    "            useful_dict[i] = lgb_model.best_score['valid_0']['auc']\n",
    "        else:\n",
    "            useless_dict[i] = lgb_model.best_score['valid_0']['auc']\n",
    "    useful_cols = list(useful_dict.keys())\n",
    "    useless_cols = list(useless_dict.keys())\n",
    "    return useful_dict, useless_dict, useful_cols, useless_cols\n",
    "\n",
    "\n",
    "def correlation(df, useful_dict, threshold=0.98):\n",
    "    \"\"\"\n",
    "    去除特征相关系数大于阈值的特征\n",
    "    @param df:\n",
    "    @param threshold:\n",
    "    @param useful_dict:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    col_corr = set()\n",
    "    corr_matrix = df.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                colName_i = corr_matrix.columns[i]\n",
    "                colName_j = corr_matrix.columns[j]\n",
    "                if useful_dict[colName_i] >= useful_dict[colName_j]:\n",
    "                    col_corr.add(colName_j)\n",
    "                else:\n",
    "                    col_corr.add(colName_i)\n",
    "    return col_corr\n",
    "\n",
    "\n",
    "def train_func(train_path):\n",
    "    # 请填写训练代码\n",
    "    train = pd.read_csv(train_path)\n",
    "\n",
    "    single_cols = ['appProtocol']\n",
    "    train.drop(single_cols, axis=1, inplace=True)\n",
    "\n",
    "    cat_cols = ['srcAddress', 'destAddress',\n",
    "                'tlsVersion', 'tlsSubject', 'tlsIssuerDn', 'tlsSni']\n",
    "\n",
    "    train['srcAddressPort'] = train['srcAddress'].astype(str) + train['srcPort'].astype(str)\n",
    "    train['destAddressPort'] = train['destAddress'].astype(str) + train['destPort'].astype(str)\n",
    "\n",
    "    str_cols = ['srcAddress', 'destAddress', 'srcAddressPort', 'destAddressPort']\n",
    "    count_nunique = {}\n",
    "    for i in str_cols:\n",
    "        for j in str_cols:\n",
    "            if j == i:\n",
    "                continue\n",
    "            tr = train[i].groupby(train[j]).agg(['count', 'nunique'])\n",
    "            train['{}_gp_{}_count'.format(i, j)] = train[j].map(tr['count'])\n",
    "            train['{}_gp_{}_nunique'.format(i, j)] = train[j].map(tr['nunique'])\n",
    "            count_nunique['{}_gp_{}'.format(i, j)] = tr\n",
    "            train['{}_gp_{}_nunique_rate'.format(i, j)] = (train['{}_gp_{}_nunique'.format(i, j)]\n",
    "                                                           / train['{}_gp_{}_count'.format(i, j)])\n",
    "            train.drop(['{}_gp_{}_count'.format(i, j), '{}_gp_{}_nunique'.format(i, j)], axis=1, inplace=True)\n",
    "    # joblib.dump(res, './res.pkl')\n",
    "    # joblib.load('./res.pkl)\n",
    "    # joblib.dump(count_nunique, './count_nunique.pkl')\n",
    "\n",
    "    # # srcAddress To destAddress\n",
    "    # tmp = train.groupby('srcAddress', as_index=False)['destAddress'].agg({\n",
    "    #     's2d_count': 'count',\n",
    "    #     's2d_nunique': 'nunique'\n",
    "    # })\n",
    "    # train = train.merge(tmp, on='srcAddress', how='left')\n",
    "    #\n",
    "    # # srcAddressPort To destAddressPort\n",
    "    # tmp = train.groupby('srcAddressPort', as_index=False)['destAddressPort'].agg({\n",
    "    #     'sp2dp_count': 'count',\n",
    "    #     'sp2dp_nunique': 'nunique'\n",
    "    # })\n",
    "    # train = train.merge(tmp, on='srcAddressPort', how='left')\n",
    "    #\n",
    "    # # srcAddress To destAddressPort\n",
    "    # tmp = train.groupby('srcAddress', as_index=False)['destAddressPort'].agg({\n",
    "    #     's2dp_count': 'count',\n",
    "    #     's2dp_nunique': 'nunique'\n",
    "    # })\n",
    "    # train = train.merge(tmp, on='srcAddress', how='left')\n",
    "    #\n",
    "    # # srcAddressPort To destAddress\n",
    "    # tmp = train.groupby('srcAddressPort', as_index=False)['destAddress'].agg({\n",
    "    #     'sp2d_count': 'count',\n",
    "    #     'sp2d_nunique': 'nunique'\n",
    "    # })\n",
    "    # train = train.merge(tmp, on='srcAddressPort', how='left')\n",
    "    #\n",
    "    # # destAddress To srcAddress\n",
    "    # tmp = train.groupby('destAddress', as_index=False)['srcAddress'].agg({\n",
    "    #     'd2s_count': 'count',\n",
    "    #     'd2s_nunique': 'nunique'\n",
    "    # })\n",
    "    # train = train.merge(tmp, on='destAddress', how='left')\n",
    "    #\n",
    "    # # destAddressPort To srcAddressPort\n",
    "    # tmp = train.groupby('destAddressPort', as_index=False)['srcAddressPort'].agg({\n",
    "    #     'dp2sp_count': 'count',\n",
    "    #     'dp2sp_nunique': 'nunique'\n",
    "    # })\n",
    "    # train = train.merge(tmp, on='destAddressPort', how='left')\n",
    "    #\n",
    "    # # destAddressPort To srcAddress\n",
    "    # tmp = train.groupby('destAddressPort', as_index=False)['srcAddress'].agg({\n",
    "    #     'dp2s_count': 'count',\n",
    "    #     'dp2s_nunique': 'nunique'\n",
    "    # })\n",
    "    # train = train.merge(tmp, on='destAddressPort', how='left')\n",
    "    #\n",
    "    # # destAddress To srcAddressProt\n",
    "    # tmp = train.groupby('destAddress', as_index=False)['srcAddressPort'].agg({\n",
    "    #     'd2sp_count': 'count',\n",
    "    #     'd2sp_nunique': 'nunique'\n",
    "    # })\n",
    "    # train = train.merge(tmp, on='destAddress', how='left')\n",
    "\n",
    "    tlsVersion_map = {\n",
    "        'TLSv1': 1,\n",
    "        'TLS 1.2': 1,\n",
    "        'TLS 1.3': 1,\n",
    "        'SSLv2': 2,\n",
    "        'SSLv3': 3,\n",
    "        '0x4854': 4,\n",
    "        '0x4752': 4,\n",
    "        'UNDETERMINED': 5\n",
    "    }\n",
    "    train['tlsVersion_map'] = train['tlsVersion'].map(tlsVersion_map)\n",
    "    cat_cols.append('tlsVersion_map')\n",
    "\n",
    "    cat_cols += ['srcAddressPort', 'destAddressPort']\n",
    "    num_cols = ['bytesOut', 'bytesIn', 'pktsIn', 'pktsOut']\n",
    "    \n",
    "    for i in num_cols:\n",
    "        train[i] = np.log1p(train[i])\n",
    "\n",
    "    count_encode = {}\n",
    "    for col in cat_cols:\n",
    "        print(col)\n",
    "        vc = train[col].value_counts(dropna=True, normalize=True)\n",
    "        train[col + '_count'] = train[col].map(vc).astype('float32')\n",
    "        count_encode[col + '_count'] = vc\n",
    "    # joblib.dump(count_encode, './count_encode.pkl')\n",
    "\n",
    "    def max_min(s):\n",
    "        return s.max() - s.min()\n",
    "    def quantile(s, q=0.25):\n",
    "        return s.quantile(q)\n",
    "\n",
    "    cross_encode = {}\n",
    "    for f1 in cat_cols:\n",
    "        for f2 in num_cols:\n",
    "            tmp = train.groupby(f1, as_index=False)[f2].agg({\n",
    "                '{}_{}_count'.format(f1, f2): 'count',\n",
    "                '{}_{}_max'.format(f1, f2): 'max',\n",
    "                '{}_{}_min'.format(f1, f2): 'min',\n",
    "                '{}_{}_median'.format(f1, f2): 'median',\n",
    "                '{}_{}_mean'.format(f1, f2): 'mean',\n",
    "                '{}_{}_sum'.format(f1, f2): 'sum',\n",
    "                '{}_{}_skew'.format(f1, f2): 'skew',\n",
    "                '{}_{}_std'.format(f1, f2): 'std',\n",
    "                '{}_{}_nunique'.format(f1, f2): 'nunique',\n",
    "                '{}_{}_max_min'.format(f1, f2): max_min,\n",
    "                '{}_{}_quantile_25'.format(f1, f2): lambda x: quantile(x, 0.25),\n",
    "                '{}_{}_quantile_75'.format(f1, f2): lambda x: quantile(x, 0.75)\n",
    "            })\n",
    "            train = train.merge(tmp, on=f1, how='left')\n",
    "            cross_encode['{}_stats_{}'.format(f1, f2)] = tmp\n",
    "    # joblib.dump(cross_encode, './cross_encode.pkl')\n",
    "\n",
    "    train = arithmetic(train, num_cols)\n",
    "\n",
    "    cols = train.columns.to_list()\n",
    "    print('Features:\\n', len(cols))\n",
    "    train.to_csv('fe_test_1.csv', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_path = '../../大数据队_eta_submission_1011/data/test_1.csv'\n",
    "    train_func(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
