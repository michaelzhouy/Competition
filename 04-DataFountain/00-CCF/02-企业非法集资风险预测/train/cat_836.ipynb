{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data60767\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "# !mkdir /home/aistudio/external-libraries\n",
    "# !pip install beautifulsoup4 -t /home/aistudio/external-libraries\n",
    "# !pip install lightgbm==2.3.1 -t /home/aistudio/external-libraries\n",
    "# !pip install catboost==0.23 -t /home/aistudio/external-libraries\n",
    "# !pip uninstall --yes pandas\n",
    "# !pip install pandas==1.0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_encode(df, cat_cols):\r\n",
    "    \"\"\"\r\n",
    "    类别特征的频次编码\r\n",
    "    @param df:\r\n",
    "    @param cat_cols:\r\n",
    "    @return:\r\n",
    "    \"\"\"\r\n",
    "    for col in cat_cols:\r\n",
    "        print(col)\r\n",
    "        vc = df[col].value_counts(dropna=True, normalize=True)\r\n",
    "        df[col + '_count'] = df[col].map(vc).astype('float32')\r\n",
    "    return df\r\n",
    "\r\n",
    "\r\n",
    "def cat_num_stats(df, cat_cols, num_cols):\r\n",
    "    \"\"\"\r\n",
    "    类别特征与数据特征groupby统计特征，简单版\r\n",
    "    @param df:\r\n",
    "    @param cat_cols: 类别特征\r\n",
    "    @param num_cols: 数值特征\r\n",
    "    @return:\r\n",
    "    \"\"\"\r\n",
    "    for f1 in tqdm(cat_cols):\r\n",
    "        g = df.groupby(f1, as_index=False)\r\n",
    "        for f2 in tqdm(num_cols):\r\n",
    "            tmp = g[f2].agg({\r\n",
    "                '{}_{}_max'.format(f1, f2): 'max',\r\n",
    "                '{}_{}_min'.format(f1, f2): 'min',\r\n",
    "                '{}_{}_median'.format(f1, f2): 'median',\r\n",
    "                '{}_{}_mean'.format(f1, f2): 'mean',\r\n",
    "                '{}_{}_sum'.format(f1, f2): 'sum',\r\n",
    "                '{}_{}_skew'.format(f1, f2): 'skew',\r\n",
    "                '{}_{}_std'.format(f1, f2): 'std'\r\n",
    "            })\r\n",
    "            df = df.merge(tmp, on=f1, how='left')\r\n",
    "            del tmp\r\n",
    "            gc.collect()\r\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from tqdm import tqdm\r\n",
    "import json\r\n",
    "import lightgbm as lgb\r\n",
    "from catboost import CatBoostClassifier\r\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "import gc\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data60767/data.csv')\r\n",
    "base_info = pd.read_csv('data/data60767/base_info.csv')\r\n",
    "annual_report_info = pd.read_csv('data/data60767/annual_report_info.csv')\r\n",
    "tax_info = pd.read_csv('data/data60767/tax_info.csv')\r\n",
    "change_info = pd.read_csv('data/data60767/change_info.csv')\r\n",
    "news_info = pd.read_csv('data/data60767/news_info.csv')\r\n",
    "other_info = pd.read_csv('data/data60767/other_info.csv')\r\n",
    "\r\n",
    "data = data.merge(base_info, how='left', on='id')\r\n",
    "data = data.merge(annual_report_info, how='left', on='id')\r\n",
    "data = data.merge(tax_info, how='left', on='id')\r\n",
    "data = data.merge(change_info, how='left', on='id')\r\n",
    "data = data.merge(news_info, how='left', on='id')\r\n",
    "data = data.merge(other_info, how='left', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 13/19 [00:00<00:00, 61.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oplocdistrict\n",
      "industryphy\n",
      "industryco\n",
      "enttype\n",
      "enttypeitem\n",
      "state\n",
      "orgid\n",
      "jobid\n",
      "regtype\n",
      "opform\n",
      "venind\n",
      "enttypeminu\n",
      "oploc\n",
      "enttypegb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 60.05it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 56.61it/s]\n"
     ]
    }
   ],
   "source": [
    "cat_cols = ['oplocdistrict', 'industryphy', 'industryco', 'enttype', 'enttypeitem', 'state', 'orgid', 'jobid', 'regtype', 'opform', 'venind', 'enttypeminu', 'oploc', 'enttypegb']\r\n",
    "two_values = ['adbusign', 'townsign', 'compform', 'protype']\r\n",
    "num_cols = ['empnum', 'parnum', 'exenum', 'regcap', 'reccap', 'forreccap', 'forregcap', 'congro']\r\n",
    "many_cols = ['dom', 'opscope']\r\n",
    "dt_cols = ['opfrom', 'opto']\r\n",
    "null_to_drop = ['midpreindcode', 'ptbusscope', 'protype', 'forreccap', 'congro', 'forregcap', 'exenum', 'parnum']\r\n",
    "imp_to_drop = ['adbusign', 'regtype', 'opform', 'venind', 'oploc', 'state']\r\n",
    "\r\n",
    "cat_cols = [i for i in cat_cols if i not in null_to_drop]\r\n",
    "two_values = [i for i in two_values if i not in null_to_drop]\r\n",
    "num_cols = [i for i in num_cols if i not in null_to_drop]\r\n",
    "\r\n",
    "# cat_cols = [i for i in cat_cols if i not in imp_to_drop]\r\n",
    "# two_values = [i for i in two_values if i not in imp_to_drop]\r\n",
    "# num_cols = [i for i in num_cols if i not in imp_to_drop]\r\n",
    "\r\n",
    "# data.drop(imp_to_drop, axis=1, inplace=True)\r\n",
    "\r\n",
    "data = count_encode(data, cat_cols)\r\n",
    "\r\n",
    "# data = cat_num_stats(data, cat_cols, num_cols)\r\n",
    "\r\n",
    "\r\n",
    "# industryphy_industryco_enttypeminu\r\n",
    "data['industryphy_industryco_enttypeminu'] = data['industryphy'].astype(str) + '_' + data['industryco'].astype(str) + '_' + data['enttypeminu'].astype(str)\r\n",
    "cat_cols.append('industryphy_industryco_enttypeminu')\r\n",
    "\r\n",
    "# enttype_enttypeitem\r\n",
    "data['enttype_enttypeitem'] = data['enttype'].astype(str) + '_' + data['enttypeitem'].astype(str)\r\n",
    "cat_cols.append('enttype_enttypeitem')\r\n",
    "\r\n",
    "# enttypegb_enttype\r\n",
    "data['enttypegb_enttype'] = data['enttypegb'].astype(str) + '_' + data['enttype'].astype(str)\r\n",
    "cat_cols.append('enttypegb_enttype')\r\n",
    "\r\n",
    "data['regcap+reccap'] = data['regcap'] + data['reccap']\r\n",
    "\r\n",
    "for i in tqdm(cat_cols + many_cols):\r\n",
    "    le = LabelEncoder()\r\n",
    "    data[i] = le.fit_transform(data[i].astype(str))\r\n",
    "\r\n",
    "for i in tqdm(cat_cols + many_cols):\r\n",
    "    data[i] = data[i].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stat(df, df_merge, group_by, agg):\r\n",
    "    group = df.groupby(group_by).agg(agg)\r\n",
    "\r\n",
    "    columns = []\r\n",
    "    for on, methods in agg.items():\r\n",
    "        for method in methods:\r\n",
    "            columns.append('{}_{}_{}'.format('_'.join(group_by), on, method))\r\n",
    "    group.columns = columns\r\n",
    "    group.reset_index(inplace=True)\r\n",
    "    df_merge = df_merge.merge(group, on=group_by, how='left')\r\n",
    "\r\n",
    "    del (group)\r\n",
    "    gc.collect()\r\n",
    "    return df_merge\r\n",
    "\r\n",
    "\r\n",
    "def statis_feat(df_know, df_unknow):\r\n",
    "    df_unknow = stat(df_know, df_unknow, ['industryphy'], {'label': ['mean']})\r\n",
    "    df_unknow = stat(df_know, df_unknow, ['industryco'], {'label': ['mean']})\r\n",
    "    df_unknow = stat(df_know, df_unknow, ['industryphy', 'industryco'], {'label': ['mean']})\r\n",
    "    df_unknow = stat(df_know, df_unknow, ['enttypegb'], {'label': ['mean']})\r\n",
    "    df_unknow = stat(df_know, df_unknow, ['enttype'], {'label': ['mean']})\r\n",
    "    df_unknow = stat(df_know, df_unknow, ['enttypegb', 'enttype'], {'label': ['mean']})\r\n",
    "    # df_unknow = stat(df_know, df_unknow, ['age', 'op_device'], {'label': ['mean']})\r\n",
    "    # df_unknow = stat(df_know, df_unknow, ['using_time'], {'label': ['mean']})\r\n",
    "    # df_unknow = stat(df_know, df_unknow, ['city', 'op_device'], {'label': ['mean']})\r\n",
    "    # df_unknow = stat(df_know, df_unknow, ['age', 'city'], {'label': ['mean']})\r\n",
    "    # df_unknow = stat(df_know, df_unknow, ['op_device', 'level'], {'label': ['mean']})\r\n",
    "\r\n",
    "    return df_unknow\r\n",
    "\r\n",
    "\r\n",
    "df_train = data[data['label'].notnull()]\r\n",
    "df_test = data[data['label'].isnull()]\r\n",
    "\r\n",
    "df_stas_feat = None\r\n",
    "kf = StratifiedKFold(n_splits=5, random_state=1024, shuffle=True)\r\n",
    "for train_index, val_index in kf.split(df_train, df_train['label']):\r\n",
    "    df_fold_train = df_train.iloc[train_index]\r\n",
    "    df_fold_val = df_train.iloc[val_index]\r\n",
    "\r\n",
    "    df_fold_val = statis_feat(df_fold_train, df_fold_val)\r\n",
    "    df_stas_feat = pd.concat([df_stas_feat, df_fold_val], axis=0)\r\n",
    "\r\n",
    "    del (df_fold_train)\r\n",
    "    del (df_fold_val)\r\n",
    "    gc.collect()\r\n",
    "\r\n",
    "df_test = statis_feat(df_train, df_test)\r\n",
    "df_feature = pd.concat([df_stas_feat, df_test], axis=0)\r\n",
    "# df_feature = df_feature.reset_index(drop=True)\r\n",
    "\r\n",
    "del (df_stas_feat)\r\n",
    "del (df_train)\r\n",
    "del (df_test)\r\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp_drop = ['bgxmdm_cnt_median', 'UNENUM', 'positive_negtive_0', 'PUBSTATE', 'state_count', 'positive_negtive_mean', 'bgxmdm_190.0', 'compform', 'BUSSTNAME', 'UNEEMPLNUM', 'bgxmdm_117.0', 'regtype_count', 'DISPERNUM+DISEMPLNUM', 'ANCHEYEAR', 'COLGRANUM', 'bgxmdm_cnt_min', 'legal_judgment_num', 'positive_negtive_1', 'RETEMPLNUM', 'brand_num', 'RETSOLNUM+RETEMPLNUM', 'STOCKTRANSIGN', 'bgxmdm_129.0', 'FUNDAM', 'patent_num']\r\n",
    "df_feature.drop(imp_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.8002602\ttest: 0.8091603\tbest: 0.8091603 (0)\ttotal: 120ms\tremaining: 3h 20m 24s\n",
      "500:\tlearn: 0.8719212\ttest: 0.8459658\tbest: 0.8480392 (331)\ttotal: 49.3s\tremaining: 2h 43m 3s\n",
      "1000:\tlearn: 0.9003096\ttest: 0.8446602\tbest: 0.8536585 (771)\ttotal: 1m 37s\tremaining: 2h 40m 48s\n",
      "bestTest = 0.8536585366\n",
      "bestIteration = 771\n",
      "Shrink model to first 772 iterations.\n",
      "0:\tlearn: 0.7963446\ttest: 0.8060453\tbest: 0.8060453 (0)\ttotal: 95.6ms\tremaining: 2h 39m 22s\n",
      "500:\tlearn: 0.8731026\ttest: 0.8390244\tbest: 0.8390244 (440)\ttotal: 48.6s\tremaining: 2h 40m 57s\n",
      "1000:\tlearn: 0.9078624\ttest: 0.8472906\tbest: 0.8493827 (750)\ttotal: 1m 38s\tremaining: 2h 42m 18s\n",
      "bestTest = 0.849382716\n",
      "bestIteration = 750\n",
      "Shrink model to first 751 iterations.\n",
      "0:\tlearn: 0.7994809\ttest: 0.8040712\tbest: 0.8040712 (0)\ttotal: 91.7ms\tremaining: 2h 32m 53s\n",
      "500:\tlearn: 0.8665425\ttest: 0.8402948\tbest: 0.8402948 (419)\ttotal: 50.8s\tremaining: 2h 48m 6s\n",
      "bestTest = 0.8402948403\n",
      "bestIteration = 419\n",
      "Shrink model to first 420 iterations.\n",
      "0:\tlearn: 0.8020632\ttest: 0.8041775\tbest: 0.8041775 (0)\ttotal: 97.8ms\tremaining: 2h 42m 55s\n",
      "500:\tlearn: 0.8783200\ttest: 0.8421053\tbest: 0.8471178 (362)\ttotal: 48.7s\tremaining: 2h 41m 6s\n",
      "1000:\tlearn: 0.8977556\ttest: 0.8442211\tbest: 0.8492462 (673)\ttotal: 1m 36s\tremaining: 2h 39m 4s\n",
      "bestTest = 0.8492462312\n",
      "bestIteration = 673\n",
      "Shrink model to first 674 iterations.\n",
      "0:\tlearn: 0.8053777\ttest: 0.7937337\tbest: 0.7937337 (0)\ttotal: 96ms\tremaining: 2h 40m\n",
      "500:\tlearn: 0.8824257\ttest: 0.8463476\tbest: 0.8463476 (414)\ttotal: 49.5s\tremaining: 2h 43m 44s\n",
      "bestTest = 0.8463476071\n",
      "bestIteration = 414\n",
      "Shrink model to first 415 iterations.\n",
      "OOF-MEAN-F1:0.847786, OOF-STD-F1:0.004413\n",
      "feature importance:\n",
      "['industryphy_count', 'positive_negtive_max', 'bgxmdm_cnt_max', 'rate_1', 'bgxmdm_121.0', 'rate_0', 'bgxmdm_133.0', 'oploc_count', 'bgxmdm_115.0', 'bgxmdm_930.0', 'bgxmdm_137.0', 'EMPNUMSIGN', 'bgxmdm_939.0']\n",
      "                         feature  importance  normalized_importance  \\\n",
      "49                   industryphy    7.570602               0.075706   \n",
      "40  enttypegb_enttype_label_mean    7.134069               0.071341   \n",
      "53        industryphy_label_mean    6.544912               0.065449   \n",
      "41          enttypegb_label_mean    6.434207               0.064342   \n",
      "48         industryco_label_mean    4.086963               0.040870   \n",
      "..                           ...         ...                    ...   \n",
      "15                  bgxmdm_115.0    0.069498               0.000695   \n",
      "22                  bgxmdm_930.0    0.064376               0.000644   \n",
      "21                  bgxmdm_137.0    0.055705               0.000557   \n",
      "6                     EMPNUMSIGN    0.046744               0.000467   \n",
      "23                  bgxmdm_939.0    0.024640               0.000246   \n",
      "\n",
      "    cumulative_importance  \n",
      "49               0.075706  \n",
      "40               0.147047  \n",
      "53               0.212496  \n",
      "41               0.276838  \n",
      "48               0.317708  \n",
      "..                    ...  \n",
      "15               0.998085  \n",
      "22               0.998729  \n",
      "21               0.999286  \n",
      "6                0.999754  \n",
      "23               1.000000  \n",
      "\n",
      "[82 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "train = df_feature[df_feature['label'].notnull()]\r\n",
    "test = df_feature[df_feature['label'].isnull()]\r\n",
    "sub = test[['id']]\r\n",
    "# train.shape, test.shape\r\n",
    "\r\n",
    "used_cols = [i for i in train.columns if i not in ['id', 'label', 'opfrom', 'opto']]\r\n",
    "y = train['label']\r\n",
    "train = train[used_cols]\r\n",
    "test = test[used_cols]\r\n",
    "\r\n",
    "\r\n",
    "num_folds=5\r\n",
    "kfold = StratifiedKFold(n_splits=num_folds, random_state=1024, shuffle=True)\r\n",
    "\r\n",
    "oof_probs = np.zeros(train.shape[0])\r\n",
    "output_probs = np.zeros((test.shape[0], 5))\r\n",
    "offline_score = []\r\n",
    "feature_importance_df = pd.DataFrame()\r\n",
    "\r\n",
    "for fold, (train_idx, valid_idx) in enumerate(kfold.split(train, y)):\r\n",
    "    X_train, y_train = train.iloc[train_idx], y.iloc[train_idx]\r\n",
    "    X_valid, y_valid = train.iloc[valid_idx], y.iloc[valid_idx]\r\n",
    "    \r\n",
    "    model=CatBoostClassifier(\r\n",
    "        loss_function=\"Logloss\",\r\n",
    "        eval_metric=\"F1\",\r\n",
    "        task_type=\"GPU\",\r\n",
    "        learning_rate=0.01,\r\n",
    "        iterations=100000,\r\n",
    "        random_seed=2020,\r\n",
    "        od_type=\"Iter\",\r\n",
    "        depth=8,\r\n",
    "        early_stopping_rounds=500\r\n",
    "    )\r\n",
    "\r\n",
    "    clf = model.fit(X_train, y_train, eval_set=(X_valid,y_valid), verbose=500, cat_features=cat_cols)\r\n",
    "    yy_pred_valid=clf.predict(X_valid)\r\n",
    "    y_pred_valid = clf.predict(X_valid, prediction_type='Probability')[:, -1]\r\n",
    "    oof_probs[valid_idx] = y_pred_valid\r\n",
    "    offline_score.append(f1_score(y_valid, yy_pred_valid))\r\n",
    "    output_probs[:, fold] = clf.predict(test, prediction_type='Probability')[:,-1]\r\n",
    "    \r\n",
    "    # feature importance\r\n",
    "    fold_importance_df = pd.DataFrame()\r\n",
    "    fold_importance_df[\"feature\"] = model.feature_names_\r\n",
    "    fold_importance_df[\"importance\"] = model.feature_importances_\r\n",
    "    fold_importance_df[\"fold\"] = fold + 1\r\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\r\n",
    "\r\n",
    "print('OOF-MEAN-F1:%.6f, OOF-STD-F1:%.6f' % (np.mean(offline_score), np.std(offline_score)))\r\n",
    "print('feature importance:')\r\n",
    "feature_importance_df_ = feature_importance_df.groupby('feature', as_index=False)['importance'].mean().sort_values(by='importance', ascending=False)\r\n",
    "feature_importance_df_['normalized_importance'] = feature_importance_df_['importance'] / feature_importance_df_['importance'].sum()\r\n",
    "feature_importance_df_['cumulative_importance'] = np.cumsum(feature_importance_df_['normalized_importance'])\r\n",
    "record_low_importance = feature_importance_df_[feature_importance_df_['cumulative_importance'] > 0.99]\r\n",
    "to_drop = list(record_low_importance['feature'])\r\n",
    "print(to_drop)\r\n",
    "# print(feature_importance_df_.head(15))\r\n",
    "print(feature_importance_df_)\r\n",
    "# feature_importance_df_.to_csv(\"./importance.csv\")\r\n",
    "\r\n",
    "sub['score'] = np.mean(output_probs, axis=1)\r\n",
    "# print(sub['score'])\r\n",
    "sub.to_csv('cat_sub_{}_{}.csv'.format(time.strftime('%Y%m%d'), np.mean(offline_score)), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
