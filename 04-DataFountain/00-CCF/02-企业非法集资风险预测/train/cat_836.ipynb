{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data60767\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "# !mkdir /home/aistudio/external-libraries\n",
    "# !pip install beautifulsoup4 -t /home/aistudio/external-libraries\n",
    "# !pip install lightgbm==2.3.1 -t /home/aistudio/external-libraries\n",
    "# !pip install catboost==0.23 -t /home/aistudio/external-libraries\n",
    "# !pip uninstall --yes pandas\n",
    "# !pip install pandas==1.0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_encode(df, cat_cols):\n",
    "    \"\"\"\n",
    "    类别特征的频次编码\n",
    "    @param df:\n",
    "    @param cat_cols:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    for col in cat_cols:\n",
    "        print(col)\n",
    "        vc = df[col].value_counts(dropna=True, normalize=True)\n",
    "        df[col + '_count'] = df[col].map(vc).astype('float32')\n",
    "    return df\n",
    "\n",
    "\n",
    "def cat_num_stats(df, cat_cols, num_cols):\n",
    "    \"\"\"\n",
    "    类别特征与数据特征groupby统计特征，简单版\n",
    "    @param df:\n",
    "    @param cat_cols: 类别特征\n",
    "    @param num_cols: 数值特征\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    for f1 in tqdm(cat_cols):\n",
    "        g = df.groupby(f1, as_index=False)\n",
    "        for f2 in tqdm(num_cols):\n",
    "            tmp = g[f2].agg({\n",
    "                '{}_{}_max'.format(f1, f2): 'max',\n",
    "                '{}_{}_min'.format(f1, f2): 'min',\n",
    "                '{}_{}_median'.format(f1, f2): 'median',\n",
    "                '{}_{}_mean'.format(f1, f2): 'mean',\n",
    "                '{}_{}_sum'.format(f1, f2): 'sum',\n",
    "                '{}_{}_skew'.format(f1, f2): 'skew',\n",
    "                '{}_{}_std'.format(f1, f2): 'std'\n",
    "            })\n",
    "            df = df.merge(tmp, on=f1, how='left')\n",
    "            del tmp\n",
    "            gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data60767/data.csv')\n",
    "base_info = pd.read_csv('data/data60767/base_info.csv')\n",
    "annual_report_info = pd.read_csv('data/data60767/annual_report_info.csv')\n",
    "tax_info = pd.read_csv('data/data60767/tax_info.csv')\n",
    "change_info = pd.read_csv('data/data60767/change_info.csv')\n",
    "news_info = pd.read_csv('data/data60767/news_info.csv')\n",
    "other_info = pd.read_csv('data/data60767/other_info.csv')\n",
    "\n",
    "data = data.merge(base_info, how='left', on='id')\n",
    "data = data.merge(annual_report_info, how='left', on='id')\n",
    "data = data.merge(tax_info, how='left', on='id')\n",
    "data = data.merge(change_info, how='left', on='id')\n",
    "data = data.merge(news_info, how='left', on='id')\n",
    "data = data.merge(other_info, how='left', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 13/19 [00:00<00:00, 61.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oplocdistrict\n",
      "industryphy\n",
      "industryco\n",
      "enttype\n",
      "enttypeitem\n",
      "state\n",
      "orgid\n",
      "jobid\n",
      "regtype\n",
      "opform\n",
      "venind\n",
      "enttypeminu\n",
      "oploc\n",
      "enttypegb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 60.05it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 56.61it/s]\n"
     ]
    }
   ],
   "source": [
    "cat_cols = ['oplocdistrict', 'industryphy', 'industryco', 'enttype', 'enttypeitem', 'state', 'orgid', 'jobid', 'regtype', 'opform', 'venind', 'enttypeminu', 'oploc', 'enttypegb']\n",
    "two_values = ['adbusign', 'townsign', 'compform', 'protype']\n",
    "num_cols = ['empnum', 'parnum', 'exenum', 'regcap', 'reccap', 'forreccap', 'forregcap', 'congro']\n",
    "many_cols = ['dom', 'opscope']\n",
    "dt_cols = ['opfrom', 'opto']\n",
    "null_to_drop = ['midpreindcode', 'ptbusscope', 'protype', 'forreccap', 'congro', 'forregcap', 'exenum', 'parnum']\n",
    "imp_to_drop = ['adbusign', 'regtype', 'opform', 'venind', 'oploc', 'state']\n",
    "\n",
    "cat_cols = [i for i in cat_cols if i not in null_to_drop]\n",
    "two_values = [i for i in two_values if i not in null_to_drop]\n",
    "num_cols = [i for i in num_cols if i not in null_to_drop]\n",
    "\n",
    "# cat_cols = [i for i in cat_cols if i not in imp_to_drop]\n",
    "# two_values = [i for i in two_values if i not in imp_to_drop]\n",
    "# num_cols = [i for i in num_cols if i not in imp_to_drop]\n",
    "\n",
    "# data.drop(imp_to_drop, axis=1, inplace=True)\n",
    "\n",
    "data = count_encode(data, cat_cols)\n",
    "\n",
    "# data = cat_num_stats(data, cat_cols, num_cols)\n",
    "\n",
    "\n",
    "# industryphy_industryco_enttypeminu\n",
    "data['industryphy_industryco_enttypeminu'] = data['industryphy'].astype(str) + '_' + data['industryco'].astype(str) + '_' + data['enttypeminu'].astype(str)\n",
    "cat_cols.append('industryphy_industryco_enttypeminu')\n",
    "\n",
    "# enttype_enttypeitem\n",
    "data['enttype_enttypeitem'] = data['enttype'].astype(str) + '_' + data['enttypeitem'].astype(str)\n",
    "cat_cols.append('enttype_enttypeitem')\n",
    "\n",
    "# enttypegb_enttype\n",
    "data['enttypegb_enttype'] = data['enttypegb'].astype(str) + '_' + data['enttype'].astype(str)\n",
    "cat_cols.append('enttypegb_enttype')\n",
    "\n",
    "data['regcap+reccap'] = data['regcap'] + data['reccap']\n",
    "\n",
    "for i in tqdm(cat_cols + many_cols):\n",
    "    le = LabelEncoder()\n",
    "    data[i] = le.fit_transform(data[i].astype(str))\n",
    "\n",
    "for i in tqdm(cat_cols + many_cols):\n",
    "    data[i] = data[i].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stat(df, df_merge, group_by, agg):\n",
    "    group = df.groupby(group_by).agg(agg)\n",
    "\n",
    "    columns = []\n",
    "    for on, methods in agg.items():\n",
    "        for method in methods:\n",
    "            columns.append('{}_{}_{}'.format('_'.join(group_by), on, method))\n",
    "    group.columns = columns\n",
    "    group.reset_index(inplace=True)\n",
    "    df_merge = df_merge.merge(group, on=group_by, how='left')\n",
    "\n",
    "    del (group)\n",
    "    gc.collect()\n",
    "    return df_merge\n",
    "\n",
    "\n",
    "def statis_feat(df_know, df_unknow):\n",
    "    df_unknow = stat(df_know, df_unknow, ['industryphy'], {'label': ['mean']})\n",
    "    df_unknow = stat(df_know, df_unknow, ['industryco'], {'label': ['mean']})\n",
    "    df_unknow = stat(df_know, df_unknow, ['industryphy', 'industryco'], {'label': ['mean']})\n",
    "    df_unknow = stat(df_know, df_unknow, ['enttypegb'], {'label': ['mean']})\n",
    "    df_unknow = stat(df_know, df_unknow, ['enttype'], {'label': ['mean']})\n",
    "    df_unknow = stat(df_know, df_unknow, ['enttypegb', 'enttype'], {'label': ['mean']})\n",
    "    # df_unknow = stat(df_know, df_unknow, ['age', 'op_device'], {'label': ['mean']})\n",
    "    # df_unknow = stat(df_know, df_unknow, ['using_time'], {'label': ['mean']})\n",
    "    # df_unknow = stat(df_know, df_unknow, ['city', 'op_device'], {'label': ['mean']})\n",
    "    # df_unknow = stat(df_know, df_unknow, ['age', 'city'], {'label': ['mean']})\n",
    "    # df_unknow = stat(df_know, df_unknow, ['op_device', 'level'], {'label': ['mean']})\n",
    "\n",
    "    return df_unknow\n",
    "\n",
    "\n",
    "df_train = data[data['label'].notnull()]\n",
    "df_test = data[data['label'].isnull()]\n",
    "\n",
    "df_stas_feat = None\n",
    "kf = StratifiedKFold(n_splits=5, random_state=1024, shuffle=True)\n",
    "for train_index, val_index in kf.split(df_train, df_train['label']):\n",
    "    df_fold_train = df_train.iloc[train_index]\n",
    "    df_fold_val = df_train.iloc[val_index]\n",
    "\n",
    "    df_fold_val = statis_feat(df_fold_train, df_fold_val)\n",
    "    df_stas_feat = pd.concat([df_stas_feat, df_fold_val], axis=0)\n",
    "\n",
    "    del (df_fold_train)\n",
    "    del (df_fold_val)\n",
    "    gc.collect()\n",
    "\n",
    "df_test = statis_feat(df_train, df_test)\n",
    "df_feature = pd.concat([df_stas_feat, df_test], axis=0)\n",
    "# df_feature = df_feature.reset_index(drop=True)\n",
    "\n",
    "del (df_stas_feat)\n",
    "del (df_train)\n",
    "del (df_test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_drop = ['bgxmdm_cnt_median', 'UNENUM', 'positive_negtive_0', 'PUBSTATE', 'state_count', 'positive_negtive_mean', 'bgxmdm_190.0', 'compform', 'BUSSTNAME', 'UNEEMPLNUM', 'bgxmdm_117.0', 'regtype_count', 'DISPERNUM+DISEMPLNUM', 'ANCHEYEAR', 'COLGRANUM', 'bgxmdm_cnt_min', 'legal_judgment_num', 'positive_negtive_1', 'RETEMPLNUM', 'brand_num', 'RETSOLNUM+RETEMPLNUM', 'STOCKTRANSIGN', 'bgxmdm_129.0', 'FUNDAM', 'patent_num']\n",
    "df_feature.drop(imp_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.8002602\ttest: 0.8091603\tbest: 0.8091603 (0)\ttotal: 120ms\tremaining: 3h 20m 24s\n",
      "500:\tlearn: 0.8719212\ttest: 0.8459658\tbest: 0.8480392 (331)\ttotal: 49.3s\tremaining: 2h 43m 3s\n",
      "1000:\tlearn: 0.9003096\ttest: 0.8446602\tbest: 0.8536585 (771)\ttotal: 1m 37s\tremaining: 2h 40m 48s\n",
      "bestTest = 0.8536585366\n",
      "bestIteration = 771\n",
      "Shrink model to first 772 iterations.\n",
      "0:\tlearn: 0.7963446\ttest: 0.8060453\tbest: 0.8060453 (0)\ttotal: 95.6ms\tremaining: 2h 39m 22s\n",
      "500:\tlearn: 0.8731026\ttest: 0.8390244\tbest: 0.8390244 (440)\ttotal: 48.6s\tremaining: 2h 40m 57s\n",
      "1000:\tlearn: 0.9078624\ttest: 0.8472906\tbest: 0.8493827 (750)\ttotal: 1m 38s\tremaining: 2h 42m 18s\n",
      "bestTest = 0.849382716\n",
      "bestIteration = 750\n",
      "Shrink model to first 751 iterations.\n",
      "0:\tlearn: 0.7994809\ttest: 0.8040712\tbest: 0.8040712 (0)\ttotal: 91.7ms\tremaining: 2h 32m 53s\n",
      "500:\tlearn: 0.8665425\ttest: 0.8402948\tbest: 0.8402948 (419)\ttotal: 50.8s\tremaining: 2h 48m 6s\n",
      "bestTest = 0.8402948403\n",
      "bestIteration = 419\n",
      "Shrink model to first 420 iterations.\n",
      "0:\tlearn: 0.8020632\ttest: 0.8041775\tbest: 0.8041775 (0)\ttotal: 97.8ms\tremaining: 2h 42m 55s\n",
      "500:\tlearn: 0.8783200\ttest: 0.8421053\tbest: 0.8471178 (362)\ttotal: 48.7s\tremaining: 2h 41m 6s\n",
      "1000:\tlearn: 0.8977556\ttest: 0.8442211\tbest: 0.8492462 (673)\ttotal: 1m 36s\tremaining: 2h 39m 4s\n",
      "bestTest = 0.8492462312\n",
      "bestIteration = 673\n",
      "Shrink model to first 674 iterations.\n",
      "0:\tlearn: 0.8053777\ttest: 0.7937337\tbest: 0.7937337 (0)\ttotal: 96ms\tremaining: 2h 40m\n",
      "500:\tlearn: 0.8824257\ttest: 0.8463476\tbest: 0.8463476 (414)\ttotal: 49.5s\tremaining: 2h 43m 44s\n",
      "bestTest = 0.8463476071\n",
      "bestIteration = 414\n",
      "Shrink model to first 415 iterations.\n",
      "OOF-MEAN-F1:0.847786, OOF-STD-F1:0.004413\n",
      "feature importance:\n",
      "['industryphy_count', 'positive_negtive_max', 'bgxmdm_cnt_max', 'rate_1', 'bgxmdm_121.0', 'rate_0', 'bgxmdm_133.0', 'oploc_count', 'bgxmdm_115.0', 'bgxmdm_930.0', 'bgxmdm_137.0', 'EMPNUMSIGN', 'bgxmdm_939.0']\n",
      "                         feature  importance  normalized_importance  \\\n",
      "49                   industryphy    7.570602               0.075706   \n",
      "40  enttypegb_enttype_label_mean    7.134069               0.071341   \n",
      "53        industryphy_label_mean    6.544912               0.065449   \n",
      "41          enttypegb_label_mean    6.434207               0.064342   \n",
      "48         industryco_label_mean    4.086963               0.040870   \n",
      "..                           ...         ...                    ...   \n",
      "15                  bgxmdm_115.0    0.069498               0.000695   \n",
      "22                  bgxmdm_930.0    0.064376               0.000644   \n",
      "21                  bgxmdm_137.0    0.055705               0.000557   \n",
      "6                     EMPNUMSIGN    0.046744               0.000467   \n",
      "23                  bgxmdm_939.0    0.024640               0.000246   \n",
      "\n",
      "    cumulative_importance  \n",
      "49               0.075706  \n",
      "40               0.147047  \n",
      "53               0.212496  \n",
      "41               0.276838  \n",
      "48               0.317708  \n",
      "..                    ...  \n",
      "15               0.998085  \n",
      "22               0.998729  \n",
      "21               0.999286  \n",
      "6                0.999754  \n",
      "23               1.000000  \n",
      "\n",
      "[82 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "train = df_feature[df_feature['label'].notnull()]\n",
    "test = df_feature[df_feature['label'].isnull()]\n",
    "sub = test[['id']]\n",
    "# train.shape, test.shape\n",
    "\n",
    "used_cols = [i for i in train.columns if i not in ['id', 'label', 'opfrom', 'opto']]\n",
    "y = train['label']\n",
    "train = train[used_cols]\n",
    "test = test[used_cols]\n",
    "\n",
    "\n",
    "num_folds=5\n",
    "kfold = StratifiedKFold(n_splits=num_folds, random_state=1024, shuffle=True)\n",
    "\n",
    "oof_probs = np.zeros(train.shape[0])\n",
    "output_probs = np.zeros((test.shape[0], 5))\n",
    "offline_score = []\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kfold.split(train, y)):\n",
    "    X_train, y_train = train.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_valid, y_valid = train.iloc[valid_idx], y.iloc[valid_idx]\n",
    "    \n",
    "    model=CatBoostClassifier(\n",
    "        loss_function=\"Logloss\",\n",
    "        eval_metric=\"F1\",\n",
    "        task_type=\"GPU\",\n",
    "        learning_rate=0.01,\n",
    "        iterations=100000,\n",
    "        random_seed=2020,\n",
    "        od_type=\"Iter\",\n",
    "        depth=8,\n",
    "        early_stopping_rounds=500\n",
    "    )\n",
    "\n",
    "    clf = model.fit(X_train, y_train, eval_set=(X_valid,y_valid), verbose=500, cat_features=cat_cols)\n",
    "    yy_pred_valid=clf.predict(X_valid)\n",
    "    y_pred_valid = clf.predict(X_valid, prediction_type='Probability')[:, -1]\n",
    "    oof_probs[valid_idx] = y_pred_valid\n",
    "    offline_score.append(f1_score(y_valid, yy_pred_valid))\n",
    "    output_probs[:, fold] = clf.predict(test, prediction_type='Probability')[:,-1]\n",
    "    \n",
    "    # feature importance\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = model.feature_names_\n",
    "    fold_importance_df[\"importance\"] = model.feature_importances_\n",
    "    fold_importance_df[\"fold\"] = fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "print('OOF-MEAN-F1:%.6f, OOF-STD-F1:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
    "print('feature importance:')\n",
    "feature_importance_df_ = feature_importance_df.groupby('feature', as_index=False)['importance'].mean().sort_values(by='importance', ascending=False)\n",
    "feature_importance_df_['normalized_importance'] = feature_importance_df_['importance'] / feature_importance_df_['importance'].sum()\n",
    "feature_importance_df_['cumulative_importance'] = np.cumsum(feature_importance_df_['normalized_importance'])\n",
    "record_low_importance = feature_importance_df_[feature_importance_df_['cumulative_importance'] > 0.99]\n",
    "to_drop = list(record_low_importance['feature'])\n",
    "print(to_drop)\n",
    "# print(feature_importance_df_.head(15))\n",
    "print(feature_importance_df_)\n",
    "# feature_importance_df_.to_csv(\"./importance.csv\")\n",
    "\n",
    "sub['score'] = np.mean(output_probs, axis=1)\n",
    "# print(sub['score'])\n",
    "sub.to_csv('cat_sub_{}_{}.csv'.format(time.strftime('%Y%m%d'), np.mean(offline_score)), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
