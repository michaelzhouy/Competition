{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data63881\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/aistudio/external-libraries’: File exists\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting lightgbm==2.3.1\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/0b/9d/ddcb2f43aca194987f1a99e27edf41cf9bc39ea750c3371c2a62698c509a/lightgbm-2.3.1-py2.py3-none-manylinux1_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 9.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn (from lightgbm==2.3.1)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/f4/cb/64623369f348e9bfb29ff898a57ac7c91ed4921f228e9726546614d63ccb/scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8MB 13.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy (from lightgbm==2.3.1)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/dc/7e/8f6a79b102ca1ea928bae8998b05bf5dc24a90571db13cd119f275ba6252/scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9MB 8.6MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting numpy (from lightgbm==2.3.1)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/a5/bb/87d668b353848b93baab0a64cddf6408c40717f099539668c3d26fe39f7e/numpy-1.19.4-cp37-cp37m-manylinux2010_x86_64.whl (14.5MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5MB 9.9MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0 (from scikit-learn->lightgbm==2.3.1)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "Collecting joblib>=0.11 (from scikit-learn->lightgbm==2.3.1)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/fc/c9/f58220ac44a1592f79a343caba12f6837f9e0c04c196176a3d66338e1ea8/joblib-0.17.0-py3-none-any.whl (301kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 23.8MB/s eta 0:00:01\n",
      "\u001b[31mERROR: seaborn 0.10.0 requires pandas>=0.22.0, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: paddlehub 1.6.0 requires pandas; python_version >= \"3\", which is not installed.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn, lightgbm\n",
      "Successfully installed joblib-0.17.0 lightgbm-2.3.1 numpy-1.19.4 scikit-learn-0.23.2 scipy-1.5.4 threadpoolctl-2.1.0\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/numpy-1.19.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/scipy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/scipy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/joblib-0.17.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/scipy-1.5.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/scikit_learn.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/joblib already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/threadpoolctl.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/lightgbm already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/scikit_learn-0.23.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/threadpoolctl-2.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/sklearn already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/lightgbm-2.3.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Skipping pandas as it is not installed.\u001b[0m\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting pandas==1.0.5\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/af/f3/683bf2547a3eaeec15b39cef86f61e921b3b187f250fcd2b5c5fb4386369/pandas-1.0.5-cp37-cp37m-manylinux1_x86_64.whl (10.1MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1MB 14.1MB/s eta 0:00:01    |███████████████▋                | 4.9MB 14.1MB/s eta 0:00:01     |██████████████████████▋         | 7.2MB 14.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.13.3 (from pandas==1.0.5)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/a5/bb/87d668b353848b93baab0a64cddf6408c40717f099539668c3d26fe39f7e/numpy-1.19.4-cp37-cp37m-manylinux2010_x86_64.whl (14.5MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5MB 8.6MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pytz>=2017.2 (from pandas==1.0.5)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/12/f8/ff09af6ff61a3efaad5f61ba5facdf17e7722c4393f7d8a66674d2dbd29f/pytz-2020.4-py2.py3-none-any.whl (509kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 23.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-dateutil>=2.6.1 (from pandas==1.0.5)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 29.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six>=1.5 (from python-dateutil>=2.6.1->pandas==1.0.5)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Installing collected packages: numpy, pytz, six, python-dateutil, pandas\n",
      "Successfully installed numpy-1.19.4 pandas-1.0.5 python-dateutil-2.8.1 pytz-2020.4 six-1.15.0\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/numpy-1.19.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/pytz already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/python_dateutil-2.8.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/six.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/dateutil already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/pandas-1.0.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/pandas already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/pytz-2020.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/six-1.15.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "# !pip install beautifulsoup4 -t /home/aistudio/external-libraries\n",
    "!pip install lightgbm==2.3.1 -t /home/aistudio/external-libraries\n",
    "# !pip install catboost==0.23 -t /home/aistudio/external-libraries\n",
    "# !pip install jieba -t /home/aistudio/external-libraries\n",
    "# !pip install gensim -t /home/aistudio/external-libraries\n",
    "!pip uninstall --yes pandas\n",
    "!pip install pandas==1.0.5 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "import lightgbm as lgb\r\n",
    "import gc\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "pd.set_option('max_columns', None)\r\n",
    "pd.set_option('max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_transaction = pd.read_csv('data/data63881/train_transaction.csv')\r\n",
    "test_transaction = pd.read_csv('data/data63881/test_transaction.csv')\r\n",
    "train_identity = pd.read_csv('data/data63881/train_identity.csv')\r\n",
    "test_identity = pd.read_csv('data/data63881/test_identity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train_transaction.merge(train_identity, on='TransactionID', how='left')\r\n",
    "test = test_transaction.merge(test_identity, on='TransactionID', how='left')\r\n",
    "data = pd.concat([train, test], axis=0, ignore_index=True)\r\n",
    "del train, test\r\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "object_cols = ['ProductCD', 'card4', 'card6', 'DeviceType', 'DeviceInfo', 'P_emaildomain', 'R_emaildomain']\r\n",
    "M_cols = ['M{}'.format(i) for i in range(1, 10)]\r\n",
    "id_cols = ['id_12', 'id_16', 'id_27', 'id_28', 'id_29', 'id_35', 'id_36', 'id_37', 'id_38', 'id_15',\r\n",
    "           'id_23', 'id_34', 'id_30', 'id_31', 'id_33']\r\n",
    "cat_cols = object_cols + M_cols + id_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in cat_cols:\r\n",
    "    le = LabelEncoder()\r\n",
    "    data[i] = le.fit_transform(data[i].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = data[data['isFraud'].notnull()]\r\n",
    "test = data[data['isFraud'].isnull()]\r\n",
    "\r\n",
    "y = train['isFraud']\r\n",
    "train.drop(['isFraud', 'TransactionID'], axis=1, inplace=True)\r\n",
    "test.drop(['isFraud', 'TransactionID'], axis=1, inplace=True)\r\n",
    "used_cols = train.columns\r\n",
    "test = test[used_cols]\r\n",
    "\r\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train, y, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def auc_select(X_train, y_train, X_valid, y_valid, cols, threshold=0.52):\r\n",
    "#     useful_dict = dict()\r\n",
    "#     useless_dict = dict()\r\n",
    "#     params = {\r\n",
    "#         'objective': 'binary',\r\n",
    "#         'boosting': 'gbdt',\r\n",
    "#         'metric': 'auc',\r\n",
    "#         'learning_rate': 0.1,\r\n",
    "#         'num_leaves': 31,\r\n",
    "#         'lambda_l1': 0,\r\n",
    "#         'lambda_l2': 1,\r\n",
    "#         'num_threads': 23,\r\n",
    "#         'min_data_in_leaf': 20,\r\n",
    "#         'first_metric_only': True,\r\n",
    "#         'is_unbalance': True,\r\n",
    "#         'max_depth': -1,\r\n",
    "#         'seed': 2020\r\n",
    "#     }\r\n",
    "#     for i in cols:\r\n",
    "#         print(i)\r\n",
    "#         try:\r\n",
    "#             lgb_train = lgb.Dataset(X_train[[i]].values, y_train)\r\n",
    "#             lgb_valid = lgb.Dataset(X_valid[[i]].values, y_valid, reference=lgb_train)\r\n",
    "#             lgb_model = lgb.train(\r\n",
    "#                 params,\r\n",
    "#                 lgb_train,\r\n",
    "#                 valid_sets=[lgb_valid, lgb_train],\r\n",
    "#                 num_boost_round=1000,\r\n",
    "#                 early_stopping_rounds=50,\r\n",
    "#                 verbose_eval=500\r\n",
    "#             )\r\n",
    "#             print('*' * 10)\r\n",
    "#             print(lgb_model.best_score['valid_0']['auc'])\r\n",
    "#             if lgb_model.best_score['valid_0']['auc'] > threshold:\r\n",
    "#                 useful_dict[i] = lgb_model.best_score['valid_0']['auc']\r\n",
    "#             else:\r\n",
    "#                 useless_dict[i] = lgb_model.best_score['valid_0']['auc']\r\n",
    "#         except:\r\n",
    "#             print('Error: ', i)\r\n",
    "#     useful_cols = list(useful_dict.keys())\r\n",
    "#     useless_cols = list(useless_dict.keys())\r\n",
    "#     return useful_dict, useless_dict, useful_cols, useless_cols\r\n",
    "\r\n",
    "\r\n",
    "# useful_dict, useless_dict, useful_cols, useless_cols = auc_select(X_train, y_train, X_valid, y_valid, used_cols, threshold=0.52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(useless_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_to_drop = ['card4', 'C3', 'V41', 'V98', 'V100', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V113', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V135', 'V138', 'V141', 'V142', 'V161', 'V162', 'V163', 'V286', 'V297', 'V299', 'V300', 'V301', 'V305', 'V311', 'V319', 'V325', 'V326', 'V334', 'id_07', 'id_08', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_32', 'id_34']\r\n",
    "\r\n",
    "X_train.drop(auc_to_drop, axis=1, inplace=True)\r\n",
    "X_valid.drop(auc_to_drop, axis=1, inplace=True)\r\n",
    "train.drop(auc_to_drop, axis=1, inplace=True)\r\n",
    "test.drop(auc_to_drop, axis=1, inplace=True)\r\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[300]\ttraining's auc: 0.978573\tvalid_1's auc: 0.946165\n",
      "[600]\ttraining's auc: 0.991448\tvalid_1's auc: 0.953023\n"
     ]
    }
   ],
   "source": [
    "dtrain = lgb.Dataset(X_train, y_train)\r\n",
    "dvalid = lgb.Dataset(X_valid, y_valid, reference=dtrain)\r\n",
    "params = {\r\n",
    "    'objective': 'binary',\r\n",
    "    'boosting': 'gbdt',\r\n",
    "    'metric': 'auc',\r\n",
    "    # 'metric': 'None',  # 用自定义评估函数是将metric设置为'None'\r\n",
    "    'learning_rate': 0.1,\r\n",
    "    'num_leaves': 31,\r\n",
    "    'lambda_l1': 0,\r\n",
    "    'lambda_l2': 1,\r\n",
    "    'num_threads': 23,\r\n",
    "    'min_data_in_leaf': 20,\r\n",
    "    'first_metric_only': True,\r\n",
    "    'is_unbalance': True,\r\n",
    "    'max_depth': -1,\r\n",
    "    'seed': 2020\r\n",
    "}\r\n",
    "valid_model = lgb.train(\r\n",
    "    params,\r\n",
    "    dtrain,\r\n",
    "    valid_sets=[dtrain, dvalid],\r\n",
    "    num_boost_round=1000000,\r\n",
    "    early_stopping_rounds=200,\r\n",
    "    verbose_eval=300\r\n",
    ")\r\n",
    "\r\n",
    "importance = valid_model.feature_importance(importance_type='gain')\r\n",
    "feature_name = valid_model.feature_name()\r\n",
    "df_importance = pd.DataFrame({\r\n",
    "    'feature_name': feature_name,\r\n",
    "    'importance': importance\r\n",
    "}).sort_values(by='importance', ascending=False)\r\n",
    "df_importance['normalized_importance'] = df_importance['importance'] / df_importance['importance'].sum()\r\n",
    "df_importance['cumulative_importance'] = np.cumsum(df_importance['normalized_importance'])\r\n",
    "record_low_importance = df_importance[df_importance['cumulative_importance'] > 0.95]\r\n",
    "to_drop = list(record_low_importance['feature_name'])\r\n",
    "print(to_drop)\r\n",
    "df_importance.to_csv('sub/imp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.drop(to_drop, axis=1, inplace=True)\r\n",
    "X_valid.drop(to_drop, axis=1, inplace=True)\r\n",
    "train.drop(to_drop, axis=1, inplace=True)\r\n",
    "test.drop(to_drop, axis=1, inplace=True)\r\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtrain = lgb.Dataset(X_train, y_train)\r\n",
    "dvalid = lgb.Dataset(X_valid, y_valid, reference=dtrain)\r\n",
    "dall = lgb.Dataset(train, y)\r\n",
    "\r\n",
    "valid_model2 = lgb.train(\r\n",
    "    params,\r\n",
    "    dtrain,\r\n",
    "    valid_sets=[dtrain, dvalid],\r\n",
    "    num_boost_round=1000000,\r\n",
    "    early_stopping_rounds=200,\r\n",
    "    verbose_eval=300\r\n",
    ")\r\n",
    "\r\n",
    "model = lgb.train(\r\n",
    "    params,\r\n",
    "    dall,\r\n",
    "    num_boost_round=valid_model2.best_iteration\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = model.predict(test)\r\n",
    "sub = pd.DataFrame({'id': range(len(test))})\r\n",
    "sub['isFraud'] = pred\r\n",
    "sub.to_csv('./sub/baseline_{}.csv'.format(time.strftime('%Y%m%d')), index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
