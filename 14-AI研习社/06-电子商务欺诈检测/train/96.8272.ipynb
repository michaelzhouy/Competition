{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data63881\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/aistudio/external-libraries’: File exists\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting lightgbm==2.3.1\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/0b/9d/ddcb2f43aca194987f1a99e27edf41cf9bc39ea750c3371c2a62698c509a/lightgbm-2.3.1-py2.py3-none-manylinux1_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 20.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy (from lightgbm==2.3.1)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/dc/7e/8f6a79b102ca1ea928bae8998b05bf5dc24a90571db13cd119f275ba6252/scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9MB 11.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn (from lightgbm==2.3.1)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/f4/cb/64623369f348e9bfb29ff898a57ac7c91ed4921f228e9726546614d63ccb/scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8MB 28.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy (from lightgbm==2.3.1)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/a5/bb/87d668b353848b93baab0a64cddf6408c40717f099539668c3d26fe39f7e/numpy-1.19.4-cp37-cp37m-manylinux2010_x86_64.whl (14.5MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5MB 8.6MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting joblib>=0.11 (from scikit-learn->lightgbm==2.3.1)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/fc/c9/f58220ac44a1592f79a343caba12f6837f9e0c04c196176a3d66338e1ea8/joblib-0.17.0-py3-none-any.whl (301kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 26.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0 (from scikit-learn->lightgbm==2.3.1)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "Installing collected packages: numpy, scipy, joblib, threadpoolctl, scikit-learn, lightgbm\n",
      "Successfully installed joblib-0.17.0 lightgbm-2.3.1 numpy-1.19.4 scikit-learn-0.23.2 scipy-1.5.4 threadpoolctl-2.1.0\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/lightgbm-2.3.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/lightgbm already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/scikit_learn-0.23.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/threadpoolctl-2.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/scipy-1.5.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/threadpoolctl.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/scipy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/joblib already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/numpy-1.19.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/scipy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/joblib-0.17.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/scikit_learn.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/sklearn already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "Uninstalling pandas-0.23.4:\n",
      "  Successfully uninstalled pandas-0.23.4\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting pandas==1.0.5\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/af/f3/683bf2547a3eaeec15b39cef86f61e921b3b187f250fcd2b5c5fb4386369/pandas-1.0.5-cp37-cp37m-manylinux1_x86_64.whl (10.1MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1MB 15.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-dateutil>=2.6.1 (from pandas==1.0.5)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 18.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2017.2 (from pandas==1.0.5)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/12/f8/ff09af6ff61a3efaad5f61ba5facdf17e7722c4393f7d8a66674d2dbd29f/pytz-2020.4-py2.py3-none-any.whl (509kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 17.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.13.3 (from pandas==1.0.5)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/a5/bb/87d668b353848b93baab0a64cddf6408c40717f099539668c3d26fe39f7e/numpy-1.19.4-cp37-cp37m-manylinux2010_x86_64.whl (14.5MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5MB 8.5MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting six>=1.5 (from python-dateutil>=2.6.1->pandas==1.0.5)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
      "Installing collected packages: six, python-dateutil, pytz, numpy, pandas\n",
      "Successfully installed numpy-1.19.4 pandas-1.0.5 python-dateutil-2.8.1 pytz-2020.4 six-1.15.0\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/python_dateutil-2.8.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/pandas already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/pytz-2020.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/pandas-1.0.5.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/six-1.15.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/numpy-1.19.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/dateutil already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/six.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/pytz already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "# !pip install beautifulsoup4 -t /home/aistudio/external-libraries\n",
    "!pip install lightgbm==2.3.1 -t /home/aistudio/external-libraries\n",
    "# !pip install catboost==0.23 -t /home/aistudio/external-libraries\n",
    "# !pip install jieba -t /home/aistudio/external-libraries\n",
    "# !pip install gensim -t /home/aistudio/external-libraries\n",
    "!pip uninstall --yes pandas\n",
    "!pip install pandas==1.0.5 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "import lightgbm as lgb\r\n",
    "import gc\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "pd.set_option('max_columns', None)\r\n",
    "pd.set_option('max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_transaction = pd.read_csv('data/data63881/train_transaction.csv')\r\n",
    "test_transaction = pd.read_csv('data/data63881/test_transaction.csv')\r\n",
    "train_identity = pd.read_csv('data/data63881/train_identity.csv')\r\n",
    "test_identity = pd.read_csv('data/data63881/test_identity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train_transaction.merge(train_identity, on='TransactionID', how='left')\r\n",
    "test = test_transaction.merge(test_identity, on='TransactionID', how='left')\r\n",
    "data = pd.concat([train, test], axis=0, ignore_index=True)\r\n",
    "del train, test\r\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "object_cols = ['ProductCD', 'card4', 'card6', 'DeviceType', 'DeviceInfo', 'P_emaildomain', 'R_emaildomain']\r\n",
    "M_cols = ['M{}'.format(i) for i in range(1, 10)]\r\n",
    "id_cols = ['id_12', 'id_16', 'id_27', 'id_28', 'id_29', 'id_35', 'id_36', 'id_37', 'id_38', 'id_15',\r\n",
    "           'id_23', 'id_34', 'id_30', 'id_31', 'id_33']\r\n",
    "cat_cols = object_cols + M_cols + id_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in cat_cols:\r\n",
    "    le = LabelEncoder()\r\n",
    "    data[i] = le.fit_transform(data[i].astype(str))\r\n",
    "    data[i] = data[i].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = data[data['isFraud'].notnull()]\r\n",
    "test = data[data['isFraud'].isnull()]\r\n",
    "\r\n",
    "y = train['isFraud']\r\n",
    "train.drop(['isFraud', 'TransactionID'], axis=1, inplace=True)\r\n",
    "test.drop(['isFraud', 'TransactionID'], axis=1, inplace=True)\r\n",
    "used_cols = train.columns\r\n",
    "test = test[used_cols]\r\n",
    "\r\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train, y, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def auc_select(X_train, y_train, X_valid, y_valid, cols, threshold=0.52):\r\n",
    "#     useful_dict = dict()\r\n",
    "#     useless_dict = dict()\r\n",
    "#     params = {\r\n",
    "#         'objective': 'binary',\r\n",
    "#         'boosting': 'gbdt',\r\n",
    "#         'metric': 'auc',\r\n",
    "#         'learning_rate': 0.1,\r\n",
    "#         'num_leaves': 31,\r\n",
    "#         'lambda_l1': 0,\r\n",
    "#         'lambda_l2': 1,\r\n",
    "#         'num_threads': 23,\r\n",
    "#         'min_data_in_leaf': 20,\r\n",
    "#         'first_metric_only': True,\r\n",
    "#         'is_unbalance': True,\r\n",
    "#         'max_depth': -1,\r\n",
    "#         'seed': 2020\r\n",
    "#     }\r\n",
    "#     for i in cols:\r\n",
    "#         print(i)\r\n",
    "#         try:\r\n",
    "#             lgb_train = lgb.Dataset(X_train[[i]].values, y_train)\r\n",
    "#             lgb_valid = lgb.Dataset(X_valid[[i]].values, y_valid, reference=lgb_train)\r\n",
    "#             lgb_model = lgb.train(\r\n",
    "#                 params,\r\n",
    "#                 lgb_train,\r\n",
    "#                 valid_sets=[lgb_valid, lgb_train],\r\n",
    "#                 num_boost_round=1000,\r\n",
    "#                 early_stopping_rounds=50,\r\n",
    "#                 verbose_eval=500\r\n",
    "#             )\r\n",
    "#             print('*' * 10)\r\n",
    "#             print(lgb_model.best_score['valid_0']['auc'])\r\n",
    "#             if lgb_model.best_score['valid_0']['auc'] > threshold:\r\n",
    "#                 useful_dict[i] = lgb_model.best_score['valid_0']['auc']\r\n",
    "#             else:\r\n",
    "#                 useless_dict[i] = lgb_model.best_score['valid_0']['auc']\r\n",
    "#         except:\r\n",
    "#             print('Error: ', i)\r\n",
    "#     useful_cols = list(useful_dict.keys())\r\n",
    "#     useless_cols = list(useless_dict.keys())\r\n",
    "#     return useful_dict, useless_dict, useful_cols, useless_cols\r\n",
    "\r\n",
    "\r\n",
    "# useful_dict, useless_dict, useful_cols, useless_cols = auc_select(X_train, y_train, X_valid, y_valid, used_cols, threshold=0.52)\r\n",
    "# print(useless_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_to_drop = ['card4', 'C3', 'V41', 'V98', 'V100', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V113', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V135', 'V138', 'V141', 'V142', 'V161', 'V162', 'V163', 'V286', 'V297', 'V299', 'V300', 'V301', 'V305', 'V311', 'V319', 'V325', 'V326', 'V334', 'id_07', 'id_08', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_32', 'id_34']\r\n",
    "\r\n",
    "X_train.drop(auc_to_drop, axis=1, inplace=True)\r\n",
    "X_valid.drop(auc_to_drop, axis=1, inplace=True)\r\n",
    "train.drop(auc_to_drop, axis=1, inplace=True)\r\n",
    "test.drop(auc_to_drop, axis=1, inplace=True)\r\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[300]\ttraining's auc: 0.966523\tvalid_1's auc: 0.93992\n",
      "[600]\ttraining's auc: 0.983255\tvalid_1's auc: 0.950282\n",
      "[900]\ttraining's auc: 0.990536\tvalid_1's auc: 0.955485\n",
      "[1200]\ttraining's auc: 0.994207\tvalid_1's auc: 0.957933\n",
      "[1500]\ttraining's auc: 0.996358\tvalid_1's auc: 0.959413\n",
      "[1800]\ttraining's auc: 0.997786\tvalid_1's auc: 0.960713\n",
      "[2100]\ttraining's auc: 0.998512\tvalid_1's auc: 0.961549\n",
      "[2400]\ttraining's auc: 0.99901\tvalid_1's auc: 0.961998\n",
      "Early stopping, best iteration is:\n",
      "[2414]\ttraining's auc: 0.999032\tvalid_1's auc: 0.962046\n",
      "Evaluated only: auc\n",
      "['V67', 'V30', 'V291', 'C10', 'V99', 'D13', 'id_03', 'id_13', 'V19', 'V332', 'V86', 'V90', 'V136', 'V126', 'M2', 'V5', 'V69', 'V243', 'V26', 'V282', 'D9', 'V24', 'V94', 'V10', 'V4', 'id_17', 'D12', 'id_09', 'V124', 'V6', 'V37', 'id_18', 'V152', 'V149', 'V321', 'V96', 'V169', 'V77', 'V137', 'D7', 'V11', 'V306', 'V34', 'V292', 'V217', 'id_11', 'V267', 'V7', 'V66', 'V81', 'V74', 'V52', 'V289', 'V295', 'V251', 'V171', 'id_15', 'V257', 'V265', 'V316', 'V73', 'V160', 'V25', 'V132', 'V139', 'V234', 'id_38', 'V318', 'V47', 'V335', 'V221', 'V264', 'V333', 'V134', 'V178', 'id_04', 'V279', 'V8', 'C4', 'V209', 'V261', 'V143', 'V63', 'V215', 'V266', 'V115', 'V208', 'V207', 'V290', 'V274', 'V277', 'V259', 'V206', 'V225', 'V293', 'V164', 'V157', 'V260', 'V229', 'V3', 'V166', 'V64', 'V2', 'V101', 'V263', 'V271', 'V224', 'V256', 'V203', 'V95', 'V211', 'V159', 'V272', 'V39', 'V276', 'V280', 'V33', 'V220', 'V270', 'V167', 'V51', 'V273', 'V97', 'V103', 'V331', 'V210', 'id_10', 'V253', 'V298', 'V116', 'V218', 'V337', 'V72', 'V288', 'V214', 'V50', 'DeviceType', 'V212', 'V268', 'V145', 'V123', 'V287', 'V246', 'V275', 'V238', 'V222', 'V79', 'V188', 'V172', 'V59', 'V80', 'V278', 'V284', 'V176', 'V9', 'V150', 'V42', 'V245', 'V205', 'V170', 'V328', 'V17', 'V226', 'V46', 'V125', 'V60', 'V40', 'addr2', 'V92', 'V200', 'V303', 'V32', 'V114', 'V151', 'V262', 'V336', 'M1', 'V255', 'V216', 'V43', 'V175', 'V179', 'V338', 'V154', 'V57', 'V339', 'V180', 'id_37', 'V58', 'V182', 'V219', 'V84', 'V112', 'V202', 'V140', 'V85', 'V330', 'V304', 'V244', 'V239', 'V302', 'V329', 'V146', 'V184', 'V204', 'V185', 'V177', 'V237', 'id_16', 'V174', 'V199', 'V71', 'V194', 'V233', 'V227', 'V231', 'V213', 'V31', 'V242', 'V93', 'V223', 'id_28', 'id_36', 'V168', 'V186', 'V21', 'V173', 'V254', 'V197', 'V327', 'V248', 'V144', 'V252', 'V192', 'V230', 'V269', 'id_29', 'V15', 'V183', 'V190', 'V22', 'V181', 'V324', 'id_12', 'V195', 'V247', 'V228', 'V249', 'V236', 'V148', 'V27', 'V16', 'V196', 'V89', 'V155', 'id_35', 'V191', 'V198', 'V193', 'V1', 'V65', 'V14', 'V18', 'V68', 'V235', 'V240', 'V241', 'V250', 'V153', 'V147', 'V322', 'V88', 'V28']\n"
     ]
    }
   ],
   "source": [
    "dtrain = lgb.Dataset(X_train, y_train)\r\n",
    "dvalid = lgb.Dataset(X_valid, y_valid, reference=dtrain)\r\n",
    "params = {\r\n",
    "    'objective': 'binary',\r\n",
    "    'boosting': 'gbdt',\r\n",
    "    'metric': 'auc',\r\n",
    "    # 'metric': 'None',  # 用自定义评估函数是将metric设置为'None'\r\n",
    "    'learning_rate': 0.05,\r\n",
    "    'num_leaves': 31,\r\n",
    "    'lambda_l1': 0,\r\n",
    "    'lambda_l2': 1,\r\n",
    "    'num_threads': 23,\r\n",
    "    'min_data_in_leaf': 20,\r\n",
    "    'first_metric_only': True,\r\n",
    "    'is_unbalance': True,\r\n",
    "    'max_depth': -1,\r\n",
    "    'seed': 2020\r\n",
    "}\r\n",
    "valid_model = lgb.train(\r\n",
    "    params,\r\n",
    "    dtrain,\r\n",
    "    valid_sets=[dtrain, dvalid],\r\n",
    "    num_boost_round=1000000,\r\n",
    "    early_stopping_rounds=200,\r\n",
    "    verbose_eval=300\r\n",
    ")\r\n",
    "\r\n",
    "importance = valid_model.feature_importance(importance_type='gain')\r\n",
    "feature_name = valid_model.feature_name()\r\n",
    "df_importance = pd.DataFrame({\r\n",
    "    'feature_name': feature_name,\r\n",
    "    'importance': importance\r\n",
    "}).sort_values(by='importance', ascending=False)\r\n",
    "df_importance['normalized_importance'] = df_importance['importance'] / df_importance['importance'].sum()\r\n",
    "df_importance['cumulative_importance'] = np.cumsum(df_importance['normalized_importance'])\r\n",
    "record_low_importance = df_importance[df_importance['cumulative_importance'] > 0.95]\r\n",
    "to_drop = list(record_low_importance['feature_name'])\r\n",
    "print(to_drop)\r\n",
    "df_importance.to_csv('sub/imp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.drop(to_drop, axis=1, inplace=True)\r\n",
    "X_valid.drop(to_drop, axis=1, inplace=True)\r\n",
    "train.drop(to_drop, axis=1, inplace=True)\r\n",
    "test.drop(to_drop, axis=1, inplace=True)\r\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[300]\ttraining's auc: 0.966531\tvalid_1's auc: 0.939578\n",
      "[600]\ttraining's auc: 0.983513\tvalid_1's auc: 0.950423\n",
      "[900]\ttraining's auc: 0.990669\tvalid_1's auc: 0.955318\n",
      "[1200]\ttraining's auc: 0.994434\tvalid_1's auc: 0.958396\n",
      "[1500]\ttraining's auc: 0.996485\tvalid_1's auc: 0.960045\n",
      "[1800]\ttraining's auc: 0.997741\tvalid_1's auc: 0.961269\n",
      "[2100]\ttraining's auc: 0.998544\tvalid_1's auc: 0.961746\n",
      "[2400]\ttraining's auc: 0.999001\tvalid_1's auc: 0.96219\n",
      "[2700]\ttraining's auc: 0.99936\tvalid_1's auc: 0.962626\n",
      "[3000]\ttraining's auc: 0.999573\tvalid_1's auc: 0.962819\n",
      "Early stopping, best iteration is:\n",
      "[2970]\ttraining's auc: 0.999557\tvalid_1's auc: 0.962867\n",
      "Evaluated only: auc\n"
     ]
    }
   ],
   "source": [
    "dtrain = lgb.Dataset(X_train, y_train)\r\n",
    "dvalid = lgb.Dataset(X_valid, y_valid, reference=dtrain)\r\n",
    "dall = lgb.Dataset(train, y)\r\n",
    "\r\n",
    "valid_model2 = lgb.train(\r\n",
    "    params,\r\n",
    "    dtrain,\r\n",
    "    valid_sets=[dtrain, dvalid],\r\n",
    "    num_boost_round=1000000,\r\n",
    "    early_stopping_rounds=200,\r\n",
    "    verbose_eval=300\r\n",
    ")\r\n",
    "\r\n",
    "model = lgb.train(\r\n",
    "    params,\r\n",
    "    dall,\r\n",
    "    num_boost_round=valid_model2.best_iteration\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = model.predict(test)\r\n",
    "sub = pd.DataFrame({'id': range(len(test))})\r\n",
    "sub['isFraud'] = pred\r\n",
    "sub.to_csv('./sub/baseline_{}.csv'.format(time.strftime('%Y%m%d')), index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
