{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:06:48.825587Z",
     "start_time": "2020-12-06T11:06:47.510422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       ".datatable .frame thead tr.colnames {  background-image: url('data:image/png;base64,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');  background-repeat: repeat-x;  background-size: 14px;  height: 28px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', None)\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:06:48.849548Z",
     "start_time": "2020-12-06T11:06:48.827583Z"
    }
   },
   "outputs": [],
   "source": [
    "class MeanEncoder:\n",
    "    def __init__(self, categorical_features, n_splits=10, target_type='classification', prior_weight_func=None):\n",
    "        \"\"\"\n",
    "        :param categorical_features: list of str, the name of the categorical columns to encode\n",
    " \n",
    "        :param n_splits: the number of splits used in mean encoding\n",
    " \n",
    "        :param target_type: str, 'regression' or 'classification'\n",
    " \n",
    "        :param prior_weight_func:\n",
    "        a function that takes in the number of observations, and outputs prior weight\n",
    "        when a dict is passed, the default exponential decay function will be used:\n",
    "        k: the number of observations needed for the posterior to be weighted equally as the prior\n",
    "        f: larger f --> smaller slope\n",
    "        \"\"\"\n",
    " \n",
    "        self.categorical_features = categorical_features\n",
    "        self.n_splits = n_splits\n",
    "        self.learned_stats = {}\n",
    " \n",
    "        if target_type == 'classification':\n",
    "            self.target_type = target_type\n",
    "            self.target_values = []\n",
    "        else:\n",
    "            self.target_type = 'regression'\n",
    "            self.target_values = None\n",
    " \n",
    "        if isinstance(prior_weight_func, dict):\n",
    "            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n",
    "        elif callable(prior_weight_func):\n",
    "            self.prior_weight_func = prior_weight_func\n",
    "        else:\n",
    "            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n",
    " \n",
    "    @staticmethod\n",
    "    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n",
    "        X_train = X_train[[variable]].copy()\n",
    "        X_test = X_test[[variable]].copy()\n",
    " \n",
    "        if target is not None:\n",
    "            nf_name = '{}_pred_{}'.format(variable, target)\n",
    "            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n",
    "        else:\n",
    "            nf_name = '{}_pred'.format(variable)\n",
    "            X_train['pred_temp'] = y_train  # regression\n",
    "        prior = X_train['pred_temp'].mean()\n",
    " \n",
    "        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg(['mean', 'size'])\n",
    "        col_avg_y['size'] = prior_weight_func(col_avg_y['size'])\n",
    "        col_avg_y[nf_name] = col_avg_y['size'] * prior + (1 - col_avg_y['size']) * col_avg_y['mean']\n",
    "        col_avg_y.drop(['size', 'mean'], axis=1, inplace=True)\n",
    " \n",
    "        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n",
    "        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n",
    " \n",
    "        return nf_train, nf_test, prior, col_avg_y\n",
    " \n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :param y: pandas Series or numpy array, n_samples\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "        if self.target_type == 'classification':\n",
    "            skf = StratifiedKFold(self.n_splits)\n",
    "        else:\n",
    "            skf = KFold(self.n_splits)\n",
    " \n",
    "        if self.target_type == 'classification':\n",
    "            self.target_values = sorted(set(y))\n",
    "            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n",
    "                                  product(self.categorical_features, self.target_values)}\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(X, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        else:\n",
    "            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(X, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        return X_new\n",
    " \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    " \n",
    "        if self.target_type == 'classification':\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "        else:\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    " \n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:06:49.151396Z",
     "start_time": "2020-12-06T11:06:48.851117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_mean:  65868516.67875\n",
      "(2400, 23)\n",
      "(600, 22)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../input/train.csv', encoding='latin-1')\n",
    "test = pd.read_csv('../input/test.csv', encoding='latin-1')\n",
    "y = train['revenue']\n",
    "y_mean = train['revenue'].mean()\n",
    "print('y_mean: ', y_mean)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "# train['revenue'] = np.log1p(train['revenue'])\n",
    "\n",
    "data = pd.concat([train, test], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:06:49.170322Z",
     "start_time": "2020-12-06T11:06:49.152392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>poster_path</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>cast</th>\n",
       "      <th>crew</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 86780, 'name': 'Clash of the Titans Co...</td>\n",
       "      <td>150000000</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}]</td>\n",
       "      <td>http://www.wrathofthetitansmovie.org</td>\n",
       "      <td>tt1646987</td>\n",
       "      <td>en</td>\n",
       "      <td>Wrath of the Titans</td>\n",
       "      <td>A decade after his heroic defeat of the monstr...</td>\n",
       "      <td>7.739904</td>\n",
       "      <td>/Albfq3ziSCQVyh5PzMSsFmmgHmy.jpg</td>\n",
       "      <td>[{'name': 'Legendary Pictures', 'id': 923}, {'...</td>\n",
       "      <td>[{'iso_3166_1': 'ES', 'name': 'Spain'}, {'iso_...</td>\n",
       "      <td>3/27/12</td>\n",
       "      <td>99.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Feel the Wrath</td>\n",
       "      <td>Wrath of the Titans</td>\n",
       "      <td>[{'id': 1449, 'name': 'underworld'}, {'id': 20...</td>\n",
       "      <td>[{'cast_id': 4, 'character': 'Perseus', 'credi...</td>\n",
       "      <td>[{'credit_id': '52fe4926c3a36847f818b96d', 'de...</td>\n",
       "      <td>301000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35000000</td>\n",
       "      <td>[{'id': 27, 'name': 'Horror'}, {'id': 9648, 'n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt0120681</td>\n",
       "      <td>en</td>\n",
       "      <td>From Hell</td>\n",
       "      <td>Frederick Abberline is an opium-huffing inspec...</td>\n",
       "      <td>7.790140</td>\n",
       "      <td>/f3J77Cy3pRSeeN52Pk8oIvgi6IN.jpg</td>\n",
       "      <td>[{'name': 'Twentieth Century Fox Film Corporat...</td>\n",
       "      <td>[{'iso_3166_1': 'CZ', 'name': 'Czech Republic'...</td>\n",
       "      <td>10/19/01</td>\n",
       "      <td>122.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Only the legend will survive.</td>\n",
       "      <td>From Hell</td>\n",
       "      <td>[{'id': 1465, 'name': 'loss of family'}, {'id'...</td>\n",
       "      <td>[{'cast_id': 19, 'character': 'Inspector Frede...</td>\n",
       "      <td>[{'credit_id': '52fe4273c3a36847f801fbfb', 'de...</td>\n",
       "      <td>74558115.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                              belongs_to_collection     budget  \\\n",
       "0   0  [{'id': 86780, 'name': 'Clash of the Titans Co...  150000000   \n",
       "1   1                                                NaN   35000000   \n",
       "\n",
       "                                              genres  \\\n",
       "0                  [{'id': 12, 'name': 'Adventure'}]   \n",
       "1  [{'id': 27, 'name': 'Horror'}, {'id': 9648, 'n...   \n",
       "\n",
       "                               homepage    imdb_id original_language  \\\n",
       "0  http://www.wrathofthetitansmovie.org  tt1646987                en   \n",
       "1                                   NaN  tt0120681                en   \n",
       "\n",
       "        original_title                                           overview  \\\n",
       "0  Wrath of the Titans  A decade after his heroic defeat of the monstr...   \n",
       "1            From Hell  Frederick Abberline is an opium-huffing inspec...   \n",
       "\n",
       "   popularity                       poster_path  \\\n",
       "0    7.739904  /Albfq3ziSCQVyh5PzMSsFmmgHmy.jpg   \n",
       "1    7.790140  /f3J77Cy3pRSeeN52Pk8oIvgi6IN.jpg   \n",
       "\n",
       "                                production_companies  \\\n",
       "0  [{'name': 'Legendary Pictures', 'id': 923}, {'...   \n",
       "1  [{'name': 'Twentieth Century Fox Film Corporat...   \n",
       "\n",
       "                                production_countries release_date  runtime  \\\n",
       "0  [{'iso_3166_1': 'ES', 'name': 'Spain'}, {'iso_...      3/27/12     99.0   \n",
       "1  [{'iso_3166_1': 'CZ', 'name': 'Czech Republic'...     10/19/01    122.0   \n",
       "\n",
       "                                    spoken_languages    status  \\\n",
       "0           [{'iso_639_1': 'en', 'name': 'English'}]  Released   \n",
       "1  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...  Released   \n",
       "\n",
       "                         tagline                title  \\\n",
       "0                 Feel the Wrath  Wrath of the Titans   \n",
       "1  Only the legend will survive.            From Hell   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  [{'id': 1449, 'name': 'underworld'}, {'id': 20...   \n",
       "1  [{'id': 1465, 'name': 'loss of family'}, {'id'...   \n",
       "\n",
       "                                                cast  \\\n",
       "0  [{'cast_id': 4, 'character': 'Perseus', 'credi...   \n",
       "1  [{'cast_id': 19, 'character': 'Inspector Frede...   \n",
       "\n",
       "                                                crew      revenue  \n",
       "0  [{'credit_id': '52fe4926c3a36847f818b96d', 'de...  301000000.0  \n",
       "1  [{'credit_id': '52fe4273c3a36847f801fbfb', 'de...   74558115.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:06:49.185315Z",
     "start_time": "2020-12-06T11:06:49.171314Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_cols = ['poster_path', 'imdb_id']\n",
    "data.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时间特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:06:49.228206Z",
     "start_time": "2020-12-06T11:06:49.186311Z"
    }
   },
   "outputs": [],
   "source": [
    "data['release_year'] = data['release_date'].apply(lambda x: '19' + x.split('/')[2] if int(x.split('/')[2]) > 20 else '20' + x.split('/')[2]).astype(int)\n",
    "data['release_month'] = data['release_date'].apply(lambda x: x.split('/')[0]).astype(int)\n",
    "data['release_day'] = data['release_date'].apply(lambda x: x.split('/')[1]).astype(int)\n",
    "\n",
    "data['release_date'] = pd.to_datetime(data['release_year'].astype(str) + '-' + data['release_month'].astype(str) + '-' + data['release_day'].astype(str))\n",
    "\n",
    "data['release_date_weekday'] = data['release_date'].apply(lambda x: x.weekday())\n",
    "data['release_date_TONOW'] = (datetime.now() - data['release_date']).dt.days\n",
    "\n",
    "data.drop(['release_day', 'release_date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 判断是否为空"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:06:49.241172Z",
     "start_time": "2020-12-06T11:06:49.231154Z"
    }
   },
   "outputs": [],
   "source": [
    "isnull_cols = ['homepage', 'tagline', 'belongs_to_collection', 'overview']\n",
    "for i in isnull_cols:\n",
    "    data[i + '_isnull'] = data[i].isnull().astype(int)\n",
    "data.drop(isnull_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数值特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:06:49.250103Z",
     "start_time": "2020-12-06T11:06:49.243122Z"
    }
   },
   "outputs": [],
   "source": [
    "num_cols = ['runtime', 'popularity', 'budget']\n",
    "\n",
    "for i in num_cols:\n",
    "    data[i] = np.log1p(data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类别特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:06:49.257088Z",
     "start_time": "2020-12-06T11:06:49.251100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            Wrath of the Titans\n",
       "1                                      From Hell\n",
       "2                   Guess Who's Coming to Dinner\n",
       "3    Talladega Nights: The Ballad of Ricky Bobby\n",
       "4                                         Xanadu\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:06:49.267058Z",
     "start_time": "2020-12-06T11:06:49.259079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'].isnull().sum(), data['original_title'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:06:49.294028Z",
     "start_time": "2020-12-06T11:06:49.269054Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 501.29it/s]\n"
     ]
    }
   ],
   "source": [
    "cat_cols = ['original_language', 'status', 'title', 'original_title']\n",
    "\n",
    "data['title=original_title'] = (data['title'] == data['original_title']).astype(int)\n",
    "\n",
    "for i in tqdm(['original_language', 'status']):\n",
    "    le = LabelEncoder()\n",
    "    data[i] = le.fit_transform(data[i])\n",
    "\n",
    "data['original_language_count'] = data['original_language'].map(data['original_language'].value_counts())\n",
    "data['release_year_count'] = data['release_year'].map(data['release_year'].value_counts())\n",
    "data['release_month_count'] = data['release_month'].map(data['release_month'].value_counts())\n",
    "data['release_date_weekday_count'] = data['release_date_weekday'].map(data['release_date_weekday'].value_counts())\n",
    "data['title_count'] = data['title'].map(data['title'].value_counts())\n",
    "\n",
    "data.drop(['title', 'original_title'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 嵌套特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:06:52.198168Z",
     "start_time": "2020-12-06T11:06:49.295024Z"
    }
   },
   "outputs": [],
   "source": [
    "nested_cols = ['genres', 'production_companies', 'production_countries',\n",
    "               'Keywords', 'spoken_languages', 'cast', 'crew']\n",
    "for i in nested_cols:\n",
    "    data[i + '_length'] = data[i].apply(lambda x: 0 if pd.isnull(x) else len(eval(x)))\n",
    "\n",
    "# data['genres_0'] = data['genres'].apply(lambda x: np.nan if pd.isnull(x) else eval(x)[0]['name'])\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# data['genres_0'] = le.fit_transform(data['genres_0'].astype(str))\n",
    "# data['genres_0_count'] = data['genres_0'].map(data['genres_0'].value_counts())\n",
    "\n",
    "# data.drop(nested_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:07:07.148310Z",
     "start_time": "2020-12-06T11:06:52.199149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genres\n",
      "production_companies\n",
      "production_countries\n",
      "Keywords\n",
      "spoken_languages\n",
      "cast\n",
      "crew\n"
     ]
    }
   ],
   "source": [
    "def get_name(x):\n",
    "    if pd.isnull(x):\n",
    "        return []\n",
    "    else:\n",
    "        df = pd.DataFrame(eval(x))\n",
    "        if 'name' in df.columns:\n",
    "#             df['name'] = df['name'].apply(lambda s: ''.join(s.split()))\n",
    "            return df['name'].tolist()\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "\n",
    "for i in nested_cols:\n",
    "    print(i)\n",
    "    data[i + '_name'] = data[i].apply(lambda x: get_name(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:07:10.581878Z",
     "start_time": "2020-12-06T11:07:07.149308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genres_name\n",
      "Start tfidf ...\n",
      "production_companies_name\n",
      "Start tfidf ...\n",
      "production_countries_name\n",
      "Start tfidf ...\n",
      "Keywords_name\n",
      "Start tfidf ...\n",
      "spoken_languages_name\n",
      "Start tfidf ...\n",
      "cast_name\n",
      "Start tfidf ...\n",
      "crew_name\n",
      "Start tfidf ...\n",
      "genres_name\n",
      "Start count2vec ...\n",
      "production_companies_name\n",
      "Start count2vec ...\n",
      "production_countries_name\n",
      "Start count2vec ...\n",
      "Keywords_name\n",
      "Start count2vec ...\n",
      "spoken_languages_name\n",
      "Start count2vec ...\n",
      "cast_name\n",
      "Start count2vec ...\n",
      "crew_name\n",
      "Start count2vec ...\n"
     ]
    }
   ],
   "source": [
    "def tfidf_emb(df_, cat_col, emb_size=10, seed=1024):\n",
    "    print('Start tfidf ...')\n",
    "    df = df_.copy()\n",
    "    df[cat_col] = df[cat_col].fillna('-1')\n",
    "    df[cat_col] = df[cat_col].apply(lambda x: ' '.join(x))\n",
    "    tfidf_enc = TfidfVectorizer()\n",
    "    tfidf_vec = tfidf_enc.fit_transform(df[cat_col])\n",
    "    svd_enc = TruncatedSVD(n_components=emb_size, n_iter=20, random_state=seed)\n",
    "    svd_vec = svd_enc.fit_transform(tfidf_vec)\n",
    "    tfidf_df = pd.DataFrame(svd_vec)\n",
    "    tfidf_df.columns = ['{}_tfidf_{}'.format(cat_col, i) for i in range(emb_size)]\n",
    "    res = tfidf_df\n",
    "    return res\n",
    "\n",
    "\n",
    "def count2vec_emb(df_, cat_col, emb_size=10, seed=1024):\n",
    "    print('Start count2vec ...')\n",
    "    df = df_.copy()\n",
    "    df[cat_col] = df[cat_col].fillna('-1')\n",
    "    df[cat_col] = df[cat_col].apply(lambda x: ' '.join(x))\n",
    "    count_enc = CountVectorizer()\n",
    "    count_vec = count_enc.fit_transform(df[cat_col])\n",
    "    svd_enc = TruncatedSVD(n_components=emb_size, n_iter=20, random_state=seed)\n",
    "    svd_vec = svd_enc.fit_transform(count_vec)\n",
    "    c2v_df = pd.DataFrame(svd_vec)\n",
    "    c2v_df.columns = ['{}_count2vec_{}'.format(cat_col, i) for i in range(emb_size)]\n",
    "    res = c2v_df\n",
    "    return res\n",
    "\n",
    "\n",
    "for i in [i + '_name' for i in nested_cols]:\n",
    "    print(i)\n",
    "    tfidf_df = tfidf_emb(data, i, emb_size=10, seed=1024)\n",
    "    data = pd.concat([data, tfidf_df], axis=1)\n",
    "\n",
    "for i in [i + '_name' for i in nested_cols]:\n",
    "    print(i)\n",
    "    c2v_df = count2vec_emb(data, i, emb_size=10, seed=1024)\n",
    "    data = pd.concat([data, c2v_df], axis=1)\n",
    "\n",
    "data.drop(nested_cols, axis=1, inplace=True)\n",
    "data.drop([i + '_name' for i in nested_cols], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:07:10.597975Z",
     "start_time": "2020-12-06T11:07:10.582875Z"
    }
   },
   "outputs": [],
   "source": [
    "train = data[data['revenue'].notnull()]\n",
    "test = data[data['revenue'].isnull()]\n",
    "\n",
    "train.fillna(-999, inplace=True)\n",
    "test.fillna(-999, inplace=True)\n",
    "\n",
    "used_cols = [i for i in train.columns if i not in ['id', 'release_date', 'revenue']]\n",
    "train = train[used_cols]\n",
    "test = test[used_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:07:10.873389Z",
     "start_time": "2020-12-06T11:07:10.599971Z"
    }
   },
   "outputs": [],
   "source": [
    "class_list = ['release_year', 'release_month', 'release_date_weekday', 'original_language']\n",
    "\n",
    "ME = MeanEncoder(categorical_features=class_list, n_splits=5, target_type='regression', prior_weight_func=None)\n",
    "train = ME.fit_transform(train, y)\n",
    "test = ME.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:07:11.214275Z",
     "start_time": "2020-12-06T11:07:10.874386Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 12.36it/s]\n"
     ]
    }
   ],
   "source": [
    "train['revenue'] = y\n",
    "\n",
    "# 暂且选择这三种编码\n",
    "enc_cols = []\n",
    "stats_default_dict = {\n",
    "    'max': train['revenue'].max(),\n",
    "    'min': train['revenue'].min(),\n",
    "    'median': train['revenue'].median(),\n",
    "    'mean': train['revenue'].mean(),\n",
    "    'sum': train['revenue'].sum(),\n",
    "    'std': train['revenue'].std(),\n",
    "    'skew': train['revenue'].skew(),\n",
    "    'kurt': train['revenue'].kurt(),\n",
    "    'mad': train['revenue'].mad()\n",
    "}\n",
    "enc_stats = ['max', 'min', 'mean']\n",
    "skf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for f in tqdm(['release_year', 'release_month', 'release_date_weekday', 'original_language']):\n",
    "    enc_dict = {}\n",
    "    for stat in enc_stats:\n",
    "        enc_dict['{}_target_{}'.format(f, stat)] = stat\n",
    "        train['{}_target_{}'.format(f, stat)] = 0\n",
    "        test['{}_target_{}'.format(f, stat)] = 0\n",
    "        enc_cols.append('{}_target_{}'.format(f, stat))\n",
    "    for i, (trn_idx, val_idx) in enumerate(skf.split(train, y)):\n",
    "        trn_x, val_x = train.iloc[trn_idx].reset_index(drop=True), train.iloc[val_idx].reset_index(drop=True)\n",
    "        enc_df = trn_x.groupby(f, as_index=False)['revenue'].agg(enc_dict)\n",
    "        val_x = val_x[[f]].merge(enc_df, on=f, how='left')\n",
    "        test_x = test[[f]].merge(enc_df, on=f, how='left')\n",
    "        for stat in enc_stats:\n",
    "            val_x['{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].fillna(stats_default_dict[stat])\n",
    "            test_x['{}_target_{}'.format(f, stat)] = test_x['{}_target_{}'.format(f, stat)].fillna(stats_default_dict[stat])\n",
    "            train.loc[val_idx, '{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].values\n",
    "            test['{}_target_{}'.format(f, stat)] += test_x['{}_target_{}'.format(f, stat)].values / skf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:07:11.255222Z",
     "start_time": "2020-12-06T11:07:11.215260Z"
    }
   },
   "outputs": [],
   "source": [
    "n_train = train.shape[0]\n",
    "\n",
    "train.drop('revenue', axis=1, inplace=True)\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(pd.concat([train, test]).values)\n",
    "all_data = min_max_scaler.transform(pd.concat([train, test]).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:07:11.327996Z",
     "start_time": "2020-12-06T11:07:11.256150Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "all_features = pca.fit_transform(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:07:11.332944Z",
     "start_time": "2020-12-06T11:07:11.328956Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 1024\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:07:25.495245Z",
     "start_time": "2020-12-06T11:07:11.333943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2400, 100]) torch.Size([2400, 1])\n",
      "Train Loss:\n",
      " [15.237401008605957, 15.154300689697266, 15.080608367919922, 15.025989532470703, 14.994147300720215, 14.974706649780273, 14.967708587646484, 14.928750038146973, 14.893306732177734, 14.843949317932129, 14.757121086120605, 14.67077922821045, 14.60492992401123, 14.537823677062988, 14.459628105163574, 14.345142364501953, 14.269889831542969, 14.09898567199707, 14.010929107666016, 13.781705856323242, 13.777726173400879, 13.55643081665039, 13.448911666870117, 13.34668254852295, 13.074029922485352, 13.208847045898438, 12.850961685180664, 12.802734375, 12.725997924804688, 12.578690528869629, 12.54976749420166, 12.437601089477539, 12.343708038330078, 12.216184616088867, 12.18624496459961, 12.101295471191406, 12.01229476928711, 11.951849937438965, 11.862275123596191, 11.854185104370117, 11.757683753967285, 11.693095207214355, 11.641303062438965, 11.57534408569336, 11.527058601379395, 11.477983474731445, 11.420373916625977, 11.368770599365234, 11.305463790893555, 11.272604942321777, 11.255624771118164, 11.16110610961914, 11.169977188110352, 11.066641807556152, 11.06620979309082, 10.99098014831543, 10.966727256774902, 10.920426368713379, 10.869857788085938, 10.837760925292969, 10.79523754119873, 10.760531425476074, 10.722583770751953, 10.689486503601074, 10.638945579528809, 10.62154483795166, 10.577642440795898, 10.555790901184082, 10.502402305603027, 10.496654510498047, 10.441553115844727, 10.426980972290039, 10.379812240600586, 10.373102188110352, 10.31489372253418, 10.313508987426758, 10.25728988647461, 10.248617172241211, 10.202473640441895, 10.193317413330078, 10.144725799560547, 10.136930465698242, 10.095208168029785, 10.071819305419922, 10.057876586914062, 10.015410423278809, 10.002634048461914, 9.973487854003906, 9.946579933166504, 9.927762031555176, 9.897383689880371, 9.884774208068848, 9.847245216369629, 9.834753036499023, 9.803581237792969, 9.787347793579102, 9.761457443237305, 9.739031791687012, 9.720257759094238, 9.691282272338867]\n",
      "               ID      revenue\n",
      "count  600.000000   600.000000\n",
      "mean   299.500000   929.257385\n",
      "std    173.349358  1108.635620\n",
      "min      0.000000    61.812187\n",
      "25%    149.750000   380.226242\n",
      "50%    299.500000   500.060104\n",
      "75%    449.250000  1010.577042\n",
      "max    599.000000  8153.320801\n"
     ]
    }
   ],
   "source": [
    "train_features = torch.tensor(all_features[:n_train], dtype=torch.float)\n",
    "test_features = torch.tensor(all_features[n_train:], dtype=torch.float)\n",
    "train_labels = torch.tensor(y, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "print(train_features.shape, train_labels.shape)\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.lin1 = nn.Linear(100, 300)\n",
    "        self.lin2 = nn.Linear(300, 50)\n",
    "        self.lin3 = nn.Linear(50, 1)\n",
    "#         self.bn_in = nn.BatchNorm1d(100)\n",
    "        self.bn1 = nn.BatchNorm1d(300)\n",
    "        self.bn2 = nn.BatchNorm1d(50)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.bn_in(x_in)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.lin3(x)\n",
    "#         x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "def log_rmse(net, features, labels):\n",
    "    with torch.no_grad():\n",
    "        # 将小于1的值设成1，使得取对数时数值更稳定\n",
    "        clipped_preds = torch.max(net(features), torch.tensor(1.0))\n",
    "        rmse = torch.sqrt(loss((clipped_preds + 1).log(), (labels + 1).log()))\n",
    "    return rmse.item()\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "net = net()\n",
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=lr)\n",
    "train_ls = []\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in train_iter:\n",
    "        l = loss(net(X.float()), y.float())\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    train_ls.append(log_rmse(net, train_features, train_labels))\n",
    "print('Train Loss:\\n', train_ls)\n",
    "\n",
    "\n",
    "net.eval()\n",
    "preds = net(test_features).detach().numpy()\n",
    "sub = pd.DataFrame({'ID': np.arange(0, 600)})\n",
    "sub['revenue'] = pd.Series(preds.reshape(1, -1)[0])\n",
    "sub['revenue'] = np.where(sub['revenue'] <= 0, y_mean, sub['revenue'])\n",
    "print(sub.describe())\n",
    "sub.to_csv('../sub/sub_mlp_{}.csv'.format(time.strftime('%Y%m%d')), index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T11:07:25.511202Z",
     "start_time": "2020-12-06T11:07:25.501229Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_features = torch.tensor(all_features[:n_train], dtype=torch.float)\n",
    "# test_features = torch.tensor(all_features[n_train:], dtype=torch.float)\n",
    "# train_labels = torch.tensor(y, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "# loss = torch.nn.MSELoss()\n",
    "\n",
    "# def get_net(feature_num):\n",
    "#     net = nn.Linear(feature_num, 1)\n",
    "#     for param in net.parameters():\n",
    "#         nn.init.normal_(param, mean=0, std=0.01)\n",
    "#     return net\n",
    "\n",
    "\n",
    "# def log_rmse(net, features, labels):\n",
    "#     with torch.no_grad():\n",
    "#         # 将小于1的值设成1，使得取对数时数值更稳定\n",
    "#         clipped_preds = torch.max(net(features), torch.tensor(1.0))\n",
    "#         rmse = torch.sqrt(loss((clipped_preds + 1).log(), (labels + 1).log()))\n",
    "#     return rmse.item()\n",
    "\n",
    "\n",
    "# def train(net, train_features, train_labels, test_features, test_labels,\n",
    "#           num_epochs, learning_rate, weight_decay, batch_size):\n",
    "#     train_ls, test_ls = [], []\n",
    "#     dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "#     train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "#     # 这里使用了Adam优化算法\n",
    "#     optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "#     net = net.float()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for X, y in train_iter:\n",
    "#             l = loss(net(X.float()), y.float())\n",
    "#             optimizer.zero_grad()\n",
    "#             l.backward()\n",
    "#             optimizer.step()\n",
    "#         train_ls.append(log_rmse(net, train_features, train_labels))\n",
    "#         if test_labels is not None:\n",
    "#             test_ls.append(log_rmse(net, test_features, test_labels))\n",
    "#     return train_ls, test_ls\n",
    "\n",
    "\n",
    "# def get_k_fold_data(k, i, X, y):\n",
    "#     # 返回第i折交叉验证时所需要的训练和验证数据\n",
    "#     assert k > 1\n",
    "#     fold_size = X.shape[0] // k\n",
    "#     X_train, y_train = None, None\n",
    "#     for j in range(k):\n",
    "#         idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "#         X_part, y_part = X[idx, :], y[idx]\n",
    "#         if j == i:\n",
    "#             X_valid, y_valid = X_part, y_part\n",
    "#         elif X_train is None:\n",
    "#             X_train, y_train = X_part, y_part\n",
    "#         else:\n",
    "#             X_train = torch.cat((X_train, X_part), dim=0)\n",
    "#             y_train = torch.cat((y_train, y_part), dim=0)\n",
    "#     return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "\n",
    "# def k_fold(k, X_train, y_train, num_epochs,\n",
    "#            learning_rate, weight_decay, batch_size):\n",
    "#     train_l_sum, valid_l_sum = 0, 0\n",
    "#     for i in range(k):\n",
    "#         data = get_k_fold_data(k, i, X_train, y_train)\n",
    "#         net = get_net(X_train.shape[1])\n",
    "#         train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\n",
    "#                                    weight_decay, batch_size)\n",
    "#         train_l_sum += train_ls[-1]\n",
    "#         valid_l_sum += valid_ls[-1]\n",
    "# #         if i == 0:\n",
    "# #             semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'rmse',\n",
    "# #                          range(1, num_epochs + 1), valid_ls,\n",
    "# #                          ['train', 'valid'])\n",
    "#         print('fold %d, train rmse %f, valid rmse %f' % (i, train_ls[-1], valid_ls[-1]))\n",
    "#     return train_l_sum / k, valid_l_sum / k\n",
    "\n",
    "\n",
    "# k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64\n",
    "# train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr, weight_decay, batch_size)\n",
    "# print('%d-fold validation: avg train rmse %f, avg valid rmse %f' % (k, train_l, valid_l))\n",
    "\n",
    "# def train_and_pred(train_features, test_features, train_labels,\n",
    "#                    num_epochs, lr, weight_decay, batch_size):\n",
    "#     net = get_net(train_features.shape[1])\n",
    "#     train_ls, _ = train(net, train_features, train_labels, None, None,\n",
    "#                         num_epochs, lr, weight_decay, batch_size)\n",
    "# #     semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'rmse')\n",
    "#     print('train rmse %f' % train_ls[-1])\n",
    "#     preds = net(test_features).detach().numpy()\n",
    "#     sub = pd.DataFrame({'ID': np.arange(0, 600)})\n",
    "#     sub['revenue'] = pd.Series(preds.reshape(1, -1)[0])\n",
    "# #     sub['revenue'] = np.expm1(sub['revenue'])\n",
    "# #     sub['revenue'] = np.where(sub['revenue'] <= 0, y_mean, sub['revenue'])\n",
    "#     sub['revenue'] = sub['revenue'].apply(lambda x: y_mean if x <= 0 else x)\n",
    "#     sub['revenue'] = sub['revenue'].astype(int)\n",
    "#     print(sub.describe())\n",
    "#     sub.to_csv('../sub/sub_mlp_{}.csv'.format(time.strftime('%Y%m%d')), index=False, header=False)\n",
    "\n",
    "\n",
    "# train_and_pred(train_features, test_features, train_labels, num_epochs, lr, weight_decay, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
