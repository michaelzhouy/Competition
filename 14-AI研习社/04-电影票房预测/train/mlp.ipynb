{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:01.996504Z",
     "start_time": "2020-11-29T03:56:00.658692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', None)\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:02.019482Z",
     "start_time": "2020-11-29T03:56:01.997501Z"
    }
   },
   "outputs": [],
   "source": [
    "class MeanEncoder:\n",
    "    def __init__(self, categorical_features, n_splits=10, target_type='classification', prior_weight_func=None):\n",
    "        \"\"\"\n",
    "        :param categorical_features: list of str, the name of the categorical columns to encode\n",
    " \n",
    "        :param n_splits: the number of splits used in mean encoding\n",
    " \n",
    "        :param target_type: str, 'regression' or 'classification'\n",
    " \n",
    "        :param prior_weight_func:\n",
    "        a function that takes in the number of observations, and outputs prior weight\n",
    "        when a dict is passed, the default exponential decay function will be used:\n",
    "        k: the number of observations needed for the posterior to be weighted equally as the prior\n",
    "        f: larger f --> smaller slope\n",
    "        \"\"\"\n",
    " \n",
    "        self.categorical_features = categorical_features\n",
    "        self.n_splits = n_splits\n",
    "        self.learned_stats = {}\n",
    " \n",
    "        if target_type == 'classification':\n",
    "            self.target_type = target_type\n",
    "            self.target_values = []\n",
    "        else:\n",
    "            self.target_type = 'regression'\n",
    "            self.target_values = None\n",
    " \n",
    "        if isinstance(prior_weight_func, dict):\n",
    "            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n",
    "        elif callable(prior_weight_func):\n",
    "            self.prior_weight_func = prior_weight_func\n",
    "        else:\n",
    "            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n",
    " \n",
    "    @staticmethod\n",
    "    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n",
    "        X_train = X_train[[variable]].copy()\n",
    "        X_test = X_test[[variable]].copy()\n",
    " \n",
    "        if target is not None:\n",
    "            nf_name = '{}_pred_{}'.format(variable, target)\n",
    "            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n",
    "        else:\n",
    "            nf_name = '{}_pred'.format(variable)\n",
    "            X_train['pred_temp'] = y_train  # regression\n",
    "        prior = X_train['pred_temp'].mean()\n",
    " \n",
    "        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg(['mean', 'size'])\n",
    "        col_avg_y['size'] = prior_weight_func(col_avg_y['size'])\n",
    "        col_avg_y[nf_name] = col_avg_y['size'] * prior + (1 - col_avg_y['size']) * col_avg_y['mean']\n",
    "        col_avg_y.drop(['size', 'mean'], axis=1, inplace=True)\n",
    " \n",
    "        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n",
    "        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n",
    " \n",
    "        return nf_train, nf_test, prior, col_avg_y\n",
    " \n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :param y: pandas Series or numpy array, n_samples\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "        if self.target_type == 'classification':\n",
    "            skf = StratifiedKFold(self.n_splits)\n",
    "        else:\n",
    "            skf = KFold(self.n_splits)\n",
    " \n",
    "        if self.target_type == 'classification':\n",
    "            self.target_values = sorted(set(y))\n",
    "            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n",
    "                                  product(self.categorical_features, self.target_values)}\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(X, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        else:\n",
    "            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(X, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        return X_new\n",
    " \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    " \n",
    "        if self.target_type == 'classification':\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "        else:\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    " \n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:02.315673Z",
     "start_time": "2020-11-29T03:56:02.021438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_mean:  65868516\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../input/train.csv', encoding='latin-1')\n",
    "test = pd.read_csv('../input/test.csv', encoding='latin-1')\n",
    "y_mean = int(train['revenue'].mean())\n",
    "print('y_mean: ', y_mean)\n",
    "\n",
    "# train['revenue'] = np.log1p(train['revenue'])\n",
    "\n",
    "data = pd.concat([train, test], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:02.340606Z",
     "start_time": "2020-11-29T03:56:02.316670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>poster_path</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>cast</th>\n",
       "      <th>crew</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 86780, 'name': 'Clash of the Titans Co...</td>\n",
       "      <td>150000000</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}]</td>\n",
       "      <td>http://www.wrathofthetitansmovie.org</td>\n",
       "      <td>tt1646987</td>\n",
       "      <td>en</td>\n",
       "      <td>Wrath of the Titans</td>\n",
       "      <td>A decade after his heroic defeat of the monstr...</td>\n",
       "      <td>7.739904</td>\n",
       "      <td>/Albfq3ziSCQVyh5PzMSsFmmgHmy.jpg</td>\n",
       "      <td>[{'name': 'Legendary Pictures', 'id': 923}, {'...</td>\n",
       "      <td>[{'iso_3166_1': 'ES', 'name': 'Spain'}, {'iso_...</td>\n",
       "      <td>3/27/12</td>\n",
       "      <td>99.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Feel the Wrath</td>\n",
       "      <td>Wrath of the Titans</td>\n",
       "      <td>[{'id': 1449, 'name': 'underworld'}, {'id': 20...</td>\n",
       "      <td>[{'cast_id': 4, 'character': 'Perseus', 'credi...</td>\n",
       "      <td>[{'credit_id': '52fe4926c3a36847f818b96d', 'de...</td>\n",
       "      <td>301000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35000000</td>\n",
       "      <td>[{'id': 27, 'name': 'Horror'}, {'id': 9648, 'n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt0120681</td>\n",
       "      <td>en</td>\n",
       "      <td>From Hell</td>\n",
       "      <td>Frederick Abberline is an opium-huffing inspec...</td>\n",
       "      <td>7.790140</td>\n",
       "      <td>/f3J77Cy3pRSeeN52Pk8oIvgi6IN.jpg</td>\n",
       "      <td>[{'name': 'Twentieth Century Fox Film Corporat...</td>\n",
       "      <td>[{'iso_3166_1': 'CZ', 'name': 'Czech Republic'...</td>\n",
       "      <td>10/19/01</td>\n",
       "      <td>122.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Only the legend will survive.</td>\n",
       "      <td>From Hell</td>\n",
       "      <td>[{'id': 1465, 'name': 'loss of family'}, {'id'...</td>\n",
       "      <td>[{'cast_id': 19, 'character': 'Inspector Frede...</td>\n",
       "      <td>[{'credit_id': '52fe4273c3a36847f801fbfb', 'de...</td>\n",
       "      <td>74558115.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                              belongs_to_collection     budget  \\\n",
       "0   0  [{'id': 86780, 'name': 'Clash of the Titans Co...  150000000   \n",
       "1   1                                                NaN   35000000   \n",
       "\n",
       "                                              genres  \\\n",
       "0                  [{'id': 12, 'name': 'Adventure'}]   \n",
       "1  [{'id': 27, 'name': 'Horror'}, {'id': 9648, 'n...   \n",
       "\n",
       "                               homepage    imdb_id original_language  \\\n",
       "0  http://www.wrathofthetitansmovie.org  tt1646987                en   \n",
       "1                                   NaN  tt0120681                en   \n",
       "\n",
       "        original_title                                           overview  \\\n",
       "0  Wrath of the Titans  A decade after his heroic defeat of the monstr...   \n",
       "1            From Hell  Frederick Abberline is an opium-huffing inspec...   \n",
       "\n",
       "   popularity                       poster_path  \\\n",
       "0    7.739904  /Albfq3ziSCQVyh5PzMSsFmmgHmy.jpg   \n",
       "1    7.790140  /f3J77Cy3pRSeeN52Pk8oIvgi6IN.jpg   \n",
       "\n",
       "                                production_companies  \\\n",
       "0  [{'name': 'Legendary Pictures', 'id': 923}, {'...   \n",
       "1  [{'name': 'Twentieth Century Fox Film Corporat...   \n",
       "\n",
       "                                production_countries release_date  runtime  \\\n",
       "0  [{'iso_3166_1': 'ES', 'name': 'Spain'}, {'iso_...      3/27/12     99.0   \n",
       "1  [{'iso_3166_1': 'CZ', 'name': 'Czech Republic'...     10/19/01    122.0   \n",
       "\n",
       "                                    spoken_languages    status  \\\n",
       "0           [{'iso_639_1': 'en', 'name': 'English'}]  Released   \n",
       "1  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...  Released   \n",
       "\n",
       "                         tagline                title  \\\n",
       "0                 Feel the Wrath  Wrath of the Titans   \n",
       "1  Only the legend will survive.            From Hell   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  [{'id': 1449, 'name': 'underworld'}, {'id': 20...   \n",
       "1  [{'id': 1465, 'name': 'loss of family'}, {'id'...   \n",
       "\n",
       "                                                cast  \\\n",
       "0  [{'cast_id': 4, 'character': 'Perseus', 'credi...   \n",
       "1  [{'cast_id': 19, 'character': 'Inspector Frede...   \n",
       "\n",
       "                                                crew      revenue  \n",
       "0  [{'credit_id': '52fe4926c3a36847f818b96d', 'de...  301000000.0  \n",
       "1  [{'credit_id': '52fe4273c3a36847f801fbfb', 'de...   74558115.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:02.350579Z",
     "start_time": "2020-11-29T03:56:02.342601Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_cols = ['poster_path', 'imdb_id']\n",
    "data.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时间特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:02.393501Z",
     "start_time": "2020-11-29T03:56:02.352573Z"
    }
   },
   "outputs": [],
   "source": [
    "data['release_year'] = data['release_date'].apply(lambda x: '19' + x.split('/')[2] if int(x.split('/')[2]) > 20 else '20' + x.split('/')[2]).astype(int)\n",
    "data['release_month'] = data['release_date'].apply(lambda x: x.split('/')[0]).astype(int)\n",
    "data['release_day'] = data['release_date'].apply(lambda x: x.split('/')[1]).astype(int)\n",
    "\n",
    "data['release_date'] = pd.to_datetime(data['release_year'].astype(str) + '-' + data['release_month'].astype(str) + '-' + data['release_day'].astype(str))\n",
    "\n",
    "data['release_date_weekday'] = data['release_date'].apply(lambda x: x.weekday())\n",
    "data['release_date_TONOW'] = (datetime.now() - data['release_date']).dt.days\n",
    "\n",
    "data.drop(['release_day', 'release_date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 判断是否为空"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:02.405432Z",
     "start_time": "2020-11-29T03:56:02.395459Z"
    }
   },
   "outputs": [],
   "source": [
    "isnull_cols = ['homepage', 'tagline', 'belongs_to_collection', 'overview']\n",
    "for i in isnull_cols:\n",
    "    data[i + '_isnull'] = data[i].isnull().astype(int)\n",
    "data.drop(isnull_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数值特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:02.412451Z",
     "start_time": "2020-11-29T03:56:02.407428Z"
    }
   },
   "outputs": [],
   "source": [
    "num_cols = ['runtime', 'popularity', 'budget']\n",
    "\n",
    "for i in num_cols:\n",
    "    data[i] = np.log1p(data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类别特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:02.420393Z",
     "start_time": "2020-11-29T03:56:02.413412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            Wrath of the Titans\n",
       "1                                      From Hell\n",
       "2                   Guess Who's Coming to Dinner\n",
       "3    Talladega Nights: The Ballad of Ricky Bobby\n",
       "4                                         Xanadu\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:02.428413Z",
     "start_time": "2020-11-29T03:56:02.421390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'].isnull().sum(), data['original_title'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:02.453305Z",
     "start_time": "2020-11-29T03:56:02.429369Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 668.84it/s]\n"
     ]
    }
   ],
   "source": [
    "cat_cols = ['original_language', 'status', 'title', 'original_title']\n",
    "\n",
    "data['title=original_title'] = (data['title'] == data['original_title']).astype(int)\n",
    "\n",
    "for i in tqdm(['original_language', 'status']):\n",
    "    le = LabelEncoder()\n",
    "    data[i] = le.fit_transform(data[i])\n",
    "\n",
    "data['original_language_count'] = data['original_language'].map(data['original_language'].value_counts())\n",
    "data['release_year_count'] = data['release_year'].map(data['release_year'].value_counts())\n",
    "data['release_month_count'] = data['release_month'].map(data['release_month'].value_counts())\n",
    "data['release_date_weekday_count'] = data['release_date_weekday'].map(data['release_date_weekday'].value_counts())\n",
    "data['title_count'] = data['title'].map(data['title'].value_counts())\n",
    "\n",
    "data.drop(['title', 'original_title'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 嵌套特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:05.556288Z",
     "start_time": "2020-11-29T03:56:02.454302Z"
    }
   },
   "outputs": [],
   "source": [
    "nested_cols = ['genres', 'production_companies', 'production_countries',\n",
    "               'Keywords', 'spoken_languages', 'cast', 'crew']\n",
    "for i in nested_cols:\n",
    "    data[i + '_length'] = data[i].apply(lambda x: 0 if pd.isnull(x) else len(eval(x)))\n",
    "\n",
    "# data['genres_0'] = data['genres'].apply(lambda x: np.nan if pd.isnull(x) else eval(x)[0]['name'])\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# data['genres_0'] = le.fit_transform(data['genres_0'].astype(str))\n",
    "# data['genres_0_count'] = data['genres_0'].map(data['genres_0'].value_counts())\n",
    "\n",
    "# data.drop(nested_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:20.606739Z",
     "start_time": "2020-11-29T03:56:05.557055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genres\n",
      "production_companies\n",
      "production_countries\n",
      "Keywords\n",
      "spoken_languages\n",
      "cast\n",
      "crew\n"
     ]
    }
   ],
   "source": [
    "def get_name(x):\n",
    "    if pd.isnull(x):\n",
    "        return []\n",
    "    else:\n",
    "        df = pd.DataFrame(eval(x))\n",
    "        if 'name' in df.columns:\n",
    "#             df['name'] = df['name'].apply(lambda s: ''.join(s.split()))\n",
    "            return df['name'].tolist()\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "\n",
    "for i in nested_cols:\n",
    "    print(i)\n",
    "    data[i + '_name'] = data[i].apply(lambda x: get_name(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:24.003764Z",
     "start_time": "2020-11-29T03:56:20.607668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genres_name\n",
      "Start tfidf ...\n",
      "production_companies_name\n",
      "Start tfidf ...\n",
      "production_countries_name\n",
      "Start tfidf ...\n",
      "Keywords_name\n",
      "Start tfidf ...\n",
      "spoken_languages_name\n",
      "Start tfidf ...\n",
      "cast_name\n",
      "Start tfidf ...\n",
      "crew_name\n",
      "Start tfidf ...\n",
      "genres_name\n",
      "Start count2vec ...\n",
      "production_companies_name\n",
      "Start count2vec ...\n",
      "production_countries_name\n",
      "Start count2vec ...\n",
      "Keywords_name\n",
      "Start count2vec ...\n",
      "spoken_languages_name\n",
      "Start count2vec ...\n",
      "cast_name\n",
      "Start count2vec ...\n",
      "crew_name\n",
      "Start count2vec ...\n"
     ]
    }
   ],
   "source": [
    "def tfidf_emb(df_, cat_col, emb_size=10, seed=1024):\n",
    "    print('Start tfidf ...')\n",
    "    df = df_.copy()\n",
    "    df[cat_col] = df[cat_col].fillna('-1')\n",
    "    df[cat_col] = df[cat_col].apply(lambda x: ' '.join(x))\n",
    "    tfidf_enc = TfidfVectorizer()\n",
    "    tfidf_vec = tfidf_enc.fit_transform(df[cat_col])\n",
    "    svd_enc = TruncatedSVD(n_components=emb_size, n_iter=20, random_state=seed)\n",
    "    svd_vec = svd_enc.fit_transform(tfidf_vec)\n",
    "    tfidf_df = pd.DataFrame(svd_vec)\n",
    "    tfidf_df.columns = ['{}_tfidf_{}'.format(cat_col, i) for i in range(emb_size)]\n",
    "    res = tfidf_df\n",
    "    return res\n",
    "\n",
    "\n",
    "def count2vec_emb(df_, cat_col, emb_size=10, seed=1024):\n",
    "    print('Start count2vec ...')\n",
    "    df = df_.copy()\n",
    "    df[cat_col] = df[cat_col].fillna('-1')\n",
    "    df[cat_col] = df[cat_col].apply(lambda x: ' '.join(x))\n",
    "    count_enc = CountVectorizer()\n",
    "    count_vec = count_enc.fit_transform(df[cat_col])\n",
    "    svd_enc = TruncatedSVD(n_components=emb_size, n_iter=20, random_state=seed)\n",
    "    svd_vec = svd_enc.fit_transform(count_vec)\n",
    "    c2v_df = pd.DataFrame(svd_vec)\n",
    "    c2v_df.columns = ['{}_count2vec_{}'.format(cat_col, i) for i in range(emb_size)]\n",
    "    res = c2v_df\n",
    "    return res\n",
    "\n",
    "\n",
    "for i in [i + '_name' for i in nested_cols]:\n",
    "    print(i)\n",
    "    tfidf_df = tfidf_emb(data, i, emb_size=10, seed=1024)\n",
    "    data = pd.concat([data, tfidf_df], axis=1)\n",
    "\n",
    "for i in [i + '_name' for i in nested_cols]:\n",
    "    print(i)\n",
    "    c2v_df = count2vec_emb(data, i, emb_size=10, seed=1024)\n",
    "    data = pd.concat([data, c2v_df], axis=1)\n",
    "\n",
    "data.drop(nested_cols, axis=1, inplace=True)\n",
    "data.drop([i + '_name' for i in nested_cols], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:24.024709Z",
     "start_time": "2020-11-29T03:56:24.004762Z"
    }
   },
   "outputs": [],
   "source": [
    "train = data[data['revenue'].notnull()]\n",
    "test = data[data['revenue'].isnull()]\n",
    "\n",
    "train.fillna(-999, inplace=True)\n",
    "test.fillna(-999, inplace=True)\n",
    "\n",
    "used_cols = [i for i in train.columns if i not in ['id', 'release_date', 'revenue']]\n",
    "y = train['revenue']\n",
    "train = train[used_cols]\n",
    "test = test[used_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:24.335351Z",
     "start_time": "2020-11-29T03:56:24.026620Z"
    }
   },
   "outputs": [],
   "source": [
    "class_list = ['release_year', 'release_month', 'release_date_weekday', 'original_language']\n",
    "\n",
    "ME = MeanEncoder(categorical_features=class_list, n_splits=5, target_type='regression', prior_weight_func=None)\n",
    "train = ME.fit_transform(train, y)\n",
    "test = ME.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:24.341342Z",
     "start_time": "2020-11-29T03:56:24.336347Z"
    }
   },
   "outputs": [],
   "source": [
    "train['revenue'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:24.744027Z",
     "start_time": "2020-11-29T03:56:24.342333Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 10.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# 暂且选择这三种编码\n",
    "enc_cols = []\n",
    "stats_default_dict = {\n",
    "    'max': train['revenue'].max(),\n",
    "    'min': train['revenue'].min(),\n",
    "    'median': train['revenue'].median(),\n",
    "    'mean': train['revenue'].mean(),\n",
    "    'sum': train['revenue'].sum(),\n",
    "    'std': train['revenue'].std(),\n",
    "    'skew': train['revenue'].skew(),\n",
    "    'kurt': train['revenue'].kurt(),\n",
    "    'mad': train['revenue'].mad()\n",
    "}\n",
    "enc_stats = ['max', 'min', 'mean']\n",
    "skf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for f in tqdm(['release_year', 'release_month', 'release_date_weekday', 'original_language']):\n",
    "    enc_dict = {}\n",
    "    for stat in enc_stats:\n",
    "        enc_dict['{}_target_{}'.format(f, stat)] = stat\n",
    "        train['{}_target_{}'.format(f, stat)] = 0\n",
    "        test['{}_target_{}'.format(f, stat)] = 0\n",
    "        enc_cols.append('{}_target_{}'.format(f, stat))\n",
    "    for i, (trn_idx, val_idx) in enumerate(skf.split(train, y)):\n",
    "        trn_x, val_x = train.iloc[trn_idx].reset_index(drop=True), train.iloc[val_idx].reset_index(drop=True)\n",
    "        enc_df = trn_x.groupby(f, as_index=False)['revenue'].agg(enc_dict)\n",
    "        val_x = val_x[[f]].merge(enc_df, on=f, how='left')\n",
    "        test_x = test[[f]].merge(enc_df, on=f, how='left')\n",
    "        for stat in enc_stats:\n",
    "            val_x['{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].fillna(stats_default_dict[stat])\n",
    "            test_x['{}_target_{}'.format(f, stat)] = test_x['{}_target_{}'.format(f, stat)].fillna(stats_default_dict[stat])\n",
    "            train.loc[val_idx, '{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].values\n",
    "            test['{}_target_{}'.format(f, stat)] += test_x['{}_target_{}'.format(f, stat)].values / skf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:24.786666Z",
     "start_time": "2020-11-29T03:56:24.744956Z"
    }
   },
   "outputs": [],
   "source": [
    "n_train = train.shape[0]\n",
    "\n",
    "train.drop('revenue', axis=1, inplace=True)\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(pd.concat([train, test]).values)\n",
    "all_data = min_max_scaler.transform(pd.concat([train, test]).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:24.868750Z",
     "start_time": "2020-11-29T03:56:24.787661Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "all_features = pca.fit_transform(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:47.119498Z",
     "start_time": "2020-11-29T03:56:24.869747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0, train rmse 9.016912, valid rmse 9.098090\n",
      "fold 1, train rmse 9.033643, valid rmse 9.083103\n",
      "fold 2, train rmse 9.028946, valid rmse 9.146161\n",
      "fold 3, train rmse 9.041874, valid rmse 9.004050\n",
      "fold 4, train rmse 9.088479, valid rmse 8.974241\n",
      "5-fold validation: avg train rmse 9.041971, avg valid rmse 9.061129\n",
      "train rmse 8.878341\n",
      "               ID       revenue\n",
      "count  600.000000  6.000000e+02\n",
      "mean   299.500000  1.429278e+07\n",
      "std    173.349358  2.714754e+07\n",
      "min      0.000000  1.970000e+02\n",
      "25%    149.750000  1.569000e+04\n",
      "50%    299.500000  2.995750e+04\n",
      "75%    449.250000  6.686350e+04\n",
      "max    599.000000  6.586852e+07\n"
     ]
    }
   ],
   "source": [
    "train_features = torch.tensor(all_features[:n_train], dtype=torch.float)\n",
    "test_features = torch.tensor(all_features[n_train:], dtype=torch.float)\n",
    "train_labels = torch.tensor(y, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "def get_net(feature_num):\n",
    "    net = nn.Linear(feature_num, 1)\n",
    "    for param in net.parameters():\n",
    "        nn.init.normal_(param, mean=0, std=0.01)\n",
    "    return net\n",
    "\n",
    "\n",
    "def log_rmse(net, features, labels):\n",
    "    with torch.no_grad():\n",
    "        # 将小于1的值设成1，使得取对数时数值更稳定\n",
    "        clipped_preds = torch.max(net(features), torch.tensor(1.0))\n",
    "        rmse = torch.sqrt(loss((clipped_preds + 1).log(), (labels + 1).log()))\n",
    "    return rmse.item()\n",
    "\n",
    "\n",
    "def train(net, train_features, train_labels, test_features, test_labels,\n",
    "          num_epochs, learning_rate, weight_decay, batch_size):\n",
    "    train_ls, test_ls = [], []\n",
    "    dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "    # 这里使用了Adam优化算法\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    net = net.float()\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            l = loss(net(X.float()), y.float())\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "        train_ls.append(log_rmse(net, train_features, train_labels))\n",
    "        if test_labels is not None:\n",
    "            test_ls.append(log_rmse(net, test_features, test_labels))\n",
    "    return train_ls, test_ls\n",
    "\n",
    "\n",
    "def get_k_fold_data(k, i, X, y):\n",
    "    # 返回第i折交叉验证时所需要的训练和验证数据\n",
    "    assert k > 1\n",
    "    fold_size = X.shape[0] // k\n",
    "    X_train, y_train = None, None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "        X_part, y_part = X[idx, :], y[idx]\n",
    "        if j == i:\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train is None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            X_train = torch.cat((X_train, X_part), dim=0)\n",
    "            y_train = torch.cat((y_train, y_part), dim=0)\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "\n",
    "def k_fold(k, X_train, y_train, num_epochs,\n",
    "           learning_rate, weight_decay, batch_size):\n",
    "    train_l_sum, valid_l_sum = 0, 0\n",
    "    for i in range(k):\n",
    "        data = get_k_fold_data(k, i, X_train, y_train)\n",
    "        net = get_net(X_train.shape[1])\n",
    "        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\n",
    "                                   weight_decay, batch_size)\n",
    "        train_l_sum += train_ls[-1]\n",
    "        valid_l_sum += valid_ls[-1]\n",
    "#         if i == 0:\n",
    "#             semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'rmse',\n",
    "#                          range(1, num_epochs + 1), valid_ls,\n",
    "#                          ['train', 'valid'])\n",
    "        print('fold %d, train rmse %f, valid rmse %f' % (i, train_ls[-1], valid_ls[-1]))\n",
    "    return train_l_sum / k, valid_l_sum / k\n",
    "\n",
    "\n",
    "k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64\n",
    "train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr, weight_decay, batch_size)\n",
    "print('%d-fold validation: avg train rmse %f, avg valid rmse %f' % (k, train_l, valid_l))\n",
    "\n",
    "def train_and_pred(train_features, test_features, train_labels,\n",
    "                   num_epochs, lr, weight_decay, batch_size):\n",
    "    net = get_net(train_features.shape[1])\n",
    "    train_ls, _ = train(net, train_features, train_labels, None, None,\n",
    "                        num_epochs, lr, weight_decay, batch_size)\n",
    "#     semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'rmse')\n",
    "    print('train rmse %f' % train_ls[-1])\n",
    "    preds = net(test_features).detach().numpy()\n",
    "    sub = pd.DataFrame({'ID': np.arange(0, 600)})\n",
    "    sub['revenue'] = pd.Series(preds.reshape(1, -1)[0])\n",
    "#     sub['revenue'] = np.expm1(sub['revenue'])\n",
    "#     sub['revenue'] = np.where(sub['revenue'] <= 0, y_mean, sub['revenue'])\n",
    "    sub['revenue'] = sub['revenue'].apply(lambda x: y_mean if x <= 0 else x)\n",
    "    sub['revenue'] = sub['revenue'].astype(int)\n",
    "    print(sub.describe())\n",
    "    sub.to_csv('../sub/sub_{}.csv'.format(time.strftime('%Y%m%d')), index=False, header=False)\n",
    "\n",
    "\n",
    "train_and_pred(train_features, test_features, train_labels, num_epochs, lr, weight_decay, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T03:56:47.128473Z",
     "start_time": "2020-11-29T03:56:47.123487Z"
    }
   },
   "outputs": [],
   "source": [
    "# train.drop('revenue', axis=1, inplace=True)\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(train, y, random_state=2020)\n",
    "\n",
    "# dtrain = lgb.Dataset(X_train, y_train)\n",
    "# dvalid = lgb.Dataset(X_valid, y_valid, reference=dtrain)\n",
    "\n",
    "# params = {\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'objective': 'regression',\n",
    "#     'metric': 'None',\n",
    "#     'learning_rate': 0.05,\n",
    "#     'seed': 2020\n",
    "# }\n",
    "\n",
    "# def rmsle(y_hat, data):\n",
    "#     y_true = data.get_label()\n",
    "#     y_hat = np.where(y_hat < 0, 1, y_hat)\n",
    "#     y_true = np.where(y_true < 0, 1, y_true)\n",
    "#     res = -np.sqrt(mean_squared_log_error(y_true, y_hat))\n",
    "#     return 'rmsle', res, True\n",
    "\n",
    "# model = lgb.train(\n",
    "#     params,\n",
    "#     dtrain,\n",
    "#     valid_sets=[dtrain, dvalid],\n",
    "#     num_boost_round=1000000,\n",
    "#     early_stopping_rounds=100,\n",
    "#     verbose_eval=50,\n",
    "#     feval=rmsle\n",
    "# )\n",
    "\n",
    "# pred = model.predict(test)\n",
    "\n",
    "# sub = pd.DataFrame()\n",
    "# sub['ID'] = np.arange(0, 600)\n",
    "# sub['revenue'] = pred\n",
    "# sub['revenue'] = np.expm1(sub['revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-29T03:29:43.431Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
