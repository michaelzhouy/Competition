{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:17:25.738977Z",
     "start_time": "2020-12-06T15:17:19.875077Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:17:25.792061Z",
     "start_time": "2020-12-06T15:17:25.738977Z"
    }
   },
   "outputs": [],
   "source": [
    "class MeanEncoder:\n",
    "    def __init__(self, categorical_features, n_splits=10, target_type='classification', prior_weight_func=None):\n",
    "        \"\"\"\n",
    "        :param categorical_features: list of str, the name of the categorical columns to encode\n",
    " \n",
    "        :param n_splits: the number of splits used in mean encoding\n",
    " \n",
    "        :param target_type: str, 'regression' or 'classification'\n",
    " \n",
    "        :param prior_weight_func:\n",
    "        a function that takes in the number of observations, and outputs prior weight\n",
    "        when a dict is passed, the default exponential decay function will be used:\n",
    "        k: the number of observations needed for the posterior to be weighted equally as the prior\n",
    "        f: larger f --> smaller slope\n",
    "        \"\"\"\n",
    " \n",
    "        self.categorical_features = categorical_features\n",
    "        self.n_splits = n_splits\n",
    "        self.learned_stats = {}\n",
    " \n",
    "        if target_type == 'classification':\n",
    "            self.target_type = target_type\n",
    "            self.target_values = []\n",
    "        else:\n",
    "            self.target_type = 'regression'\n",
    "            self.target_values = None\n",
    " \n",
    "        if isinstance(prior_weight_func, dict):\n",
    "            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n",
    "        elif callable(prior_weight_func):\n",
    "            self.prior_weight_func = prior_weight_func\n",
    "        else:\n",
    "            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n",
    " \n",
    "    @staticmethod\n",
    "    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n",
    "        X_train = X_train[[variable]].copy()\n",
    "        X_test = X_test[[variable]].copy()\n",
    " \n",
    "        if target is not None:\n",
    "            nf_name = '{}_pred_{}'.format(variable, target)\n",
    "            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n",
    "        else:\n",
    "            nf_name = '{}_pred'.format(variable)\n",
    "            X_train['pred_temp'] = y_train  # regression\n",
    "        prior = X_train['pred_temp'].mean()\n",
    " \n",
    "        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg(['mean', 'size'])\n",
    "        col_avg_y['size'] = prior_weight_func(col_avg_y['size'])\n",
    "        col_avg_y[nf_name] = col_avg_y['size'] * prior + (1 - col_avg_y['size']) * col_avg_y['mean']\n",
    "        col_avg_y.drop(['size', 'mean'], axis=1, inplace=True)\n",
    " \n",
    "        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n",
    "        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n",
    " \n",
    "        return nf_train, nf_test, prior, col_avg_y\n",
    " \n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :param y: pandas Series or numpy array, n_samples\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "        if self.target_type == 'classification':\n",
    "            skf = StratifiedKFold(self.n_splits)\n",
    "        else:\n",
    "            skf = KFold(self.n_splits)\n",
    " \n",
    "        if self.target_type == 'classification':\n",
    "            self.target_values = sorted(set(y))\n",
    "            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n",
    "                                  product(self.categorical_features, self.target_values)}\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(X, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        else:\n",
    "            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(X, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        return X_new\n",
    " \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    " \n",
    "        if self.target_type == 'classification':\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "        else:\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    " \n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:17:26.621129Z",
     "start_time": "2020-12-06T15:17:25.797059Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv', encoding='latin-1')\n",
    "test = pd.read_csv('../input/test.csv', encoding='latin-1')\n",
    "\n",
    "y_mean = train['revenue'].mean()\n",
    "train['revenue'] = np.log1p(train['revenue'])\n",
    "\n",
    "data = pd.concat([train, test], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:17:26.685331Z",
     "start_time": "2020-12-06T15:17:26.621129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>poster_path</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>cast</th>\n",
       "      <th>crew</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 86780, 'name': 'Clash of the Titans Co...</td>\n",
       "      <td>150000000</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}]</td>\n",
       "      <td>http://www.wrathofthetitansmovie.org</td>\n",
       "      <td>tt1646987</td>\n",
       "      <td>en</td>\n",
       "      <td>Wrath of the Titans</td>\n",
       "      <td>A decade after his heroic defeat of the monstr...</td>\n",
       "      <td>7.739904</td>\n",
       "      <td>/Albfq3ziSCQVyh5PzMSsFmmgHmy.jpg</td>\n",
       "      <td>[{'name': 'Legendary Pictures', 'id': 923}, {'...</td>\n",
       "      <td>[{'iso_3166_1': 'ES', 'name': 'Spain'}, {'iso_...</td>\n",
       "      <td>3/27/12</td>\n",
       "      <td>99.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Feel the Wrath</td>\n",
       "      <td>Wrath of the Titans</td>\n",
       "      <td>[{'id': 1449, 'name': 'underworld'}, {'id': 20...</td>\n",
       "      <td>[{'cast_id': 4, 'character': 'Perseus', 'credi...</td>\n",
       "      <td>[{'credit_id': '52fe4926c3a36847f818b96d', 'de...</td>\n",
       "      <td>19.522621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35000000</td>\n",
       "      <td>[{'id': 27, 'name': 'Horror'}, {'id': 9648, 'n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt0120681</td>\n",
       "      <td>en</td>\n",
       "      <td>From Hell</td>\n",
       "      <td>Frederick Abberline is an opium-huffing inspec...</td>\n",
       "      <td>7.790140</td>\n",
       "      <td>/f3J77Cy3pRSeeN52Pk8oIvgi6IN.jpg</td>\n",
       "      <td>[{'name': 'Twentieth Century Fox Film Corporat...</td>\n",
       "      <td>[{'iso_3166_1': 'CZ', 'name': 'Czech Republic'...</td>\n",
       "      <td>10/19/01</td>\n",
       "      <td>122.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Only the legend will survive.</td>\n",
       "      <td>From Hell</td>\n",
       "      <td>[{'id': 1465, 'name': 'loss of family'}, {'id'...</td>\n",
       "      <td>[{'cast_id': 19, 'character': 'Inspector Frede...</td>\n",
       "      <td>[{'credit_id': '52fe4273c3a36847f801fbfb', 'de...</td>\n",
       "      <td>18.127089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                              belongs_to_collection     budget  \\\n",
       "0   0  [{'id': 86780, 'name': 'Clash of the Titans Co...  150000000   \n",
       "1   1                                                NaN   35000000   \n",
       "\n",
       "                                              genres  \\\n",
       "0                  [{'id': 12, 'name': 'Adventure'}]   \n",
       "1  [{'id': 27, 'name': 'Horror'}, {'id': 9648, 'n...   \n",
       "\n",
       "                               homepage    imdb_id original_language  \\\n",
       "0  http://www.wrathofthetitansmovie.org  tt1646987                en   \n",
       "1                                   NaN  tt0120681                en   \n",
       "\n",
       "        original_title                                           overview  \\\n",
       "0  Wrath of the Titans  A decade after his heroic defeat of the monstr...   \n",
       "1            From Hell  Frederick Abberline is an opium-huffing inspec...   \n",
       "\n",
       "   popularity                       poster_path  \\\n",
       "0    7.739904  /Albfq3ziSCQVyh5PzMSsFmmgHmy.jpg   \n",
       "1    7.790140  /f3J77Cy3pRSeeN52Pk8oIvgi6IN.jpg   \n",
       "\n",
       "                                production_companies  \\\n",
       "0  [{'name': 'Legendary Pictures', 'id': 923}, {'...   \n",
       "1  [{'name': 'Twentieth Century Fox Film Corporat...   \n",
       "\n",
       "                                production_countries release_date  runtime  \\\n",
       "0  [{'iso_3166_1': 'ES', 'name': 'Spain'}, {'iso_...      3/27/12     99.0   \n",
       "1  [{'iso_3166_1': 'CZ', 'name': 'Czech Republic'...     10/19/01    122.0   \n",
       "\n",
       "                                    spoken_languages    status  \\\n",
       "0           [{'iso_639_1': 'en', 'name': 'English'}]  Released   \n",
       "1  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...  Released   \n",
       "\n",
       "                         tagline                title  \\\n",
       "0                 Feel the Wrath  Wrath of the Titans   \n",
       "1  Only the legend will survive.            From Hell   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  [{'id': 1449, 'name': 'underworld'}, {'id': 20...   \n",
       "1  [{'id': 1465, 'name': 'loss of family'}, {'id'...   \n",
       "\n",
       "                                                cast  \\\n",
       "0  [{'cast_id': 4, 'character': 'Perseus', 'credi...   \n",
       "1  [{'cast_id': 19, 'character': 'Inspector Frede...   \n",
       "\n",
       "                                                crew    revenue  \n",
       "0  [{'credit_id': '52fe4926c3a36847f818b96d', 'de...  19.522621  \n",
       "1  [{'credit_id': '52fe4273c3a36847f801fbfb', 'de...  18.127089  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:17:26.703319Z",
     "start_time": "2020-12-06T15:17:26.688329Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_cols = ['poster_path', 'imdb_id']\n",
    "data.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时间特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:17:26.840242Z",
     "start_time": "2020-12-06T15:17:26.706318Z"
    }
   },
   "outputs": [],
   "source": [
    "data['release_year'] = data['release_date'].apply(lambda x: '19' + x.split('/')[2] if int(x.split('/')[2]) > 20 else '20' + x.split('/')[2]).astype(int)\n",
    "data['release_month'] = data['release_date'].apply(lambda x: x.split('/')[0]).astype(int)\n",
    "data['release_day'] = data['release_date'].apply(lambda x: x.split('/')[1]).astype(int)\n",
    "\n",
    "data['release_date'] = pd.to_datetime(data['release_year'].astype(str) + '-' + data['release_month'].astype(str) + '-' + data['release_day'].astype(str))\n",
    "\n",
    "data['release_date_weekday'] = data['release_date'].apply(lambda x: x.weekday())\n",
    "data['release_date_TONOW'] = (datetime.now() - data['release_date']).dt.days\n",
    "\n",
    "data.drop(['release_day', 'release_date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 判断是否为空"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:17:26.864228Z",
     "start_time": "2020-12-06T15:17:26.843240Z"
    }
   },
   "outputs": [],
   "source": [
    "isnull_cols = ['homepage', 'tagline', 'belongs_to_collection', 'overview']\n",
    "for i in isnull_cols:\n",
    "    data[i + '_isnull'] = data[i].isnull().astype(int)\n",
    "data.drop(isnull_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数值特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:17:27.021659Z",
     "start_time": "2020-12-06T15:17:26.868227Z"
    }
   },
   "outputs": [],
   "source": [
    "num_cols = ['runtime', 'popularity', 'budget']\n",
    "\n",
    "for i in num_cols:\n",
    "    data[i] = np.log1p(data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类别特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:17:27.036650Z",
     "start_time": "2020-12-06T15:17:27.026656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            Wrath of the Titans\n",
       "1                                      From Hell\n",
       "2                   Guess Who's Coming to Dinner\n",
       "3    Talladega Nights: The Ballad of Ricky Bobby\n",
       "4                                         Xanadu\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:17:27.058638Z",
     "start_time": "2020-12-06T15:17:27.041647Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'].isnull().sum(), data['original_title'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:17:27.171572Z",
     "start_time": "2020-12-06T15:17:27.063636Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 250.17it/s]\n"
     ]
    }
   ],
   "source": [
    "cat_cols = ['original_language', 'status', 'title', 'original_title']\n",
    "\n",
    "data['title=original_title'] = (data['title'] == data['original_title']).astype(int)\n",
    "\n",
    "for i in tqdm(['original_language', 'status']):\n",
    "    le = LabelEncoder()\n",
    "    data[i] = le.fit_transform(data[i])\n",
    "\n",
    "data['original_language_count'] = data['original_language'].map(data['original_language'].value_counts())\n",
    "data['release_year_count'] = data['release_year'].map(data['release_year'].value_counts())\n",
    "data['release_month_count'] = data['release_month'].map(data['release_month'].value_counts())\n",
    "data['release_date_weekday_count'] = data['release_date_weekday'].map(data['release_date_weekday'].value_counts())\n",
    "data['title_count'] = data['title'].map(data['title'].value_counts())\n",
    "\n",
    "data.drop(['title', 'original_title'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 嵌套特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:17:41.010230Z",
     "start_time": "2020-12-06T15:17:27.174570Z"
    }
   },
   "outputs": [],
   "source": [
    "nested_cols = ['genres', 'production_companies', 'production_countries',\n",
    "               'Keywords', 'spoken_languages', 'cast', 'crew']\n",
    "for i in nested_cols:\n",
    "    data[i + '_length'] = data[i].apply(lambda x: 0 if pd.isnull(x) else len(eval(x)))\n",
    "\n",
    "# data['genres_0'] = data['genres'].apply(lambda x: np.nan if pd.isnull(x) else eval(x)[0]['name'])\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# data['genres_0'] = le.fit_transform(data['genres_0'].astype(str))\n",
    "# data['genres_0_count'] = data['genres_0'].map(data['genres_0'].value_counts())\n",
    "\n",
    "# data.drop(nested_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:18:54.660327Z",
     "start_time": "2020-12-06T15:17:41.013225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genres\n",
      "production_companies\n",
      "production_countries\n",
      "Keywords\n",
      "spoken_languages\n",
      "cast\n",
      "crew\n"
     ]
    }
   ],
   "source": [
    "def get_name(x):\n",
    "    if pd.isnull(x):\n",
    "        return []\n",
    "    else:\n",
    "        df = pd.DataFrame(eval(x))\n",
    "        if 'name' in df.columns:\n",
    "#             df['name'] = df['name'].apply(lambda s: ''.join(s.split()))\n",
    "            return df['name'].tolist()\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "\n",
    "for i in nested_cols:\n",
    "    print(i)\n",
    "    data[i + '_name'] = data[i].apply(lambda x: get_name(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:19:09.815250Z",
     "start_time": "2020-12-06T15:18:54.663326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genres_name\n",
      "Start tfidf ...\n",
      "production_companies_name\n",
      "Start tfidf ...\n",
      "production_countries_name\n",
      "Start tfidf ...\n",
      "Keywords_name\n",
      "Start tfidf ...\n",
      "spoken_languages_name\n",
      "Start tfidf ...\n",
      "cast_name\n",
      "Start tfidf ...\n",
      "crew_name\n",
      "Start tfidf ...\n",
      "genres_name\n",
      "Start count2vec ...\n",
      "production_companies_name\n",
      "Start count2vec ...\n",
      "production_countries_name\n",
      "Start count2vec ...\n",
      "Keywords_name\n",
      "Start count2vec ...\n",
      "spoken_languages_name\n",
      "Start count2vec ...\n",
      "cast_name\n",
      "Start count2vec ...\n",
      "crew_name\n",
      "Start count2vec ...\n"
     ]
    }
   ],
   "source": [
    "def tfidf_emb(df_, cat_col, emb_size=15, seed=1024):\n",
    "    print('Start tfidf ...')\n",
    "    df = df_.copy()\n",
    "    df[cat_col] = df[cat_col].fillna('-1')\n",
    "    df[cat_col] = df[cat_col].apply(lambda x: ' '.join(x))\n",
    "    tfidf_enc = TfidfVectorizer()\n",
    "    tfidf_vec = tfidf_enc.fit_transform(df[cat_col])\n",
    "    svd_enc = TruncatedSVD(n_components=emb_size, n_iter=20, random_state=seed)\n",
    "    svd_vec = svd_enc.fit_transform(tfidf_vec)\n",
    "    tfidf_df = pd.DataFrame(svd_vec)\n",
    "    tfidf_df.columns = ['{}_tfidf_{}'.format(cat_col, i) for i in range(emb_size)]\n",
    "    res = tfidf_df\n",
    "    return res\n",
    "\n",
    "\n",
    "def count2vec_emb(df_, cat_col, emb_size=15, seed=1024):\n",
    "    print('Start count2vec ...')\n",
    "    df = df_.copy()\n",
    "    df[cat_col] = df[cat_col].fillna('-1')\n",
    "    df[cat_col] = df[cat_col].apply(lambda x: ' '.join(x))\n",
    "    count_enc = CountVectorizer()\n",
    "    count_vec = count_enc.fit_transform(df[cat_col])\n",
    "    svd_enc = TruncatedSVD(n_components=emb_size, n_iter=20, random_state=seed)\n",
    "    svd_vec = svd_enc.fit_transform(count_vec)\n",
    "    c2v_df = pd.DataFrame(svd_vec)\n",
    "    c2v_df.columns = ['{}_count2vec_{}'.format(cat_col, i) for i in range(emb_size)]\n",
    "    res = c2v_df\n",
    "    return res\n",
    "\n",
    "\n",
    "for i in [i + '_name' for i in nested_cols]:\n",
    "    print(i)\n",
    "    tfidf_df = tfidf_emb(data, i, emb_size=10, seed=1024)\n",
    "    data = pd.concat([data, tfidf_df], axis=1)\n",
    "\n",
    "for i in [i + '_name' for i in nested_cols]:\n",
    "    print(i)\n",
    "    c2v_df = count2vec_emb(data, i, emb_size=10, seed=1024)\n",
    "    data = pd.concat([data, c2v_df], axis=1)\n",
    "\n",
    "data.drop(nested_cols, axis=1, inplace=True)\n",
    "data.drop([i + '_name' for i in nested_cols], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:19:12.652474Z",
     "start_time": "2020-12-06T15:19:09.815250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_language\n",
      "release_year\n",
      "release_date_weekday\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|████████████████████████████                                                        | 1/3 [00:00<00:00,  7.00it/s]\u001b[A\n",
      " 67%|████████████████████████████████████████████████████████                            | 2/3 [00:00<00:00,  6.95it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  5.24it/s]\u001b[A\n",
      " 33%|████████████████████████████                                                        | 1/3 [00:00<00:01,  1.71it/s]\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|████████████████████████████                                                        | 1/3 [00:00<00:00,  4.86it/s]\u001b[A\n",
      " 67%|████████████████████████████████████████████████████████                            | 2/3 [00:00<00:00,  4.12it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  3.46it/s]\u001b[A\n",
      " 67%|████████████████████████████████████████████████████████                            | 2/3 [00:01<00:00,  1.48it/s]\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|████████████████████████████                                                        | 1/3 [00:00<00:00,  5.53it/s]\u001b[A\n",
      " 67%|████████████████████████████████████████████████████████                            | 2/3 [00:00<00:00,  6.14it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  7.20it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "source": [
    "def count_encode(df, cat_cols):\n",
    "    for col in cat_cols:\n",
    "        print(col)\n",
    "        vc = df[col].value_counts(dropna=True)\n",
    "        df[col + '_count'] = df[col].apply(lambda x: -999 if vc[x] < 10 else vc[x])\n",
    "    return df\n",
    "\n",
    "data = count_encode(data, ['original_language', 'release_year', 'release_date_weekday'])\n",
    "\n",
    "def cat_num_stats(df, cat_cols, num_cols):\n",
    "    for f1 in tqdm(cat_cols):\n",
    "        g = df.groupby(f1, as_index=False)\n",
    "        for f2 in tqdm(num_cols):\n",
    "            tmp = g[f2].agg({\n",
    "                '{}_{}_max'.format(f1, f2): 'max',\n",
    "                '{}_{}_min'.format(f1, f2): 'min',\n",
    "                '{}_{}_median'.format(f1, f2): 'median',\n",
    "                '{}_{}_mean'.format(f1, f2): 'mean',\n",
    "                '{}_{}_sum'.format(f1, f2): 'sum',\n",
    "                '{}_{}_skew'.format(f1, f2): 'skew',\n",
    "                '{}_{}_std'.format(f1, f2): 'std'\n",
    "            })\n",
    "            df = df.merge(tmp, on=f1, how='left')\n",
    "            del tmp\n",
    "            gc.collect()\n",
    "    return df\n",
    "\n",
    "data = cat_num_stats(data, ['original_language', 'release_year', 'release_date_weekday'], num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:19:12.842365Z",
     "start_time": "2020-12-06T15:19:12.654473Z"
    }
   },
   "outputs": [],
   "source": [
    "train = data[data['revenue'].notnull()]\n",
    "test = data[data['revenue'].isnull()]\n",
    "\n",
    "used_cols = [i for i in train.columns if i not in ['id', 'release_date', 'revenue']]\n",
    "y = train['revenue']\n",
    "train = train[used_cols]\n",
    "test = test[used_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:19:14.120626Z",
     "start_time": "2020-12-06T15:19:12.845362Z"
    }
   },
   "outputs": [],
   "source": [
    "class_list = ['release_year', 'release_month', 'release_date_weekday', 'original_language']\n",
    "\n",
    "ME = MeanEncoder(categorical_features=class_list, n_splits=5, target_type='regression', prior_weight_func=None)\n",
    "train = ME.fit_transform(train, y)\n",
    "test = ME.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:19:14.129622Z",
     "start_time": "2020-12-06T15:19:14.122626Z"
    }
   },
   "outputs": [],
   "source": [
    "train['revenue'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:19:14.145613Z",
     "start_time": "2020-12-06T15:19:14.131621Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 暂且选择这三种编码\n",
    "# enc_cols = []\n",
    "# stats_default_dict = {\n",
    "#     'max': train['revenue'].max(),\n",
    "#     'min': train['revenue'].min(),\n",
    "#     'median': train['revenue'].median(),\n",
    "#     'mean': train['revenue'].mean(),\n",
    "#     'sum': train['revenue'].sum(),\n",
    "#     'std': train['revenue'].std(),\n",
    "#     'skew': train['revenue'].skew(),\n",
    "#     'kurt': train['revenue'].kurt(),\n",
    "#     'mad': train['revenue'].mad()\n",
    "# }\n",
    "# enc_stats = ['max', 'min', 'mean']\n",
    "# skf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# for f in tqdm(['release_year', 'release_month', 'release_date_weekday', 'original_language']):\n",
    "#     enc_dict = {}\n",
    "#     for stat in enc_stats:\n",
    "#         enc_dict['{}_target_{}'.format(f, stat)] = stat\n",
    "#         train['{}_target_{}'.format(f, stat)] = 0\n",
    "#         test['{}_target_{}'.format(f, stat)] = 0\n",
    "#         enc_cols.append('{}_target_{}'.format(f, stat))\n",
    "#     for i, (trn_idx, val_idx) in enumerate(skf.split(train, y)):\n",
    "#         trn_x, val_x = train.iloc[trn_idx].reset_index(drop=True), train.iloc[val_idx].reset_index(drop=True)\n",
    "#         enc_df = trn_x.groupby(f, as_index=False)['revenue'].agg(enc_dict)\n",
    "#         val_x = val_x[[f]].merge(enc_df, on=f, how='left')\n",
    "#         test_x = test[[f]].merge(enc_df, on=f, how='left')\n",
    "#         for stat in enc_stats:\n",
    "#             val_x['{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].fillna(stats_default_dict[stat])\n",
    "#             test_x['{}_target_{}'.format(f, stat)] = test_x['{}_target_{}'.format(f, stat)].fillna(stats_default_dict[stat])\n",
    "#             train.loc[val_idx, '{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].values\n",
    "#             test['{}_target_{}'.format(f, stat)] += test_x['{}_target_{}'.format(f, stat)].values / skf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:19:43.512744Z",
     "start_time": "2020-12-06T15:19:14.150618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttraining's rmsle: -0.156474\tvalid_1's rmsle: -0.198884\n",
      "[100]\ttraining's rmsle: -0.113388\tvalid_1's rmsle: -0.199147\n",
      "[150]\ttraining's rmsle: -0.0859547\tvalid_1's rmsle: -0.19974\n",
      "Early stopping, best iteration is:\n",
      "[63]\ttraining's rmsle: -0.143184\tvalid_1's rmsle: -0.197783\n"
     ]
    }
   ],
   "source": [
    "train.drop('revenue', axis=1, inplace=True)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train, y, random_state=2020)\n",
    "\n",
    "dtrain = lgb.Dataset(X_train, y_train)\n",
    "dvalid = lgb.Dataset(X_valid, y_valid, reference=dtrain)\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'None',\n",
    "    'learning_rate': 0.05,\n",
    "    'seed': 2020\n",
    "}\n",
    "\n",
    "def rmsle(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.where(y_hat < 0, 1, y_hat)\n",
    "    y_true = np.where(y_true < 0, 1, y_true)\n",
    "    res = -np.sqrt(mean_squared_log_error(y_true, y_hat))\n",
    "    return 'rmsle', res, True\n",
    "\n",
    "valid_model = lgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    valid_sets=[dtrain, dvalid],\n",
    "    num_boost_round=1000000,\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval=50,\n",
    "    feval=rmsle\n",
    ")\n",
    "\n",
    "\n",
    "dall = lgb.Dataset(train, y)\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    dall,\n",
    "    num_boost_round=valid_model.best_iteration,\n",
    "    feval=rmsle\n",
    ")\n",
    "\n",
    "\n",
    "pred = model.predict(test)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = np.arange(0, 600)\n",
    "sub['revenue'] = pred\n",
    "sub['revenue'] = np.expm1(sub['revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:19:43.545724Z",
     "start_time": "2020-12-06T15:19:43.514742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>600.000000</td>\n",
       "      <td>6.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>299.500000</td>\n",
       "      <td>3.391260e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>173.349358</td>\n",
       "      <td>5.843153e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.578025e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>149.750000</td>\n",
       "      <td>2.384451e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>299.500000</td>\n",
       "      <td>1.014726e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>449.250000</td>\n",
       "      <td>3.993131e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>599.000000</td>\n",
       "      <td>4.166047e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID       revenue\n",
       "count  600.000000  6.000000e+02\n",
       "mean   299.500000  3.391260e+07\n",
       "std    173.349358  5.843153e+07\n",
       "min      0.000000  1.578025e+04\n",
       "25%    149.750000  2.384451e+06\n",
       "50%    299.500000  1.014726e+07\n",
       "75%    449.250000  3.993131e+07\n",
       "max    599.000000  4.166047e+08"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:19:43.564712Z",
     "start_time": "2020-12-06T15:19:43.548723Z"
    }
   },
   "outputs": [],
   "source": [
    "sub['revenue'] = np.where(sub['revenue'] <= 0, y_mean, sub['revenue'])\n",
    "sub.to_csv('../sub/sub_{}.csv'.format(time.strftime('%Y%m%d')), index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
