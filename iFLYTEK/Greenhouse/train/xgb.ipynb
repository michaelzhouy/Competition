{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T11:19:26.659210Z",
     "start_time": "2020-08-03T11:19:25.238044Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T11:19:26.671889Z",
     "start_time": "2020-08-03T11:19:26.660919Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_psi(train,test,f_cols):\n",
    "    psi_res = pd.DataFrame()\n",
    "    psi_dict={}\n",
    "    for c in tqdm(f_cols):\n",
    "        try:\n",
    "            t_train = train[c].fillna(-998)\n",
    "            t_test = test[c].fillna(-998)\n",
    "            #获取切分点\n",
    "            bins=[]\n",
    "            for i in np.arange(0,1.1,0.2):\n",
    "                bins.append(t_train.quantile(i))\n",
    "            bins=sorted(set(bins))\n",
    "            bins[0]=-np.inf\n",
    "            bins[-1]=np.inf\n",
    "            #计算psi\n",
    "            t_psi = pd.DataFrame()\n",
    "            t_psi['train'] = pd.cut(t_train,bins).value_counts().sort_index()\n",
    "            t_psi['test'] = pd.cut(t_test,bins).value_counts()\n",
    "            t_psi.index=[str(x) for x in t_psi.index]\n",
    "            t_psi.loc['总计',:] = t_psi.sum()\n",
    "            t_psi['train_rate'] = t_psi['train']/t_psi.loc['总计','train']\n",
    "            t_psi['test_rate'] = t_psi['test']/t_psi.loc['总计','test']\n",
    "            t_psi['psi'] = (t_psi['test_rate']-t_psi['train_rate'])*(np.log(t_psi['test_rate'])-np.log(t_psi['train_rate']))\n",
    "            t_psi.loc['总计','psi'] = t_psi['psi'].sum()\n",
    "            t_psi.index.name=c\n",
    "            #汇总\n",
    "            t_res = pd.DataFrame([[c,t_psi.loc['总计','psi']]],\n",
    "                                 columns=['变量名','PSI'])\n",
    "            psi_res = pd.concat([psi_res,t_res])\n",
    "            psi_dict[c]=t_psi\n",
    "            print(c,'done')\n",
    "        except:\n",
    "            print(c,'error')\n",
    "    return psi_res,psi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T11:19:26.679868Z",
     "start_time": "2020-08-03T11:19:26.673884Z"
    }
   },
   "outputs": [],
   "source": [
    "def correlation(df, threshold=0.98):\n",
    "    \"\"\"\n",
    "    特征相关性计算\n",
    "    @param df: \n",
    "    @param threshold: \n",
    "    @return: \n",
    "    \"\"\"\n",
    "    col_corr = set()\n",
    "    corr_matrix = df.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                colName = corr_matrix.columns[i]\n",
    "                col_corr.add(colName)\n",
    "    return col_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T11:19:26.715066Z",
     "start_time": "2020-08-03T11:19:26.680865Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/train.csv')\n",
    "test_df = pd.read_csv('../input/test.csv')\n",
    "sub = pd.DataFrame(test_df['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T11:19:26.727740Z",
     "start_time": "2020-08-03T11:19:26.716769Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['temperature'].notnull()]\n",
    "train_df = train_df.fillna(method='bfill')\n",
    "test_df = test_df.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T11:19:26.733723Z",
     "start_time": "2020-08-03T11:19:26.728737Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.columns = ['time', 'year', 'month', 'day', 'hour', 'min', 'sec', 'outdoorTemp', 'outdoorHum', 'outdoorAtmo',\n",
    "                    'indoorHum', 'indoorAtmo', 'temperature']\n",
    "test_df.columns = ['time', 'year', 'month', 'day', 'hour', 'min', 'sec', 'outdoorTemp', 'outdoorHum', 'outdoorAtmo',\n",
    "                   'indoorHum', 'indoorAtmo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T11:19:26.748692Z",
     "start_time": "2020-08-03T11:19:26.736717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape:  (24807, 13)\n",
      "处理后 train_df.shape:  (19338, 13)\n"
     ]
    }
   ],
   "source": [
    "print('train_df.shape: ', train_df.shape)\n",
    "train_df = train_df.loc[(train_df['outdoorTemp'] >= test_df['outdoorTemp'].min()) & (train_df['outdoorTemp'] <= test_df['outdoorTemp'].max())]\n",
    "print('处理后 train_df.shape: ', train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T11:19:26.757661Z",
     "start_time": "2020-08-03T11:19:26.750678Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T11:19:26.771622Z",
     "start_time": "2020-08-03T11:19:26.758658Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 1253.45it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(['outdoorTemp', 'outdoorHum', 'outdoorAtmo', 'indoorHum', 'indoorAtmo']):\n",
    "    data_df['{}_diff'.format(i)] = data_df[i].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T11:19:26.779601Z",
     "start_time": "2020-08-03T11:19:26.772620Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.fillna(method='bfill')\n",
    "test_df = test_df.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T11:19:30.205858Z",
     "start_time": "2020-08-03T11:19:26.781598Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# 基本聚合特征\n",
    "group_feats = []\n",
    "for f in tqdm(['outdoorTemp', 'outdoorHum', 'outdoorAtmo', 'indoorHum', 'indoorAtmo'] + ['{}_diff'.format(i) for i in ['outdoorTemp', 'outdoorHum', 'outdoorAtmo', 'indoorHum', 'indoorAtmo']]):\n",
    "    data_df['MDH_{}_medi'.format(f)] = data_df.groupby(['month', 'day', 'hour'])[f].transform('median')\n",
    "    data_df['MDH_{}_mean'.format(f)] = data_df.groupby(['month', 'day', 'hour'])[f].transform('mean')\n",
    "    data_df['MDH_{}_max'.format(f)] = data_df.groupby(['month', 'day', 'hour'])[f].transform('max')\n",
    "    data_df['MDH_{}_min'.format(f)] = data_df.groupby(['month', 'day', 'hour'])[f].transform('min')\n",
    "    data_df['MDH_{}_sum'.format(f)] = data_df.groupby(['month', 'day', 'hour'])[f].transform('sum')\n",
    "    data_df['MDH_{}_std'.format(f)] = data_df.groupby(['month', 'day', 'hour'])[f].transform('std')\n",
    "    data_df['MDH_{}_skew'.format(f)] = data_df.groupby(['month', 'day', 'hour'])[f].transform('skew')\n",
    "    \n",
    "    data_df['MD_{}_medi'.format(f)] = data_df.groupby(['month', 'day'])[f].transform('median')\n",
    "    data_df['MD_{}_mean'.format(f)] = data_df.groupby(['month', 'day'])[f].transform('mean')\n",
    "    data_df['MD_{}_max'.format(f)] = data_df.groupby(['month', 'day'])[f].transform('max')\n",
    "    data_df['MD_{}_min'.format(f)] = data_df.groupby(['month', 'day'])[f].transform('min')\n",
    "    data_df['MD_{}_sum'.format(f)] = data_df.groupby(['month', 'day'])[f].transform('sum')\n",
    "    data_df['MD_{}_std'.format(f)] = data_df.groupby(['month', 'day'])[f].transform('std')\n",
    "    data_df['MD_{}_skew'.format(f)] = data_df.groupby(['month', 'day'])[f].transform('skew')\n",
    "\n",
    "    group_feats.append('MDH_{}_medi'.format(f))\n",
    "    group_feats.append('MDH_{}_mean'.format(f))\n",
    "    group_feats.append('MDH_{}_max'.format(f))\n",
    "    group_feats.append('MDH_{}_min'.format(f))\n",
    "    group_feats.append('MDH_{}_sum'.format(f))\n",
    "    group_feats.append('MDH_{}_std'.format(f))\n",
    "    group_feats.append('MDH_{}_skew'.format(f))\n",
    "    \n",
    "    group_feats.append('MD_{}_medi'.format(f))\n",
    "    group_feats.append('MD_{}_mean'.format(f))\n",
    "    group_feats.append('MD_{}_max'.format(f))\n",
    "    group_feats.append('MD_{}_min'.format(f))\n",
    "    group_feats.append('MD_{}_sum'.format(f))\n",
    "    group_feats.append('MD_{}_std'.format(f))\n",
    "    group_feats.append('MD_{}_skew'.format(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.243Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|████████████████████████████████████████▉                                     | 76/145 [48:31<2:29:28, 129.98s/it]"
     ]
    }
   ],
   "source": [
    "# 基本交叉特征\n",
    "for f1 in tqdm(['outdoorTemp', 'outdoorHum', 'outdoorAtmo', 'indoorHum', 'indoorAtmo'] + group_feats):\n",
    "    for f2 in ['outdoorTemp', 'outdoorHum', 'outdoorAtmo', 'indoorHum', 'indoorAtmo'] + group_feats:\n",
    "        if f1 != f2:\n",
    "            colname_substract = '{}_{}_subtract'.format(f1, f2)\n",
    "            colname_add = '{}_{}_add'.format(f1, f2)            \n",
    "            colname_multiply = '{}_{}_multyply'.format(f1, f2)\n",
    "            colname_ratio = '{}_{}_ratio'.format(f1, f2)\n",
    "            \n",
    "            data_df[colname_substract] = data_df[f1].values - data_df[f2].values\n",
    "            data_df[colname_add] = data_df[f1].values + data_df[f2].values\n",
    "            data_df[colname_multiply] = data_df[f1].values / (data_df[f2].values + 0.001)\n",
    "            data_df[colname_ratio] = data_df[f1].values * data_df[f2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.245Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df = data_df.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.246Z"
    }
   },
   "outputs": [],
   "source": [
    "# 历史信息提取\n",
    "data_df['dt'] = data_df['day'].values + (data_df['month'].values - 3) * 31\n",
    "\n",
    "for f in ['outdoorTemp', 'outdoorHum', 'outdoorAtmo', 'indoorHum', 'indoorAtmo', 'temperature']:\n",
    "    tmp_df = pd.DataFrame()\n",
    "    for t in tqdm(range(15, 45)):\n",
    "        tmp = data_df.loc[data_df['dt'] < t, :].groupby(['hour'], as_index=False)[f].agg({\n",
    "            'hit_{}_mean'.format(f): 'mean',\n",
    "            'hit_{}_median'.format(f): 'median',\n",
    "            'hit_{}_max'.format(f): 'max',\n",
    "            'hit_{}_min'.format(f): 'min',\n",
    "            'hit_{}_sum'.format(f): 'sum',\n",
    "            'hit_{}_std'.format(f): 'std',\n",
    "            'hit_{}_skew'.format(f): 'skew'\n",
    "        })\n",
    "        tmp['dt'] = t\n",
    "        tmp_df = tmp_df.append(tmp)\n",
    "        del tmp\n",
    "        gc.collect()\n",
    "\n",
    "    data_df = data_df.merge(tmp_df, on=['dt', 'hour'], how='left')\n",
    "    del tmp_df\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "for f in ['outdoorTemp', 'outdoorHum', 'outdoorAtmo', 'indoorHum', 'indoorAtmo', 'temperature']:\n",
    "    tmp_df = pd.DataFrame()\n",
    "    for t in tqdm(range(15, 45)):\n",
    "        tmp = data_df.loc[(data_df['dt'] < t) & (data_df['dt'] > t - 1), :].groupby(['hour'], as_index=False)[f].agg({\n",
    "            'hit_t_1_{}_mean'.format(f): 'mean',\n",
    "            'hit_t_1_{}_median'.format(f): 'median',\n",
    "            'hit_t_1_{}_max'.format(f): 'max',\n",
    "            'hit_t_1_{}_min'.format(f): 'min',\n",
    "            'hit_t_1_{}_sum'.format(f): 'sum',\n",
    "            'hit_t_1_{}_std'.format(f): 'std',\n",
    "            'hit_t_1_{}_skew'.format(f): 'skew'\n",
    "        })\n",
    "        tmp['dt'] = t\n",
    "        tmp_df = tmp_df.append(tmp)\n",
    "        del tmp\n",
    "        gc.collect()\n",
    "\n",
    "    data_df = data_df.merge(tmp_df, on=['dt', 'hour'], how='left')\n",
    "    del tmp_df\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "for f in ['outdoorTemp', 'outdoorHum', 'outdoorAtmo', 'indoorHum', 'indoorAtmo', 'temperature']:\n",
    "    tmp_df = pd.DataFrame()\n",
    "    for t in tqdm(range(15, 45)):\n",
    "        tmp = data_df.loc[(data_df['dt'] < t) & (data_df['dt'] > t - 2), :].groupby(['hour'], as_index=False)[f].agg({\n",
    "            'hit_t_2_{}_mean'.format(f): 'mean',\n",
    "            'hit_t_2_{}_median'.format(f): 'median',\n",
    "            'hit_t_2_{}_max'.format(f): 'max',\n",
    "            'hit_t_2_{}_min'.format(f): 'min',\n",
    "            'hit_t_2_{}_sum'.format(f): 'sum',\n",
    "            'hit_t_2_{}_std'.format(f): 'std',\n",
    "            'hit_t_2_{}_skew'.format(f): 'skew'\n",
    "        })\n",
    "        tmp['dt'] = t\n",
    "        tmp_df = tmp_df.append(tmp)\n",
    "        del tmp\n",
    "        gc.collect()\n",
    "\n",
    "    data_df = data_df.merge(tmp_df, on=['dt', 'hour'], how='left')\n",
    "    del tmp_df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.248Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df = data_df.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.249Z"
    }
   },
   "outputs": [],
   "source": [
    "# 离散化\n",
    "for f in ['outdoorTemp', 'outdoorHum', 'outdoorAtmo', 'indoorHum', 'indoorAtmo']:\n",
    "    data_df[f + '_20_bin'] = pd.cut(data_df[f], 20, duplicates='drop').apply(lambda x: x.left).astype(int)\n",
    "    data_df[f + '_50_bin'] = pd.cut(data_df[f], 50, duplicates='drop').apply(lambda x: x.left).astype(int)\n",
    "    data_df[f + '_100_bin'] = pd.cut(data_df[f], 100, duplicates='drop').apply(lambda x: x.left).astype(int)\n",
    "    data_df[f + '_200_bin'] = pd.cut(data_df[f], 200, duplicates='drop').apply(lambda x: x.left).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.251Z"
    }
   },
   "outputs": [],
   "source": [
    "for f1 in tqdm(['outdoorTemp_20_bin', 'outdoorHum_20_bin', 'outdoorAtmo_20_bin', 'indoorHum_20_bin', 'indoorAtmo_20_bin']):\n",
    "    for f2 in ['outdoorTemp', 'outdoorHum', 'outdoorAtmo', 'indoorHum', 'indoorAtmo']:\n",
    "        data_df['{}_{}_medi'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('median')\n",
    "        data_df['{}_{}_mean'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('mean')\n",
    "        data_df['{}_{}_max'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('max')\n",
    "        data_df['{}_{}_min'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('min')\n",
    "        data_df['{}_{}_sum'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('sum')\n",
    "        data_df['{}_{}_std'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('std')\n",
    "        data_df['{}_{}_skew'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('skew')\n",
    "\n",
    "for f1 in tqdm(['outdoorTemp_50_bin', 'outdoorHum_50_bin', 'outdoorAtmo_50_bin', 'indoorHum_50_bin', 'indoorAtmo_50_bin']):\n",
    "    for f2 in ['outdoorTemp', 'outdoorHum', 'outdoorAtmo', 'indoorHum', 'indoorAtmo']:\n",
    "        data_df['{}_{}_medi'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('median')\n",
    "        data_df['{}_{}_mean'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('mean')\n",
    "        data_df['{}_{}_max'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('max')\n",
    "        data_df['{}_{}_min'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('min')\n",
    "        data_df['{}_{}_sum'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('sum')\n",
    "        data_df['{}_{}_std'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('std')\n",
    "        data_df['{}_{}_skew'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('skew')\n",
    "\n",
    "for f1 in tqdm(['outdoorTemp_100_bin', 'outdoorHum_100_bin', 'outdoorAtmo_100_bin', 'indoorHum_100_bin',\n",
    "                'indoorAtmo_100_bin']):\n",
    "    for f2 in ['outdoorTemp', 'outdoorHum', 'outdoorAtmo', 'indoorHum', 'indoorAtmo']:\n",
    "        data_df['{}_{}_medi'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('median')\n",
    "        data_df['{}_{}_mean'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('mean')\n",
    "        data_df['{}_{}_max'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('max')\n",
    "        data_df['{}_{}_min'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('min')\n",
    "        data_df['{}_{}_sum'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('sum')\n",
    "        data_df['{}_{}_std'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('std')\n",
    "        data_df['{}_{}_skew'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('skew')\n",
    "\n",
    "for f1 in tqdm(['outdoorTemp_200_bin', 'outdoorHum_200_bin', 'outdoorAtmo_200_bin', 'indoorHum_200_bin',\n",
    "                'indoorAtmo_200_bin']):\n",
    "    for f2 in ['outdoorTemp', 'outdoorHum', 'outdoorAtmo', 'indoorHum', 'indoorAtmo']:\n",
    "        data_df['{}_{}_medi'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('median')\n",
    "        data_df['{}_{}_mean'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('mean')\n",
    "        data_df['{}_{}_max'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('max')\n",
    "        data_df['{}_{}_min'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('min')\n",
    "        data_df['{}_{}_sum'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('sum')\n",
    "        data_df['{}_{}_std'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('std')\n",
    "        data_df['{}_{}_skew'.format(f1, f2)] = data_df.groupby([f1])[f2].transform('skew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.253Z"
    }
   },
   "outputs": [],
   "source": [
    "train_count = train_df.shape[0]\n",
    "train_df = data_df[:train_count].copy().reset_index(drop=True)\n",
    "test_df = data_df[train_count:].copy().reset_index(drop=True)\n",
    "del data_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.255Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_columns = [\"time\", \"year\", \"sec\", \"temperature\"]\n",
    "\n",
    "features = train_df[:1].drop(drop_columns, axis=1).columns\n",
    "x_train = train_df[features]\n",
    "x_test = test_df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.257Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = train_df['temperature'].values - train_df['outdoorTemp'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.258Z"
    }
   },
   "outputs": [],
   "source": [
    "psi_res, psi_dict = get_psi(x_train, x_test, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.259Z"
    }
   },
   "outputs": [],
   "source": [
    "features = list(psi_res[psi_res['PSI'] <= 0.2]['变量名'].values) + ['outdoorTemp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.261Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train[features]\n",
    "x_test = x_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.262Z"
    }
   },
   "outputs": [],
   "source": [
    "col_corr = correlation(x_train, 0.98)\n",
    "print(col_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.264Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train.drop(list(col_corr), axis=1, inplace=True)\n",
    "x_test.drop(list(col_corr), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.265Z"
    }
   },
   "outputs": [],
   "source": [
    "nums = int(x_train.shape[0] * 0.8)\n",
    "\n",
    "trn_x, trn_y, val_x, val_y = x_train[:nums], y_train[:nums], x_train[nums:], y_train[nums:]\n",
    "\n",
    "train_matrix = xgb.DMatrix(trn_x, label=trn_y, missing=np.nan)\n",
    "valid_matrix = xgb.DMatrix(val_x, label=val_y, missing=np.nan)\n",
    "train_all_matrix = xgb.DMatrix(x_train, y_train, missing=np.nan)\n",
    "test_matrix = xgb.DMatrix(x_test, label=val_y, missing=np.nan)\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'eval_metric': 'mae',\n",
    "    'min_child_weight': 5,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.5,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'eta': 0.01,\n",
    "    'seed': 2020,\n",
    "    'nthread': 36,\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "watchlist = [(train_matrix, 'train'), (valid_matrix, 'eval')]\n",
    "\n",
    "model_eval = xgb.train(params,\n",
    "                       train_matrix,\n",
    "                       num_boost_round=50000,\n",
    "                       evals=watchlist,\n",
    "                       verbose_eval=500,\n",
    "                       early_stopping_rounds=1000)\n",
    "val_pred = model_eval.predict(valid_matrix, ntree_limit=model_eval.best_ntree_limit).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.266Z"
    }
   },
   "outputs": [],
   "source": [
    "mse = mean_squared_error(val_y, val_pred)\n",
    "print(\"mse_score:\", mse)\n",
    "print(\"mse_score:\", str(mse)[2:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.268Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_imp_dict = model_eval.get_score(importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.269Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_imp = pd.Series(feat_imp_dict).sort_values(ascending=False).reset_index().rename(columns={'index': 'feature',\n",
    "                                                                                               0: 'importance'})\n",
    "feat_imp['normalized_importance'] = feat_imp['importance'] / feat_imp['importance'].sum()\n",
    "feat_imp['cumulative_importance'] = np.cumsum(feat_imp['normalized_importance'])\n",
    "record_low_importance = feat_imp[feat_imp['cumulative_importance'] > 0.95]\n",
    "\n",
    "to_drop = list(record_low_importance['feature'])\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.271Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train.drop(to_drop, axis=1, inplace=True)\n",
    "x_test.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.272Z"
    }
   },
   "outputs": [],
   "source": [
    "train_all_matrix = xgb.DMatrix(x_train, y_train, missing=np.nan)\n",
    "test_matrix = xgb.DMatrix(x_test, label=val_y, missing=np.nan)\n",
    "\n",
    "model = xgb.train(params,\n",
    "                  train_all_matrix,\n",
    "                  num_boost_round=model_eval.best_ntree_limit + 20)\n",
    "\n",
    "test_pred = model.predict(test_matrix, ntree_limit=model.best_ntree_limit).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-03T11:19:25.273Z"
    }
   },
   "outputs": [],
   "source": [
    "sub['temperature'] = test_pred[:, 0] + test_df['outdoorTemp'].values\n",
    "sub.to_csv('../sub/sub_{}_{}.csv'.format(time.strftime('%Y%m%d'), str(mse)[2:7]), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
