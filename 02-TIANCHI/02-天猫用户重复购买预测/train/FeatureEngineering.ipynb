{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:37:35.902946Z",
     "start_time": "2020-09-17T06:37:34.909571Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import gc\n",
    "from collections import Counter\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:37:35.914914Z",
     "start_time": "2020-09-17T06:37:35.903948Z"
    }
   },
   "outputs": [],
   "source": [
    "# 节省内存读文件\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\"\n",
    "    iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\n",
    "    @param df:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "            df[col] = df[col].astype('str')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:38:00.215867Z",
     "start_time": "2020-09-17T06:37:35.916450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 5.97 MB\n",
      "Memory usage after optimization is: 1.74 MB\n",
      "Decreased by 70.8%\n",
      "Memory usage of dataframe is 5.98 MB\n",
      "Memory usage after optimization is: 3.49 MB\n",
      "Decreased by 41.7%\n",
      "Memory usage of dataframe is 9.71 MB\n",
      "Memory usage after optimization is: 3.24 MB\n",
      "Decreased by 66.7%\n",
      "Memory usage of dataframe is 2933.33 MB\n",
      "Memory usage after optimization is: 890.48 MB\n",
      "Decreased by 69.6%\n"
     ]
    }
   ],
   "source": [
    "train_file = '../input/data_format1/train_format1.csv'\n",
    "test_file = '../input/data_format1/test_format1.csv'\n",
    "user_info_file = '../input/data_format1/user_info_format1.csv'\n",
    "user_log_file = '../input/data_format1/user_log_format1.csv'\n",
    "\n",
    "train_data = reduce_mem_usage(pd.read_csv(train_file))\n",
    "test_data = reduce_mem_usage(pd.read_csv(test_file))\n",
    "user_info = reduce_mem_usage(pd.read_csv(user_info_file))\n",
    "user_log = reduce_mem_usage(pd.read_csv(user_log_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:38:00.225425Z",
     "start_time": "2020-09-17T06:38:00.217837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260864 entries, 0 to 260863\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype\n",
      "---  ------       --------------   -----\n",
      " 0   user_id      260864 non-null  int32\n",
      " 1   merchant_id  260864 non-null  int16\n",
      " 2   label        260864 non-null  int8 \n",
      "dtypes: int16(1), int32(1), int8(1)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:38:00.248338Z",
     "start_time": "2020-09-17T06:38:00.226397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 261477 entries, 0 to 261476\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   user_id      261477 non-null  int32  \n",
      " 1   merchant_id  261477 non-null  int16  \n",
      " 2   prob         0 non-null       float64\n",
      "dtypes: float64(1), int16(1), int32(1)\n",
      "memory usage: 3.5 MB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:38:00.266316Z",
     "start_time": "2020-09-17T06:38:00.249336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 424170 entries, 0 to 424169\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   user_id    424170 non-null  int32  \n",
      " 1   age_range  421953 non-null  float16\n",
      " 2   gender     417734 non-null  float16\n",
      "dtypes: float16(2), int32(1)\n",
      "memory usage: 3.2 MB\n"
     ]
    }
   ],
   "source": [
    "user_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:38:00.986526Z",
     "start_time": "2020-09-17T06:38:00.267288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54925330 entries, 0 to 54925329\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   user_id      int32  \n",
      " 1   item_id      int32  \n",
      " 2   cat_id       int16  \n",
      " 3   seller_id    int16  \n",
      " 4   brand_id     float16\n",
      " 5   time_stamp   int16  \n",
      " 6   action_type  int8   \n",
      "dtypes: float16(1), int16(3), int32(2), int8(1)\n",
      "memory usage: 890.5 MB\n"
     ]
    }
   ],
   "source": [
    "user_log.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:38:01.204733Z",
     "start_time": "2020-09-17T06:38:00.989556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = train_data.append(test_data)\n",
    "all_data = all_data.merge(user_info, on=['user_id'], how='left')\n",
    "\n",
    "del train_data, test_data, user_info\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:38:10.213949Z",
     "start_time": "2020-09-17T06:38:01.206689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按时间排序\n",
    "user_log = user_log.sort_values(['user_id', 'time_stamp'])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:01.805270Z",
     "start_time": "2020-09-17T06:38:10.214945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对每个用户逐个合并所有字段\n",
    "list_join_func = lambda x: ' '.join([str(i) for i in x])\n",
    "\n",
    "agg_dict = {\n",
    "    'item_id': list_join_func,\n",
    "    'cat_id': list_join_func,\n",
    "    'seller_id': list_join_func,\n",
    "    'brand_id': list_join_func,\n",
    "    'time_stamp': list_join_func,\n",
    "    'action_type': list_join_func\n",
    "}\n",
    "\n",
    "rename_dict = {\n",
    "    'item_id': 'item_path',\n",
    "    'cat_id': 'cat_path',\n",
    "    'seller_id': 'seller_path',\n",
    "    'brand_id': 'brand_path',\n",
    "    'time_stamp': 'time_stamp_path',\n",
    "    'action_type': 'action_type_path'\n",
    "}\n",
    "\n",
    "\n",
    "def merge_list(df_ID, join_columns, df_data, agg_dict, rename_dict):\n",
    "    df_data = df_data.groupby(join_columns).agg(agg_dict).reset_index().rename(columns=rename_dict)\n",
    "    \n",
    "    df_ID = df_ID.merge(df_data, on=join_columns, how='left')\n",
    "    return df_ID\n",
    "\n",
    "all_data = merge_list(all_data, 'user_id', user_log, agg_dict, rename_dict)\n",
    "\n",
    "del user_log\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义特征统计函数\n",
    "## 定义统计函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 定义统计数据总数的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:01.810230Z",
     "start_time": "2020-09-17T06:40:01.806241Z"
    }
   },
   "outputs": [],
   "source": [
    "def cnt_(x):\n",
    "    try:\n",
    "        return len(x.split(' '))\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 定义统计数据唯一值总数的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:01.826204Z",
     "start_time": "2020-09-17T06:40:01.811228Z"
    }
   },
   "outputs": [],
   "source": [
    "def nunique_(x):\n",
    "    try:\n",
    "        return len(set(x.split(' ')))\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 定义统计数据最大值的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:01.837174Z",
     "start_time": "2020-09-17T06:40:01.827201Z"
    }
   },
   "outputs": [],
   "source": [
    "def max_(x):\n",
    "    try:\n",
    "        return np.max([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 定义统计数据最小值的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:01.849142Z",
     "start_time": "2020-09-17T06:40:01.840168Z"
    }
   },
   "outputs": [],
   "source": [
    "def min_(x):\n",
    "    try:\n",
    "        return np.min([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 定义统计数据标准差的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:01.862110Z",
     "start_time": "2020-09-17T06:40:01.852134Z"
    }
   },
   "outputs": [],
   "source": [
    "def std_(x):\n",
    "    try:\n",
    "        return np.std([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 定义统计数据中频次为$topN$数据的元素的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:01.871083Z",
     "start_time": "2020-09-17T06:40:01.863106Z"
    }
   },
   "outputs": [],
   "source": [
    "def most_n(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][0]\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 定义统计数据中频次为$topN$数据的元素的频次的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:01.880059Z",
     "start_time": "2020-09-17T06:40:01.873082Z"
    }
   },
   "outputs": [],
   "source": [
    "def most_n_cnt(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][1]\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用定义的统计函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:01.891030Z",
     "start_time": "2020-09-17T06:40:01.881057Z"
    }
   },
   "outputs": [],
   "source": [
    "def user_cnt(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(cnt_)\n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_nunique(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(nunique_)\n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_max(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(max_)\n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_min(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(min_)\n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_std(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(std_)\n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_most_n(df_data, single_col, name, n=1):\n",
    "    df_data[name] = df_data[single_col].apply(lambda x: most_n(x, n))\n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_most_n_cnt(df_data, single_col, name, n=1):\n",
    "    df_data[name] = df_data[single_col].apply(lambda x: most_n_cnt(x, n))\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取统计特征\n",
    "## 特征统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 店铺特征统计：统计与店铺特点相关的特征，如店铺、商品、品牌等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:02.727264Z",
     "start_time": "2020-09-17T06:40:01.892028Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "基本统计特征\n",
    "\"\"\"\n",
    "all_data_test = all_data.head(2000)\n",
    "\n",
    "# 统计用户点击、浏览、加购、购买行为\n",
    "# 总次数\n",
    "all_data_test = user_cnt(all_data_test, 'seller_path', 'user_cnt')\n",
    "\n",
    "# 不同店铺个数\n",
    "all_data_test = user_nunique(all_data_test, 'seller_path', 'seller_nunique')\n",
    "\n",
    "# 不同品类个数\n",
    "all_data_test = user_nunique(all_data_test, 'cat_path', 'cat_nunique')\n",
    "\n",
    "# 不同品牌个数\n",
    "all_data_test = user_nunique(all_data_test, 'brand_path', 'brand_nunique')\n",
    "\n",
    "# 不同商品个数\n",
    "all_data_test = user_nunique(all_data_test, 'item_path', 'item_nunique')\n",
    "\n",
    "# 活跃天数\n",
    "all_data_test = user_nunique(all_data_test, 'time_stamp_path', 'time_stamp_nunique')\n",
    "\n",
    "# 不同用户行为种数\n",
    "all_data_test = user_nunique(all_data_test, 'action_type_path', 'action_type_nunique')\n",
    "\n",
    "# 最晚时间\n",
    "all_data_test = user_max(all_data_test, 'time_stamp_path', 'time_stamp_max')\n",
    "\n",
    "# 最早时间\n",
    "all_data_test = user_min(all_data_test, 'time_stamp_path', 'time_stamp_min')\n",
    "\n",
    "# 活跃天数方差\n",
    "all_data_test = user_std(all_data_test, 'time_stamp_path', 'time_stamp_std')\n",
    "\n",
    "# 最早与最晚相差天数\n",
    "all_data_test['time_stamp_range'] = all_data_test['time_stamp_max'] - all_data_test['time_stamp_min']\n",
    "\n",
    "# 用户最喜欢的店铺\n",
    "all_data_test = user_most_n(all_data_test, 'seller_path', 'seller_most_1', n=1)\n",
    "\n",
    "# 最喜欢的品类\n",
    "all_data_test = user_most_n(all_data_test, 'cat_path', 'cat_most_1', n=1)\n",
    "\n",
    "# 最喜欢的品牌\n",
    "all_data_test = user_most_n(all_data_test, 'brand_path', 'brand_most_1', n=1)\n",
    "\n",
    "# 最喜欢的商品\n",
    "all_data_test = user_most_n(all_data_test, 'item_path', 'item_most_1', n=1)\n",
    "\n",
    "# 最常见的行为动作\n",
    "all_data_test = user_most_n(all_data_test, 'action_type_path', 'action_type_most_1', n=1)\n",
    "\n",
    "# 用户最喜欢的店铺 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'seller_path', 'seller_most_1_cnt', n=1)\n",
    "\n",
    "# 最喜欢的品类 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'cat_path', 'cat_most_1_cnt', n=1)\n",
    "\n",
    "# 最喜欢的品牌 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'brand_path', 'brand_most_1_cnt', n=1)\n",
    "\n",
    "# 最喜欢的商品 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'item_path', 'item_most_1_cnt', n=1)\n",
    "\n",
    "# 最常见的行为动作 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'action_type_path', 'action_type_most_1_cnt', n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 用户特征统计：对用户的点击、加购、购买、收藏等特征进行统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:02.740796Z",
     "start_time": "2020-09-17T06:40:02.728828Z"
    }
   },
   "outputs": [],
   "source": [
    "# 对点击、加购、购买、收藏分开统计\n",
    "def col_cnt_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "        \n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "        \n",
    "        path_len = len(data_dict[col])  # 总行数\n",
    "        \n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "        return len(data_out)\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "    \n",
    "def col_nunique_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "        \n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "        \n",
    "        path_len = len(data_dict[col])  # 总行数\n",
    "        \n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "        return len(set(data_out))\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "\n",
    "def user_col_cnt(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_cnt_(x, columns_list, action_type), axis=1)\n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_col_nunique(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_nunique_(x, columns_list, action_type), axis=1)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 统计用户和店铺的关系：对店铺的用户点击次数、加购次数、购买次数、收藏次数等进行统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:03.472275Z",
     "start_time": "2020-09-17T06:40:02.741946Z"
    }
   },
   "outputs": [],
   "source": [
    "# 点击次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path'], '0', 'user_cnt_0')\n",
    "\n",
    "# 加购次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path'], '1', 'user_cnt_1')\n",
    "\n",
    "# 购买次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path'], '2', 'user_cnt_2')\n",
    "\n",
    "# 收藏次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path'], '3', 'user_cnt_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征组合\n",
    "1. 特征组合进行业务特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:03.932095Z",
     "start_time": "2020-09-17T06:40:03.473163Z"
    }
   },
   "outputs": [],
   "source": [
    "# 对店铺+商品的点击次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path', 'item_path'], '0', 'user_item_cnt_0')\n",
    "\n",
    "# 用户对店铺中多少种不同的商品做了点击\n",
    "all_data_test = user_col_nunique(all_data_test, ['seller_path', 'item_path'], '0', 'seller_nunique_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:03.942220Z",
     "start_time": "2020-09-17T06:40:03.936789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'merchant_id', 'label', 'prob', 'age_range', 'gender',\n",
       "       'item_path', 'cat_path', 'seller_path', 'brand_path', 'time_stamp_path',\n",
       "       'action_type_path', 'user_cnt', 'seller_nunique', 'cat_nunique',\n",
       "       'brand_nunique', 'item_nunique', 'time_stamp_nunique',\n",
       "       'action_type_nunique', 'time_stamp_max', 'time_stamp_min',\n",
       "       'time_stamp_std', 'time_stamp_range', 'seller_most_1', 'cat_most_1',\n",
       "       'brand_most_1', 'item_most_1', 'action_type_most_1',\n",
       "       'seller_most_1_cnt', 'cat_most_1_cnt', 'brand_most_1_cnt',\n",
       "       'item_most_1_cnt', 'action_type_most_1_cnt', 'user_cnt_0', 'user_cnt_1',\n",
       "       'user_cnt_2', 'user_cnt_3', 'user_item_cnt_0', 'seller_nunique_0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用Countvector和TF-IDF提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:04.317576Z",
     "start_time": "2020-09-17T06:40:03.944200Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from scipy import sparse\n",
    "\n",
    "tfidfVec = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS,\n",
    "                           ngram_range=(1, 1),\n",
    "                           max_features=100)\n",
    "columns_list = ['seller_path']\n",
    "for i, col in enumerate(columns_list):\n",
    "    tfidfVec.fit(all_data_test[col])\n",
    "    data_ = tfidfVec.transform(all_data_test[col])\n",
    "    if i == 0:\n",
    "        data_cat = data_\n",
    "    else:\n",
    "        data_cat = sparse.hstack((data_cat, data_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:04.340435Z",
     "start_time": "2020-09-17T06:40:04.318540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_0</th>\n",
       "      <th>tfidf_1</th>\n",
       "      <th>tfidf_2</th>\n",
       "      <th>tfidf_3</th>\n",
       "      <th>tfidf_4</th>\n",
       "      <th>tfidf_5</th>\n",
       "      <th>tfidf_6</th>\n",
       "      <th>tfidf_7</th>\n",
       "      <th>tfidf_8</th>\n",
       "      <th>tfidf_9</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidf_90</th>\n",
       "      <th>tfidf_91</th>\n",
       "      <th>tfidf_92</th>\n",
       "      <th>tfidf_93</th>\n",
       "      <th>tfidf_94</th>\n",
       "      <th>tfidf_95</th>\n",
       "      <th>tfidf_96</th>\n",
       "      <th>tfidf_97</th>\n",
       "      <th>tfidf_98</th>\n",
       "      <th>tfidf_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009531</td>\n",
       "      <td>0.186124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173224</td>\n",
       "      <td>0.036434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.011178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009531</td>\n",
       "      <td>0.186124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173224</td>\n",
       "      <td>0.036434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.011178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009531</td>\n",
       "      <td>0.186124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173224</td>\n",
       "      <td>0.036434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.011178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009531</td>\n",
       "      <td>0.186124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173224</td>\n",
       "      <td>0.036434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.011178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tfidf_0  tfidf_1   tfidf_2   tfidf_3  tfidf_4   tfidf_5   tfidf_6  tfidf_7  \\\n",
       "0      0.0      0.0  0.009531  0.186124      0.0  0.173224  0.036434      0.0   \n",
       "1      0.0      0.0  0.009531  0.186124      0.0  0.173224  0.036434      0.0   \n",
       "2      0.0      0.0  0.009531  0.186124      0.0  0.173224  0.036434      0.0   \n",
       "3      0.0      0.0  0.009531  0.186124      0.0  0.173224  0.036434      0.0   \n",
       "4      0.0      0.0  0.000000  0.000000      0.0  0.000000  0.000000      0.0   \n",
       "\n",
       "   tfidf_8   tfidf_9  ...  tfidf_90  tfidf_91  tfidf_92  tfidf_93  tfidf_94  \\\n",
       "0      0.0  0.012205  ...       0.0       0.0  0.012666  0.011178       0.0   \n",
       "1      0.0  0.012205  ...       0.0       0.0  0.012666  0.011178       0.0   \n",
       "2      0.0  0.012205  ...       0.0       0.0  0.012666  0.011178       0.0   \n",
       "3      0.0  0.012205  ...       0.0       0.0  0.012666  0.011178       0.0   \n",
       "4      0.0  0.000000  ...       0.0       0.0  0.000000  0.000000       0.0   \n",
       "\n",
       "   tfidf_95  tfidf_96  tfidf_97  tfidf_98  tfidf_99  \n",
       "0       0.0       0.0       0.0  0.012925       0.0  \n",
       "1       0.0       0.0       0.0  0.012925       0.0  \n",
       "2       0.0       0.0       0.0  0.012925       0.0  \n",
       "3       0.0       0.0       0.0  0.012925       0.0  \n",
       "4       0.0       0.0       0.0  0.000000       0.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(data_cat.toarray())\n",
    "df_tfidf.columns = ['tfidf_' + str(i) for i in df_tfidf.columns]\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:04.350408Z",
     "start_time": "2020-09-17T06:40:04.341433Z"
    }
   },
   "outputs": [],
   "source": [
    "all_data_test = pd.concat([all_data_test, df_tfidf], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 嵌入特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:06.378404Z",
     "start_time": "2020-09-17T06:40:04.351405Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "def split_(x):\n",
    "    try:\n",
    "        return x.split(' ')\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "model = gensim.models.Word2Vec(all_data_test['seller_path'].apply(lambda x: split_(x)),\n",
    "                               size=100,\n",
    "                               window=5,\n",
    "                               min_count=5,\n",
    "                               workers=4)\n",
    "\n",
    "def mean_w2v_(x, model, size=100):\n",
    "    try:\n",
    "        i = 0\n",
    "        for word in x.split(' '):\n",
    "            if word in model.wv.vocab:\n",
    "                i += 1\n",
    "                if i == 0:\n",
    "                    vec = np.zeros(size)\n",
    "                vec += model.wv[word]\n",
    "        return vec / i\n",
    "    except:\n",
    "        return np.zeros(size)\n",
    "    \n",
    "\n",
    "def get_mean_w2v(df_data, columns, model, size):\n",
    "    data_array = []\n",
    "    for index, row in df_data.iterrows():\n",
    "        w2v = mean_w2v_(row[columns], model, size)\n",
    "        data_array.append(w2v)\n",
    "    return pd.DataFrame(data_array)\n",
    "\n",
    "df_embedding = get_mean_w2v(all_data_test, 'seller_path', model, 100)\n",
    "df_embedding.columns = ['embedding_' + str(i) for i in df_embedding.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:06.390418Z",
     "start_time": "2020-09-17T06:40:06.380333Z"
    }
   },
   "outputs": [],
   "source": [
    "all_data_test = pd.concat([all_data_test, df_embedding], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking分类工具\n",
    "## Stacking特征工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:06.942301Z",
     "start_time": "2020-09-17T06:40:06.391348Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss, mean_absolute_error, mean_squared_error\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义Stacking分类特征相关函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:06.974213Z",
     "start_time": "2020-09-17T06:40:06.943228Z"
    }
   },
   "outputs": [],
   "source": [
    "def stacking_clf(clf, train_x, train_y, test_x, clf_name, kf, label_split=None):\n",
    "    train = np.zeros((train_x.shape[0], 1))\n",
    "    test = np.zeros((test_x.shape[0], 1))\n",
    "    test_pre = np.empty((folds, test_x.shape[0], 1))\n",
    "    cv_scores = []\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(train_x, label_split)):\n",
    "        tr_x = train_x[train_index]\n",
    "        tr_y = train_y[train_index]\n",
    "        te_x = train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "        \n",
    "        if clf_name in ['rf', 'ada', 'gb', 'et', 'lr', 'knn',' gnb']:\n",
    "            clf.fit(tr_x, tr_y)\n",
    "            pre = clf.predict_proba(te_x)\n",
    "            train[test_index] = pre[:, 0].reshape(-1, 1)\n",
    "            test_pre[i, :] = clf.predict_proba(test_x)[:, 0].reshape(-1, 1)\n",
    "            \n",
    "            cv_scores.append(log_loss(te_y, pre[:, 0].reshape(-1, 1)))\n",
    "            \n",
    "        elif clf_name in ['xgb']:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x, label=te_y, missing=-1)\n",
    "            params = {\n",
    "                'booster': 'gbtree',\n",
    "                'objective': 'multi:softprob',\n",
    "                'eval_metric': 'mlogloss',\n",
    "                'gamma': 1,\n",
    "                'min_child_weight': 1.5,\n",
    "                'max_depth': 5,\n",
    "                'lambda': 10,\n",
    "                'subsample': 0.7,\n",
    "                'colsample_bytree': 0.7,\n",
    "                'colsample_bylevel': 0.7,\n",
    "                'eta': 0.03,\n",
    "                'tree_method': 'exact',\n",
    "                'seed': 2017,\n",
    "                'num_class': 2\n",
    "            }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'), (test_matrix, 'eval')]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params,\n",
    "                                  train_matrix,\n",
    "                                  num_boost_round=num_round,\n",
    "                                  evals=watchlist,\n",
    "                                  early_stopping_rounds=early_stopping_rounds,\n",
    "                                  verbose_eval=50)\n",
    "                pre = model.predict(test_matrix, ntree_limit=model.best_ntree_limit)\n",
    "                train[test_index] = pre[:, 0].reshape(-1, 1)\n",
    "                z_pred = model.predict(z, ntree_limit=model.best_ntree_limit)\n",
    "                print(z_pred[:10])\n",
    "                print('z_pred.shape', z_pred.shape)\n",
    "                test_pre[i, :] = z_pred.reshape(-1, 1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:, 0].reshape(-1, 1)))\n",
    "                \n",
    "        elif clf_name in ['lgb']:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                # 'boosting_type': 'dart',\n",
    "                'objective': 'multiclass',\n",
    "                'metric': 'multi_logloss',\n",
    "                'min_child_weight': 1.5,\n",
    "                'num_leaves': 2 ** 5,\n",
    "                'lambda_l2': 10,\n",
    "                'subsample': 0.7,\n",
    "                'colsample_bytree': 0.7,\n",
    "                'colsample_bylevel': 0.7,\n",
    "                'learning_rate': 0.03,\n",
    "                'tree_method': 'exact',\n",
    "                'seed': 2017,\n",
    "                'num_class': 2,\n",
    "                'silent': True\n",
    "            }\n",
    "            num_round=10000\n",
    "            early_stopping_rounds=100\n",
    "            if test_matrix:\n",
    "                model = clf.train(params,\n",
    "                                  train_matrix,\n",
    "                                  num_round,\n",
    "                                  valid_sets=test_matrix,\n",
    "                                  early_stopping_rounds=early_stopping_rounds,\n",
    "                                  verbose_eval=50)\n",
    "                pre = model.predict(te_x, num_iteration=model.best_iteration)\n",
    "                train[test_index] = pre[:, 0].reshape(-1, 1)\n",
    "                z_pred = model.predict(test_x, ntree_limit=model.best_iteration)\n",
    "                print(z_pred[:10])\n",
    "                print('z_pred.shape', z_pred.shape)\n",
    "                test_pre[i, :] = z_pred.reshape(-1, 1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:, 0].reshape(-1, 1)))\n",
    "        else:\n",
    "            raise IOError('Please add clf.')\n",
    "        print('%s now score is:' % clf_name, cv_scores)\n",
    "    test[:] = test_pre.mean(axis=0)\n",
    "    print('%s_score_list:' % clf_name, cv_scores)\n",
    "    print('%s_score_mean:' % clf_name, np.mean(cv_scores))\n",
    "    return train.reshape(-1, 1), test.reshape(-1, 1)\n",
    "\n",
    "\n",
    "def rf_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    randomforest = RandomForestClassifier(n_estimators=1200,\n",
    "                                          max_depth=20,\n",
    "                                          n_jobs=-1,\n",
    "                                          random_state=2017,\n",
    "                                          max_features='auto',\n",
    "                                          verbose=1)\n",
    "    rf_train, rf_test = stacking_clf(randomforest, x_train, y_train, x_valid, 'rf', kf, label_split=label_split)\n",
    "    return rf_train, rf_test, 'rf'\n",
    "\n",
    "\n",
    "def ada_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    adaboost = AdaBoostClassifier(n_estimators=50,\n",
    "                                  random_state=2017,\n",
    "                                  learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking_clf(adaboost, x_train, y_train, x_valid, 'ada', kf, label_split=label_split)\n",
    "    return rf_train, rf_test, 'ada'\n",
    "\n",
    "\n",
    "def gb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gbdt = GradientBoostingClassifier(n_estimators=100,\n",
    "                                          learning_rate=0.04,\n",
    "                                          subsample=0.8,\n",
    "                                          max_depth=5,\n",
    "                                          verbose=1,\n",
    "                                          random_state=2017)\n",
    "    gbdt_train, gbdt_test = stacking_clf(gbdt, x_train, y_train, x_valid, 'gb', kf, label_split=label_split)\n",
    "    return gbdt_train, rf_test, 'gb'\n",
    "\n",
    "\n",
    "def et_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    extratree = ExtraTreesRegressor(n_estimators=1200,\n",
    "                                    max_depth=35,\n",
    "                                    max_features='auto',\n",
    "                                    n_jobs=-1,\n",
    "                                    verbose=1,\n",
    "                                    random_state=2017)\n",
    "    et_train, et_test = stacking_clf(gbdt, x_train, y_train, x_valid, 'et', kf, label_split=label_split)\n",
    "    return et_train, et_test, 'et'\n",
    "\n",
    "\n",
    "def xgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(xgboost, x_train, y_train, x_valid, 'xgb', kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test, 'xgb'\n",
    "\n",
    "\n",
    "def lgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lgb_train, lgb_test = stacking_clf(lightgbm, x_train, y_train, x_valid, 'lgb', kf, label_split=label_split)\n",
    "    return lgb_train, lgb_test, 'lgb'\n",
    "\n",
    "\n",
    "def gnb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gnb = GaussianNB()\n",
    "    gnb_train, gnb_test = stacking_clf(gnb, x_train, y_train, x_valid, 'gnb', kf, label_split=label_split)\n",
    "    return gnb_train, gnb_test, 'gnb'\n",
    "\n",
    "\n",
    "def lr_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lr = LogisticRegression(C=0.1,\n",
    "                            max_iter=200,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=2017)\n",
    "    lr_train, lr_test = stacking_clf(lr, x_train, y_train, x_valid, 'lr', kf, label_split=label_split)\n",
    "    return lr_train, lr_test, 'lr'\n",
    "\n",
    "\n",
    "def knn_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    knn = KNeighborsClassifier(n_neighbors=200, n_jobs=-1)\n",
    "    knn_train, knn_test = stacking_clf(knn, x_train, y_train, x_valid, 'knn', kf, label_split=label_split)\n",
    "    return knn_train, knn_test, 'knn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取训练数据和验证数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:07.018808Z",
     "start_time": "2020-09-17T06:40:06.975142Z"
    }
   },
   "outputs": [],
   "source": [
    "features_columns = [c for c in all_data_test.columns if c not in ['label', 'prob', 'seller_path', 'cat_path',\n",
    "                                                                  'brand_path', 'action_type_path', 'item_path', 'time_stamp_path']]\n",
    "x_train = all_data_test[all_data_test['label'].notnull()][features_columns].values\n",
    "y_train = all_data_test[all_data_test['label'].notnull()]['label'].values\n",
    "x_valid = all_data_test[all_data_test['label'].isna()][features_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:07.024725Z",
     "start_time": "2020-09-17T06:40:07.019737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:07.034697Z",
     "start_time": "2020-09-17T06:40:07.025722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34176, 3906, 6.0, ..., 0.0, 0.0, 0.0],\n",
       "       [34176, 121, 6.0, ..., 0.0, 0.0, 0.0],\n",
       "       [34176, 4356, 6.0, ..., 0.0, 0.0, 0.0],\n",
       "       ...,\n",
       "       [231552, 3828, 5.0, ..., 0.0, 0.0, 0.0],\n",
       "       [231552, 2124, 5.0, ..., 0.0, 0.0, 0.0],\n",
       "       [232320, 1168, 4.0, ..., 0.0, 0.0, 0.0]], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:07.046666Z",
     "start_time": "2020-09-17T06:40:07.035695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 231), dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:07.066614Z",
     "start_time": "2020-09-17T06:40:07.048661Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_matrix(data):\n",
    "    where_are_nan = np.isnan(data)\n",
    "    where_are_inf = np.isinf(data)\n",
    "    data[where_are_nan] = 0\n",
    "    data[where_are_inf] = 0\n",
    "    return data\n",
    "\n",
    "\n",
    "x_train = np.float_(get_matrix(np.float_(x_train)))\n",
    "y_train = np.int_(y_train)\n",
    "x_valid = np.float_(get_matrix(np.float_(x_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用lgb和xgb构造Stacking特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:07.072596Z",
     "start_time": "2020-09-17T06:40:07.067610Z"
    }
   },
   "outputs": [],
   "source": [
    "folds=5\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:07.083567Z",
     "start_time": "2020-09-17T06:40:07.073594Z"
    }
   },
   "outputs": [],
   "source": [
    "clf_list = [lgb_clf, xgb_clf]\n",
    "clf_list_col = ['lgb_clf', 'xgb_clf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:24.333043Z",
     "start_time": "2020-09-17T06:40:07.084565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's multi_logloss: 0.241901\n",
      "[100]\tvalid_0's multi_logloss: 0.247853\n",
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's multi_logloss: 0.239734\n",
      "[]\n",
      "z_pred.shape (0,)\n",
      "lgb now score is: [2.6559997282941747]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's multi_logloss: 0.297223\n",
      "[100]\tvalid_0's multi_logloss: 0.317045\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_logloss: 0.281976\n",
      "[]\n",
      "z_pred.shape (0,)\n",
      "lgb now score is: [2.6559997282941747, 2.586516769469715]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's multi_logloss: 0.265978\n",
      "[100]\tvalid_0's multi_logloss: 0.277071\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_logloss: 0.25415\n",
      "[]\n",
      "z_pred.shape (0,)\n",
      "lgb now score is: [2.6559997282941747, 2.586516769469715, 2.5786889164949485]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's multi_logloss: 0.204049\n",
      "[100]\tvalid_0's multi_logloss: 0.208761\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's multi_logloss: 0.202928\n",
      "[]\n",
      "z_pred.shape (0,)\n",
      "lgb now score is: [2.6559997282941747, 2.586516769469715, 2.5786889164949485, 2.663735793395645]\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\tvalid_0's multi_logloss: 0.213087\n",
      "[100]\tvalid_0's multi_logloss: 0.218289\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid_0's multi_logloss: 0.211054\n",
      "[]\n",
      "z_pred.shape (0,)\n",
      "lgb now score is: [2.6559997282941747, 2.586516769469715, 2.5786889164949485, 2.663735793395645, 2.6633956639682377]\n",
      "lgb_score_list: [2.6559997282941747, 2.586516769469715, 2.5786889164949485, 2.663735793395645, 2.6633956639682377]\n",
      "lgb_score_mean: 2.6296673743245442\n",
      "[0]\ttrain-mlogloss:0.67087\teval-mlogloss:0.67107\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-mlogloss:0.26992\teval-mlogloss:0.28250\n",
      "[100]\ttrain-mlogloss:0.21075\teval-mlogloss:0.24201\n",
      "[150]\ttrain-mlogloss:0.18498\teval-mlogloss:0.23920\n",
      "[200]\ttrain-mlogloss:0.16730\teval-mlogloss:0.24134\n",
      "[250]\ttrain-mlogloss:0.15525\teval-mlogloss:0.24383\n",
      "Stopping. Best iteration:\n",
      "[161]\ttrain-mlogloss:0.18022\teval-mlogloss:0.23852\n",
      "\n",
      "[]\n",
      "z_pred.shape (0,)\n",
      "xgb now score is: [2.6312218746729195]\n",
      "[0]\ttrain-mlogloss:0.67061\teval-mlogloss:0.67189\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-mlogloss:0.26189\teval-mlogloss:0.30713\n",
      "[100]\ttrain-mlogloss:0.19771\teval-mlogloss:0.28447\n",
      "[150]\ttrain-mlogloss:0.16914\teval-mlogloss:0.29711\n",
      "Stopping. Best iteration:\n",
      "[88]\ttrain-mlogloss:0.20761\teval-mlogloss:0.28372\n",
      "\n",
      "[]\n",
      "z_pred.shape (0,)\n",
      "xgb now score is: [2.6312218746729195, 2.228697444498539]\n",
      "[0]\ttrain-mlogloss:0.67053\teval-mlogloss:0.67127\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-mlogloss:0.26425\teval-mlogloss:0.29365\n",
      "[100]\ttrain-mlogloss:0.20207\teval-mlogloss:0.26129\n",
      "[150]\ttrain-mlogloss:0.17585\teval-mlogloss:0.26487\n",
      "[200]\ttrain-mlogloss:0.15926\teval-mlogloss:0.26714\n",
      "Stopping. Best iteration:\n",
      "[107]\ttrain-mlogloss:0.19757\teval-mlogloss:0.26068\n",
      "\n",
      "[]\n",
      "z_pred.shape (0,)\n",
      "xgb now score is: [2.6312218746729195, 2.228697444498539, 2.4304462041240185]\n",
      "[0]\ttrain-mlogloss:0.67115\teval-mlogloss:0.67056\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-mlogloss:0.27772\teval-mlogloss:0.26525\n",
      "[100]\ttrain-mlogloss:0.21782\teval-mlogloss:0.21413\n",
      "[150]\ttrain-mlogloss:0.19200\teval-mlogloss:0.20677\n",
      "[200]\ttrain-mlogloss:0.17463\teval-mlogloss:0.20513\n",
      "[250]\ttrain-mlogloss:0.16041\teval-mlogloss:0.20615\n",
      "[300]\ttrain-mlogloss:0.14948\teval-mlogloss:0.20657\n",
      "Stopping. Best iteration:\n",
      "[200]\ttrain-mlogloss:0.17463\teval-mlogloss:0.20513\n",
      "\n",
      "[]\n",
      "z_pred.shape (0,)\n",
      "xgb now score is: [2.6312218746729195, 2.228697444498539, 2.4304462041240185, 2.698017404903658]\n",
      "[0]\ttrain-mlogloss:0.67123\teval-mlogloss:0.67095\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[50]\ttrain-mlogloss:0.27493\teval-mlogloss:0.26909\n",
      "[100]\ttrain-mlogloss:0.21599\teval-mlogloss:0.22093\n",
      "[150]\ttrain-mlogloss:0.19034\teval-mlogloss:0.21383\n",
      "[200]\ttrain-mlogloss:0.17277\teval-mlogloss:0.21431\n",
      "[250]\ttrain-mlogloss:0.15906\teval-mlogloss:0.21698\n",
      "Stopping. Best iteration:\n",
      "[158]\ttrain-mlogloss:0.18694\teval-mlogloss:0.21347\n",
      "\n",
      "[]\n",
      "z_pred.shape (0,)\n",
      "xgb now score is: [2.6312218746729195, 2.228697444498539, 2.4304462041240185, 2.698017404903658, 2.654706679554656]\n",
      "xgb_score_list: [2.6312218746729195, 2.228697444498539, 2.4304462041240185, 2.698017404903658, 2.654706679554656]\n",
      "xgb_score_mean: 2.528617921550758\n"
     ]
    }
   ],
   "source": [
    "column_list = []\n",
    "train_data_list = []\n",
    "test_data_list = []\n",
    "for clf in clf_list:\n",
    "    train_data, test_data, clf_name = clf(x_train, y_train, x_valid, kf, label_split=None)\n",
    "    train_data_list.append(train_data)\n",
    "    test_data_list.append(test_data)\n",
    "\n",
    "train_stacking = np.concatenate(train_data_list, axis=1)\n",
    "test_stacking = np.concatenate(test_data_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原始特征和Stacking特征合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:24.350025Z",
     "start_time": "2020-09-17T06:40:24.336035Z"
    }
   },
   "outputs": [],
   "source": [
    "train = np.concatenate([x_train, train_stacking], axis=1)\n",
    "test = np.concatenate([x_valid, test_stacking], axis=1)\n",
    "\n",
    "df_train_all = pd.DataFrame(train)\n",
    "df_test_all = pd.DataFrame(test)\n",
    "df_train_all.columns = features_columns + clf_list_col\n",
    "df_test_all.columns = features_columns + clf_list_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:24.417835Z",
     "start_time": "2020-09-17T06:40:24.353009Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_all['user_id'] = all_data_test[all_data_test['label'].notnull()]['user_id']\n",
    "df_test_all['user_id'] = all_data_test[all_data_test['label'].isnull()]['user_id']\n",
    "df_train_all['label'] = all_data_test[all_data_test['label'].notnull()]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-17T06:40:24.827035Z",
     "start_time": "2020-09-17T06:40:24.420826Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_all.to_csv('../input/train_all.csv', index=False)\n",
    "df_test_all.to_csv('../input/test_all.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
