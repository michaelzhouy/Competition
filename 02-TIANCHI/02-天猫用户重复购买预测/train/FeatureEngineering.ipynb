{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:19:09.506376Z",
     "start_time": "2020-09-16T09:19:08.466732Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import gc\n",
    "from collections import Counter\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:19:09.519126Z",
     "start_time": "2020-09-16T09:19:09.508190Z"
    }
   },
   "outputs": [],
   "source": [
    "# 节省内存读文件\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\"\n",
    "    iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\n",
    "    @param df:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "            df[col] = df[col].astype('str')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:19:34.016496Z",
     "start_time": "2020-09-16T09:19:09.521120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 5.97 MB\n",
      "Memory usage after optimization is: 1.74 MB\n",
      "Decreased by 70.8%\n",
      "Memory usage of dataframe is 5.98 MB\n",
      "Memory usage after optimization is: 3.49 MB\n",
      "Decreased by 41.7%\n",
      "Memory usage of dataframe is 9.71 MB\n",
      "Memory usage after optimization is: 3.24 MB\n",
      "Decreased by 66.7%\n",
      "Memory usage of dataframe is 2933.33 MB\n",
      "Memory usage after optimization is: 890.48 MB\n",
      "Decreased by 69.6%\n"
     ]
    }
   ],
   "source": [
    "train_file = '../input/data_format1/train_format1.csv'\n",
    "test_file = '../input/data_format1/test_format1.csv'\n",
    "user_info_file = '../input/data_format1/user_info_format1.csv'\n",
    "user_log_file = '../input/data_format1/user_log_format1.csv'\n",
    "\n",
    "train_data = reduce_mem_usage(pd.read_csv(train_file))\n",
    "test_data = reduce_mem_usage(pd.read_csv(test_file))\n",
    "user_info = reduce_mem_usage(pd.read_csv(user_info_file))\n",
    "user_log = reduce_mem_usage(pd.read_csv(user_log_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:19:34.026036Z",
     "start_time": "2020-09-16T09:19:34.017492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260864 entries, 0 to 260863\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype\n",
      "---  ------       --------------   -----\n",
      " 0   user_id      260864 non-null  int32\n",
      " 1   merchant_id  260864 non-null  int16\n",
      " 2   label        260864 non-null  int8 \n",
      "dtypes: int16(1), int32(1), int8(1)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:19:34.047387Z",
     "start_time": "2020-09-16T09:19:34.026036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 261477 entries, 0 to 261476\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   user_id      261477 non-null  int32  \n",
      " 1   merchant_id  261477 non-null  int16  \n",
      " 2   prob         0 non-null       float64\n",
      "dtypes: float64(1), int16(1), int32(1)\n",
      "memory usage: 3.5 MB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:19:34.066385Z",
     "start_time": "2020-09-16T09:19:34.048382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 424170 entries, 0 to 424169\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   user_id    424170 non-null  int32  \n",
      " 1   age_range  421953 non-null  float16\n",
      " 2   gender     417734 non-null  float16\n",
      "dtypes: float16(2), int32(1)\n",
      "memory usage: 3.2 MB\n"
     ]
    }
   ],
   "source": [
    "user_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:19:34.783940Z",
     "start_time": "2020-09-16T09:19:34.067370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54925330 entries, 0 to 54925329\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   user_id      int32  \n",
      " 1   item_id      int32  \n",
      " 2   cat_id       int16  \n",
      " 3   seller_id    int16  \n",
      " 4   brand_id     float16\n",
      " 5   time_stamp   int16  \n",
      " 6   action_type  int8   \n",
      "dtypes: float16(1), int16(3), int32(2), int8(1)\n",
      "memory usage: 890.5 MB\n"
     ]
    }
   ],
   "source": [
    "user_log.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:19:35.000496Z",
     "start_time": "2020-09-16T09:19:34.785929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = train_data.append(test_data)\n",
    "all_data = all_data.merge(user_info, on=['user_id'], how='left')\n",
    "\n",
    "del train_data, test_data, user_info\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:19:44.094932Z",
     "start_time": "2020-09-16T09:19:35.002490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 按时间排序\n",
    "user_log = user_log.sort_values(['user_id', 'time_stamp'])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:39.202471Z",
     "start_time": "2020-09-16T09:19:44.095900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对每个用户逐个合并所有字段\n",
    "list_join_func = lambda x: ' '.join([str(i) for i in x])\n",
    "\n",
    "agg_dict = {\n",
    "    'item_id': list_join_func,\n",
    "    'cat_id': list_join_func,\n",
    "    'seller_id': list_join_func,\n",
    "    'brand_id': list_join_func,\n",
    "    'time_stamp': list_join_func,\n",
    "    'action_type': list_join_func\n",
    "}\n",
    "\n",
    "rename_dict = {\n",
    "    'item_id': 'item_path',\n",
    "    'cat_id': 'cat_path',\n",
    "    'seller_id': 'seller_path',\n",
    "    'brand_id': 'brand_path',\n",
    "    'time_stamp': 'time_stamp_path',\n",
    "    'action_type': 'action_type_path'\n",
    "}\n",
    "\n",
    "\n",
    "def merge_list(df_ID, join_columns, df_data, agg_dict, rename_dict):\n",
    "    df_data = df_data.groupby(join_columns).agg(agg_dict).reset_index().rename(columns=rename_dict)\n",
    "    \n",
    "    df_ID = df_ID.merge(df_data, on=join_columns, how='left')\n",
    "    return df_ID\n",
    "\n",
    "all_data = merge_list(all_data, 'user_id', user_log, agg_dict, rename_dict)\n",
    "\n",
    "del user_log\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义特征统计函数\n",
    "## 定义统计函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 定义统计数据总数的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:39.207490Z",
     "start_time": "2020-09-16T09:21:39.203468Z"
    }
   },
   "outputs": [],
   "source": [
    "def cnt_(x):\n",
    "    try:\n",
    "        return len(x.split(' '))\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 定义统计数据唯一值总数的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:39.224059Z",
     "start_time": "2020-09-16T09:21:39.209064Z"
    }
   },
   "outputs": [],
   "source": [
    "def nunique_(x):\n",
    "    try:\n",
    "        return len(set(x.split(' ')))\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 定义统计数据最大值的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:39.236068Z",
     "start_time": "2020-09-16T09:21:39.226054Z"
    }
   },
   "outputs": [],
   "source": [
    "def max_(x):\n",
    "    try:\n",
    "        return np.max([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 定义统计数据最小值的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:39.245010Z",
     "start_time": "2020-09-16T09:21:39.238022Z"
    }
   },
   "outputs": [],
   "source": [
    "def min_(x):\n",
    "    try:\n",
    "        return np.min([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 定义统计数据标准差的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:39.253021Z",
     "start_time": "2020-09-16T09:21:39.246003Z"
    }
   },
   "outputs": [],
   "source": [
    "def std_(x):\n",
    "    try:\n",
    "        return np.std([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 定义统计数据中频次为$topN$数据的元素的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:39.262955Z",
     "start_time": "2020-09-16T09:21:39.253979Z"
    }
   },
   "outputs": [],
   "source": [
    "def most_n(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][0]\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 定义统计数据中频次为$topN$数据的元素的频次的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:39.270972Z",
     "start_time": "2020-09-16T09:21:39.263952Z"
    }
   },
   "outputs": [],
   "source": [
    "def most_n_cnt(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][1]\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用定义的统计函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:39.283909Z",
     "start_time": "2020-09-16T09:21:39.271932Z"
    }
   },
   "outputs": [],
   "source": [
    "def user_cnt(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(cnt_)\n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_nunique(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(nunique_)\n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_max(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(max_)\n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_min(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(min_)\n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_std(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(std_)\n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_most_n(df_data, single_col, name, n=1):\n",
    "    df_data[name] = df_data[single_col].apply(lambda x: most_n(x, n))\n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_most_n_cnt(df_data, single_col, name, n=1):\n",
    "    df_data[name] = df_data[single_col].apply(lambda x: most_n_cnt(x, n))\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取统计特征\n",
    "## 特征统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 店铺特征统计：统计与店铺特点相关的特征，如店铺、商品、品牌等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:40.132701Z",
     "start_time": "2020-09-16T09:21:39.285896Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "基本统计特征\n",
    "\"\"\"\n",
    "all_data_test = all_data.head(2000)\n",
    "\n",
    "# 统计用户点击、浏览、加购、购买行为\n",
    "# 总次数\n",
    "all_data_test = user_cnt(all_data_test, 'seller_path', 'user_cnt')\n",
    "\n",
    "# 不同店铺个数\n",
    "all_data_test = user_nunique(all_data_test, 'seller_path', 'seller_nunique')\n",
    "\n",
    "# 不同品类个数\n",
    "all_data_test = user_nunique(all_data_test, 'cat_path', 'cat_nunique')\n",
    "\n",
    "# 不同品牌个数\n",
    "all_data_test = user_nunique(all_data_test, 'brand_path', 'brand_nunique')\n",
    "\n",
    "# 不同商品个数\n",
    "all_data_test = user_nunique(all_data_test, 'item_path', 'item_nunique')\n",
    "\n",
    "# 活跃天数\n",
    "all_data_test = user_nunique(all_data_test, 'time_stamp_path', 'time_stamp_nunique')\n",
    "\n",
    "# 不同用户行为种数\n",
    "all_data_test = user_nunique(all_data_test, 'action_type_path', 'action_type_nunique')\n",
    "\n",
    "# 最晚时间\n",
    "all_data_test = user_max(all_data_test, 'time_stamp_path', 'time_stamp_max')\n",
    "\n",
    "# 最早时间\n",
    "all_data_test = user_min(all_data_test, 'time_stamp_path', 'time_stamp_min')\n",
    "\n",
    "# 活跃天数方差\n",
    "all_data_test = user_std(all_data_test, 'time_stamp_path', 'time_stamp_std')\n",
    "\n",
    "# 最早与最晚相差天数\n",
    "all_data_test['time_stamp_range'] = all_data_test['time_stamp_max'] - all_data_test['time_stamp_min']\n",
    "\n",
    "# 用户最喜欢的店铺\n",
    "all_data_test = user_most_n(all_data_test, 'seller_path', 'seller_most_1', n=1)\n",
    "\n",
    "# 最喜欢的品类\n",
    "all_data_test = user_most_n(all_data_test, 'cat_path', 'cat_most_1', n=1)\n",
    "\n",
    "# 最喜欢的品牌\n",
    "all_data_test = user_most_n(all_data_test, 'brand_path', 'brand_most_1', n=1)\n",
    "\n",
    "# 最喜欢的商品\n",
    "all_data_test = user_most_n(all_data_test, 'item_path', 'item_most_1', n=1)\n",
    "\n",
    "# 最常见的行为动作\n",
    "all_data_test = user_most_n(all_data_test, 'action_type_path', 'action_type_most_1', n=1)\n",
    "\n",
    "# 用户最喜欢的店铺 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'seller_path', 'seller_most_1_cnt', n=1)\n",
    "\n",
    "# 最喜欢的品类 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'cat_path', 'cat_most_1_cnt', n=1)\n",
    "\n",
    "# 最喜欢的品牌 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'brand_path', 'brand_most_1_cnt', n=1)\n",
    "\n",
    "# 最喜欢的商品 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'item_path', 'item_most_1_cnt', n=1)\n",
    "\n",
    "# 最常见的行为动作 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'action_type_path', 'action_type_most_1_cnt', n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 用户特征统计：对用户的点击、加购、购买、收藏等特征进行统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:40.147891Z",
     "start_time": "2020-09-16T09:21:40.134625Z"
    }
   },
   "outputs": [],
   "source": [
    "# 对点击、加购、购买、收藏分开统计\n",
    "def col_cnt_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "        \n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "        \n",
    "        path_len = len(data_dict[col])  # 总行数\n",
    "        \n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "        return len(data_out)\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "    \n",
    "def col_nunique_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "        \n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "        \n",
    "        path_len = len(data_dict[col])  # 总行数\n",
    "        \n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "        return len(set(data_out))\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "\n",
    "def user_col_cnt(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_cnt_(x, columns_list, action_type), axis=1)\n",
    "    return df_data\n",
    "\n",
    "\n",
    "def user_col_nunique(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_nunique_(x, columns_list, action_type), axis=1)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 统计用户和店铺的关系：对店铺的用户点击次数、加购次数、购买次数、收藏次数等进行统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:40.632189Z",
     "start_time": "2020-09-16T09:21:40.148833Z"
    }
   },
   "outputs": [],
   "source": [
    "# 点击次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path'], '0', 'user_cnt_0')\n",
    "\n",
    "# 加购次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path'], '1', 'user_cnt_1')\n",
    "\n",
    "# 购买次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path'], '2', 'user_cnt_2')\n",
    "\n",
    "# 收藏次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path'], '3', 'user_cnt_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征组合\n",
    "1. 特征组合进行业务特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:41.104615Z",
     "start_time": "2020-09-16T09:21:40.633106Z"
    }
   },
   "outputs": [],
   "source": [
    "# 对店铺+商品的点击次数\n",
    "all_data_test = user_col_cnt(all_data_test, ['seller_path', 'item_path'], '0', 'user_item_cnt_0')\n",
    "\n",
    "# 用户对店铺中多少种不同的商品做了点击\n",
    "all_data_test = user_col_nunique(all_data_test, ['seller_path', 'item_path'], '0', 'seller_nunique_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:41.112513Z",
     "start_time": "2020-09-16T09:21:41.108522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'merchant_id', 'label', 'prob', 'age_range', 'gender',\n",
       "       'item_path', 'cat_path', 'seller_path', 'brand_path', 'time_stamp_path',\n",
       "       'action_type_path', 'user_cnt', 'seller_nunique', 'cat_nunique',\n",
       "       'brand_nunique', 'item_nunique', 'time_stamp_nunique',\n",
       "       'action_type_nunique', 'time_stamp_max', 'time_stamp_min',\n",
       "       'time_stamp_std', 'time_stamp_range', 'seller_most_1', 'cat_most_1',\n",
       "       'brand_most_1', 'item_most_1', 'action_type_most_1',\n",
       "       'seller_most_1_cnt', 'cat_most_1_cnt', 'brand_most_1_cnt',\n",
       "       'item_most_1_cnt', 'action_type_most_1_cnt', 'user_cnt_0', 'user_cnt_1',\n",
       "       'user_cnt_2', 'user_cnt_3', 'user_item_cnt_0', 'seller_nunique_0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用Countvector和TF-IDF提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:41.492395Z",
     "start_time": "2020-09-16T09:21:41.114089Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from scipy import sparse\n",
    "\n",
    "tfidfVec = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS,\n",
    "                           ngram_range=(1, 1),\n",
    "                           max_features=100)\n",
    "columns_list = ['seller_path']\n",
    "for i, col in enumerate(columns_list):\n",
    "    tfidfVec.fit(all_data_test[col])\n",
    "    data_ = tfidfVec.transform(all_data_test[col])\n",
    "    if i == 0:\n",
    "        data_cat = data_\n",
    "    else:\n",
    "        data_cat = sparse.hstack((data_cat, data_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:41.498335Z",
     "start_time": "2020-09-16T09:21:41.493312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x100 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 137 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cat[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:41.531711Z",
     "start_time": "2020-09-16T09:21:41.499834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_0</th>\n",
       "      <th>tfidf_1</th>\n",
       "      <th>tfidf_2</th>\n",
       "      <th>tfidf_3</th>\n",
       "      <th>tfidf_4</th>\n",
       "      <th>tfidf_5</th>\n",
       "      <th>tfidf_6</th>\n",
       "      <th>tfidf_7</th>\n",
       "      <th>tfidf_8</th>\n",
       "      <th>tfidf_9</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidf_90</th>\n",
       "      <th>tfidf_91</th>\n",
       "      <th>tfidf_92</th>\n",
       "      <th>tfidf_93</th>\n",
       "      <th>tfidf_94</th>\n",
       "      <th>tfidf_95</th>\n",
       "      <th>tfidf_96</th>\n",
       "      <th>tfidf_97</th>\n",
       "      <th>tfidf_98</th>\n",
       "      <th>tfidf_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009531</td>\n",
       "      <td>0.186124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173224</td>\n",
       "      <td>0.036434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.011178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009531</td>\n",
       "      <td>0.186124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173224</td>\n",
       "      <td>0.036434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.011178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009531</td>\n",
       "      <td>0.186124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173224</td>\n",
       "      <td>0.036434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.011178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009531</td>\n",
       "      <td>0.186124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173224</td>\n",
       "      <td>0.036434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.011178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tfidf_0  tfidf_1   tfidf_2   tfidf_3  tfidf_4   tfidf_5   tfidf_6  tfidf_7  \\\n",
       "0      0.0      0.0  0.009531  0.186124      0.0  0.173224  0.036434      0.0   \n",
       "1      0.0      0.0  0.009531  0.186124      0.0  0.173224  0.036434      0.0   \n",
       "2      0.0      0.0  0.009531  0.186124      0.0  0.173224  0.036434      0.0   \n",
       "3      0.0      0.0  0.009531  0.186124      0.0  0.173224  0.036434      0.0   \n",
       "4      0.0      0.0  0.000000  0.000000      0.0  0.000000  0.000000      0.0   \n",
       "\n",
       "   tfidf_8   tfidf_9  ...  tfidf_90  tfidf_91  tfidf_92  tfidf_93  tfidf_94  \\\n",
       "0      0.0  0.012205  ...       0.0       0.0  0.012666  0.011178       0.0   \n",
       "1      0.0  0.012205  ...       0.0       0.0  0.012666  0.011178       0.0   \n",
       "2      0.0  0.012205  ...       0.0       0.0  0.012666  0.011178       0.0   \n",
       "3      0.0  0.012205  ...       0.0       0.0  0.012666  0.011178       0.0   \n",
       "4      0.0  0.000000  ...       0.0       0.0  0.000000  0.000000       0.0   \n",
       "\n",
       "   tfidf_95  tfidf_96  tfidf_97  tfidf_98  tfidf_99  \n",
       "0       0.0       0.0       0.0  0.012925       0.0  \n",
       "1       0.0       0.0       0.0  0.012925       0.0  \n",
       "2       0.0       0.0       0.0  0.012925       0.0  \n",
       "3       0.0       0.0       0.0  0.012925       0.0  \n",
       "4       0.0       0.0       0.0  0.000000       0.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(data_cat.toarray())\n",
    "df_tfidf.columns = ['tfidf_' + str(i) for i in df_tfidf.columns]\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:41.540736Z",
     "start_time": "2020-09-16T09:21:41.532747Z"
    }
   },
   "outputs": [],
   "source": [
    "all_data_test = pd.concat([all_data_test, df_tfidf], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 嵌入特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:43.620630Z",
     "start_time": "2020-09-16T09:21:41.542682Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(all_data_test['seller_path'].apply(lambda x: x.split(' ')),\n",
    "                               size=100,\n",
    "                               window=5,\n",
    "                               min_count=5,\n",
    "                               workers=4)\n",
    "\n",
    "def mean_w2v_(x, model, size=100):\n",
    "    try:\n",
    "        i = 0\n",
    "        for word in x.split(' '):\n",
    "            if word in model.wv.vocab:\n",
    "                i += 1\n",
    "                if i == 0:\n",
    "                    vec = np.zeros(size)\n",
    "                vec += model.wv[word]\n",
    "        return vec / i\n",
    "    except:\n",
    "        return np.zeros(size)\n",
    "    \n",
    "\n",
    "def get_mean_w2v(df_data, columns, model, size):\n",
    "    data_array = []\n",
    "    for index, row in df_data.iterrows():\n",
    "        w2v = mean_w2v_(row[columns], model, size)\n",
    "        data_array.append(w2v)\n",
    "    return pd.DataFrame(data_array)\n",
    "\n",
    "df_embedding = get_mean_w2v(all_data_test, 'seller_path', model, 100)\n",
    "df_embedding.columns = ['embedding_' + str(i) for i in df_embedding.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:43.633588Z",
     "start_time": "2020-09-16T09:21:43.622619Z"
    }
   },
   "outputs": [],
   "source": [
    "all_data_test = pd.concat([all_data_test, df_embedding], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking分类工具\n",
    "## Stacking特征工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:44.192625Z",
     "start_time": "2020-09-16T09:21:43.634718Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss, mean_absolute_error, mean_squared_error\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义Stacking分类特征相关函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:44.222498Z",
     "start_time": "2020-09-16T09:21:44.193539Z"
    }
   },
   "outputs": [],
   "source": [
    "def stacking_clf(clf, train_x, train_y, test_x, clf_name, kf, label_split=None):\n",
    "    train = np.zeros((train_x.shape[0], 1))\n",
    "    test = np.zeros((test_x.shape[0], 1))\n",
    "    test_pre = np.empty((folds, test_x.shape[0], 1))\n",
    "    cv_scores = []\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(train_x, label_split)):\n",
    "        tr_x = train_x[train_index]\n",
    "        tr_y = train_y[train_index]\n",
    "        te_x = train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "        \n",
    "        if clf_name in ['rf', 'ada', 'gb', 'et', 'lr', 'knn',' gnb']:\n",
    "            clf.fit(tr_x, tr_y)\n",
    "            pre = clf.predict_proba(te_x)\n",
    "            train[test_index] = pre[:, 0].reshape(-1, 1)\n",
    "            test_pre[i, :] = clf.predict_proba(test_x)[:, 0].reshape(-1, 1)\n",
    "            \n",
    "            cv_scores.append(log_loss(te_y, pre[:, 0].reshape(-1, 1)))\n",
    "            \n",
    "        elif clf_name in ['xgb']:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=tr_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x, label=te_y, missing=-1)\n",
    "            params = {\n",
    "                'booster': 'gbtree',\n",
    "                'objective': 'multi:softprob',\n",
    "                'eval_metric': 'mlogloss',\n",
    "                'gamma': 1,\n",
    "                'min_child_weight': 1.5,\n",
    "                'max_depth': 5,\n",
    "                'lambda': 10,\n",
    "                'subsample': 0.7,\n",
    "                'colsample_bytree': 0.7,\n",
    "                'colsample_bylevel': 0.7,\n",
    "                'eta': 0.03,\n",
    "                'tree_method': 'exact',\n",
    "                'seed': 2017,\n",
    "                'num_class': 2\n",
    "            }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'), (test_matrix, 'eval')]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params,\n",
    "                                  train_matrix,\n",
    "                                  num_boost_round=num_round,\n",
    "                                  evals=watchlist,\n",
    "                                  early_stopping_rounds=early_stopping_rounds)\n",
    "                pre = model.predict(test_matrix, ntree_limit=model.best_ntree_limit)\n",
    "                train[test_index] = pre[:, 0].reshape(-1, 1)\n",
    "                test_pre[i, :] = model.predict(z, ntree_limit=model.best_ntree_limit)[:, 0].reshape(-1, 1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:, 0].reshape(-1, 1)))\n",
    "                \n",
    "        elif clf_name in ['lgb']:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                # 'boosting_type': 'dart',\n",
    "                'objective': 'multiclass',\n",
    "                'metric': 'multi_logloss',\n",
    "                'min_child_weight': 1.5,\n",
    "                'num_leaves': 2 ** 5,\n",
    "                'lambda_l2': 10,\n",
    "                'subsample': 0.7,\n",
    "                'colsample_bytree': 0.7,\n",
    "                'colsample_bylevel': 0.7,\n",
    "                'learning_rate': 0.03,\n",
    "                'tree_method': 'exact',\n",
    "                'seed': 2017,\n",
    "                'num_class': 2,\n",
    "                'silent': True\n",
    "            }\n",
    "            num_round=10000\n",
    "            early_stopping_rounds=100,\n",
    "            if test_matrix:\n",
    "                model = clf.train(params,\n",
    "                                  train_matrix,\n",
    "                                  num_round,\n",
    "                                  valid_sets=test_matrix,\n",
    "                                  early_stopping_rounds=early_stopping_rounds)\n",
    "                pre = model.predict(te_x, num_iteration=model.best_iteration)\n",
    "                train[test_index] = pre[:, 0].reshape(-1, 1)\n",
    "                test_pre[i, :] = model.predict(test_x, num_iteration=model.best_iteration)[:, 0].reshape(-1, 1)\n",
    "        else:\n",
    "            raise IOError('Please add clf.')\n",
    "        print('%s now score is:' % clf_name, cv_scores)\n",
    "    test[:] = test_pre.mean(axis=0)\n",
    "    print('%s_score_list:' % clf_name, cv_scores)\n",
    "    print('%s_score_mean:' % clf_name, np.mean(cv_scores))\n",
    "    return train.reshape(-1, 1), test.reshape(-1, 1)\n",
    "\n",
    "\n",
    "def rf_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    randomforest = RandomForestClassifier(n_estimators=1200,\n",
    "                                          max_depth=20,\n",
    "                                          n_jobs=-1,\n",
    "                                          random_state=2017,\n",
    "                                          max_features='auto',\n",
    "                                          verbose=1)\n",
    "    rf_train, rf_test = stacking_clf(randomforest, x_train, y_train, x_valid, 'rf', kf, label_split=label_split)\n",
    "    return rf_train, rf_test, 'rf'\n",
    "\n",
    "\n",
    "def ada_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    adaboost = AdaBoostClassifier(n_estimators=50,\n",
    "                                      random_state=2017,\n",
    "                                      learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking_clf(adaboost, x_train, y_train, x_valid, 'ada', kf, label_split=label_split)\n",
    "    return rf_train, rf_test, 'ada'\n",
    "\n",
    "\n",
    "def gb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gbdt = GradientBoostingClassifier(n_estimators=100,\n",
    "                                          learning_rate=0.04,\n",
    "                                          subsample=0.8,\n",
    "                                          max_depth=5,\n",
    "                                          verbose=1,\n",
    "                                          random_state=2017)\n",
    "    gbdt_train, gbdt_test = stacking_clf(gbdt, x_train, y_train, x_valid, 'gb', kf, label_split=label_split)\n",
    "    return gbdt_train, rf_test, 'gb'\n",
    "\n",
    "\n",
    "def et_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    extratree = ExtraTreesRegressor(n_estimators=1200,\n",
    "                                    max_depth=35,\n",
    "                                    max_features='auto',\n",
    "                                    n_jobs=-1,\n",
    "                                    verbose=1,\n",
    "                                    random_state=2017)\n",
    "    et_train, et_test = stacking_clf(gbdt, x_train, y_train, x_valid, 'et', kf, label_split=label_split)\n",
    "    return et_train, et_test, 'et'\n",
    "\n",
    "\n",
    "def xgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(xgboost, x_train, y_train, x_valid, 'xgb', kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test, 'xgb'\n",
    "\n",
    "\n",
    "def lgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lgb_train, lgb_test = stacking_clf(lightgbm, x_train, y_train, x_valid, 'lgb', kf, label_split=label_split)\n",
    "    return lgb_train, lgb_test, 'lgb'\n",
    "\n",
    "\n",
    "def gnb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gnb = GaussianNB()\n",
    "    gnb_train, gnb_test = stacking_clf(gnb, x_train, y_train, x_valid, 'gnb', kf, label_split=label_split)\n",
    "    return gnb_train, gnb_test, 'gnb'\n",
    "\n",
    "\n",
    "def lr_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lr = LogisticRegression(C=0.1,\n",
    "                            max_iter=200,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=2017)\n",
    "    lr_train, lr_test = stacking_clf(lr, x_train, y_train, x_valid, 'lr', kf, label_split=label_split)\n",
    "    return lr_train, lr_test, 'lr'\n",
    "\n",
    "\n",
    "def knn_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    knn = KNeighborsClassifier(n_neighbors=200, n_jobs=-1)\n",
    "    knn_train, knn_test = stacking_clf(knn, x_train, y_train, x_valid, 'knn', kf, label_split=label_split)\n",
    "    return knn_train, knn_test, 'knn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取训练数据和验证数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:44.267444Z",
     "start_time": "2020-09-16T09:21:44.224049Z"
    }
   },
   "outputs": [],
   "source": [
    "features_columns = [c for c in all_data_test.columns if c not in ['label', 'prob', 'seller_path', 'cat_path',\n",
    "                                                                  'brand_path', 'action_type_path', 'item_path', 'time_stamp_path']]\n",
    "x_train = all_data_test[all_data_test['label'].notnull()][features_columns].values\n",
    "y_train = all_data_test[all_data_test['label'].notnull()]['label'].values\n",
    "x_valid = all_data_test[all_data_test['label'].isna()][features_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:44.287351Z",
     "start_time": "2020-09-16T09:21:44.268399Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_matrix(data):\n",
    "    where_are_nan = np.isnan(data)\n",
    "    where_are_inf = np.isinf(data)\n",
    "    data[where_are_nan] = 0\n",
    "    data[where_are_inf] = 0\n",
    "    return data\n",
    "\n",
    "\n",
    "x_train = np.float_(get_matrix(np.float_(x_train)))\n",
    "y_train = np.int_(y_train)\n",
    "x_valid = np.float_(get_matrix(np.float_(x_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用lgb和xgb构造Stacking特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:44.292297Z",
     "start_time": "2020-09-16T09:21:44.288309Z"
    }
   },
   "outputs": [],
   "source": [
    "folds=5\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:44.299317Z",
     "start_time": "2020-09-16T09:21:44.293294Z"
    }
   },
   "outputs": [],
   "source": [
    "clf_list = [lgb_clf, xgb_clf]\n",
    "clf_list_col = ['lgb_clf', 'xgb_clf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T09:21:44.537676Z",
     "start_time": "2020-09-16T09:21:44.300276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 0.240518\n",
      "Training until validation scores don't improve for (100,) rounds\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'int' and 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-5828509ecba4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_data_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclf_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtrain_data_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtest_data_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-a0c023c3017d>\u001b[0m in \u001b[0;36mlgb_clf\u001b[1;34m(x_train, y_train, x_valid, kf, label_split)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlgb_clf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[0mlgb_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlgb_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstacking_clf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlightgbm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lgb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlgb_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlgb_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lgb'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-a0c023c3017d>\u001b[0m in \u001b[0;36mstacking_clf\u001b[1;34m(clf, train_x, train_y, test_x, clf_name, kf, label_split)\u001b[0m\n\u001b[0;32m     80\u001b[0m                                   \u001b[0mnum_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m                                   \u001b[0mvalid_sets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m                                   early_stopping_rounds=early_stopping_rounds)\n\u001b[0m\u001b[0;32m     83\u001b[0m                 \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[0mtest_pre\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    262\u001b[0m                                         \u001b[0mbegin_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_iteration\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m                                         \u001b[0mend_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_iteration\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m                                         evaluation_result_list=evaluation_result_list))\n\u001b[0m\u001b[0;32m    265\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEarlyStopException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mearlyStopException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mearlyStopException\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\callback.py\u001b[0m in \u001b[0;36m_callback\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m    236\u001b[0m                 \u001b[0m_final_iteration_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_name_splitted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m                 \u001b[1;32mcontinue\u001b[0m  \u001b[1;31m# train data for lgb.cv or sklearn wrapper (underlying lgb.train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m             \u001b[1;32melif\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbest_iter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mstopping_rounds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m                     print('Early stopping, best iteration is:\\n[%d]\\t%s' % (\n",
      "\u001b[1;31mTypeError\u001b[0m: '>=' not supported between instances of 'int' and 'tuple'"
     ]
    }
   ],
   "source": [
    "column_list = []\n",
    "train_data_list = []\n",
    "test_data_list = []\n",
    "for clf in clf_list:\n",
    "    train_data, test_data, clf_name = clf(x_train, y_train, x_valid, kf, label_split=None)\n",
    "    train_data_list.append(train_data)\n",
    "    test_data_list.append(test_data)\n",
    "\n",
    "train_stacking = np.concatenate(train_data_list, axis=1)\n",
    "test_stacking = np.concatenate(test_data_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
